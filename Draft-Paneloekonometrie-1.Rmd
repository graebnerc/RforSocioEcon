---
title: 'Panel econometrics: introduction'
author: "Claudius"
date: "3/10/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r include=FALSE}
knitr::opts_chunk$set(comment = "#>", message = FALSE, warning = FALSE)
knitr::opts_chunk$set(out.height = '50%', out.width = '50%', fig.align = 'center') 
```
# Regressionsanalyse für Panel-Daten

EINLEITUNGSTEXT
Dabei greifen wir an zahlreichen Stellen auf das Paket `plm` [@plm] zurück,
welches zahlreiche Schätzer und Testverfahren für Panel-Daten in R 
implementiert hat.

Theoretische Aspekte der Panel-Ökonometrie werden in diesem Kapitel 
regelmäßig aufgegriffen, aber stehen definitiv nicht im Fokus.
Diese Aspekte werden in zahlreichen Lehrbüchern diskutiert, z.B. in @wooldridge
oder @greene. 
Besonders erwähnenswert ist hier @Baltagi, der sich durch eine besonders
umfassende Auseinandersetzung mit Panel-Daten auszeichnet (der Fokus bei
den erstgenannten Büchern liegt eher auf der Mikroökonometrie, s.u.).

<!--
Add: Baltagi, Trivedi: Microeconometrics
-->

## Verwendete Pakete {-}

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(data.table)
library(here)
library(viridis)
library(plm)
```

```{r, echo=FALSE}
source(here::here("R/helpers.R"))
```

## Panel-Daten: Klarstellungen und Vorbemerkungen {#sec:panel-prelims}

Bislang haben wir wir uns im Kontext der Regression auf Methoden zur Analyse 
von **Querschnittsdaten** (cross-sectional data) fokussiert.
Die Schätzer, die wir bisher kennen gelernt haben, sind dafür gemacht, 
Datensätze zu analysieren, in denen für jedes einzelne Untersuchungsobjekt
genau eine Beobachtung, typischerweise zum gleichen Zeitpunkt, existiert.
Da individuelle Untersuchungsobjekte oft mit $i=1,2,3,...,N$ indiziert werden,
sprechen wir in diesem Fall von der $N$-Dimension. Querschnittsdaten sind also
nur durch Variation in der $N$-Dimension gekennzeichnet und viele Eigenschaften
der Schätzer können asymptotisch bewiesen werden, wenn $N$ immer größer wird.
So ist die *Konsistenz* eine asymptotische Eigenschaft des OLS-Schätzers, die
gilt wenn die Stichprobe immer größer wird, also wenn $N\rightarrow\infty$.

Wenn wir für das gleiche Untersuchungsobjekt mehrere Beobachtungen zu 
unterschiedlichen Zeitpunkten haben, sprechen wir von einer **Zeitreihe**.
In diesem Fall werden die einzelnen Beobachtungen mit $t=1,2,3,...,T$ 
indiziert und wir sprechen von einer Variation in der $T$-Dimension.
Schätzer, die zur Analyse von Zeitreihendaten entwickelt wurden, haben 
Eigenschaften, die sich asymptotisch beweisen lassen für den Fall, dass
$T\rightarrow\infty$, wenn die Stichprobe also immer größer wird weil wir für
den gleichen Untersuchungsgegenstand Beobachtungen für immer mehr Zeitpunkte 
haben.

Das Thema dieses Kapitels sind **Panel-Daten**.
Diese immer weiter verbreiteten Datensätze haben Variation sowohl in der 
$N$- als auch der $T$-Dimension.
Es bestehen also Beobachtungen für unterschiedliche Untersuchungsobjekte zu
unterschiedlichen Zeitpunkten. 
Ein Panel-Datensatz besteht z.B. aus einer Menge an
Ländern, die jeweils zu unterschiedlichen Zeitpunkten beobachtet wurden.
Wenn wir für Deutschland und Österreich Daten zum BIP für die Jahre 1995-2000
haben ist das ein Panel-Datensatz mit $N=2$ (Deutschland und Österreich) sowie
$T=6$ (Beobachtungen für die Jahre 1995-2000).
Durch die Kombination von Variation in der $T$- und $N$-Dimension ergeben sich
ganz neue Möglichkeiten und Herausforderungen, sodass zahlreiche spezielle
Schätzer für Panel-Datensätze entwickelt wurden - insbesondere weil Schätzer
für Querschnittsdaten in der Regel wenig attraktive Eigenschaften besitzen, 
wenn sie für die Analyse von Panel-Daten verwendet werden.
Dabei sind die bestehenden Schätzer viel diverser und manche sind eher für
**lange** Panels (hohes $T$, kleines $N$), andere für **breite** Panels (kleines
$T$ und großes $N$) geeignet.

Zudem werden **balancierte und unbalancierte Panels** unterschieden:
in ersterem Fall haben wir für jedes Untersuchungsobjekt zu jedem Zeitpunkt
ein Beobachtung. Das Panel ist so zu sagen vollständig.
Im unbalancierten Fall fehlen für einige Untersuchungsobjekte zu einigen 
Zeitpunkten eine Beobachtung.
Das verkompliziert die Berechnungen ungemein, weswegen einige Schätzer und
Testverfahren ausschließlich für balancierte Panels umsetzbar sind.

Noch ein abschließendes Wort zur Notation. 

* Subsets
* warum wir $\alpha$ anstatt $\beta_0$ verwenden

## Die Baseline-Modelle für Panel-Daten: Fixed and Random Effects {#sec:panel-fixedrandom}

## Besonderheiten von langen Panels {#sec:panels-longpanels}

Allgemein sprechen wir von langen Panels wenn $T>N$, aber aktuell auch noch
$T\approx N$. In diesem Bereich findet aktuell noch viel grundlegende Forschung
statt und es gibt eine große Bandbreite an Schätzern und Testverfahren mit oft
nur spezifischem Anwendungsgebiet.
Gleichzeitig erlauben lange Panels zunehmend die empirische Analyse von 
sehr spannenden Fragen, insbesondere in der Makroökonomik, z.B. nicht nur die Stärke 
des langfristigen Zusammenhangs zwischen Einkommensungleichheit und Einkommen,
sondern auch seine Heterogenität zwischen Ländern.

An dieser Stelle wollen wir drei typische Herausforderungen mit langen Panels 
einführen und in diesem Zusammenhang ausgewählte Schätz- und Tesverfahren, die
speziell für diesen Kontext entwickelt wurden, einführen.
Konkret handelt es sich dabei um (1) die Annahme homogener Steigungskoeffizienten 
für alle Untersuchungsobjekte (i.e. $\beta_i = \beta_j \forall i,j \in n$) und 
ihre Abschwächung (Abschnitt \@ref(sec:panels-hetcoefs)); 
(2) die Bedeutung von *cross sectional dependence* (CSD)
zwischen Untersuchungsobjekten, also (Abschnitt \@ref(sec:panels-csd));
(3) und zuletzt die Rolle von Einheitswurzeln in und Ko-Integrationsbeziehungen
zwischen den untersuchten Zeitreihen (Abschnitt \@ref(sec:panel-unitcoint)).

### Heterogene Regressionskoeffizienten {#sec:panels-hetcoefs}

In unseren bisherigen Modellen haben wir angenommen, dass der zu schätzende
Parameter $\beta$ für alle Untersuchungsobjekte gleich ist.
Diese Annahme wird in der Literatur als *Pooling*-Annahme bezeichnet, weil
alle Untersuchungsobjekte gewissermaßen in einen Topf (oder 'Pool') 
geworfen werden um ein Modell zu schätzen.
Heterogenität wird in einem solchen Kontext nur über individuelle Achsenabschnitte,
ggf. im Kontext eines FE Settings, berücksichtigt. Aber der eigentliche 
Zusammenhang zwischen abhängier und unabhängigen Variabeln in Form der
Steigungsparameter wird als homogener Zusammenhang modelliert.
Das kann sich in der Praxis als problematisch herausstellen:
die Nichtberücksichtigung von tatsächlich vorhandener Heterogenität führt
in der Praxis zu verzerrten Schätzern [@BaltagiHet].
Gleichzeitig geht die Schätzung heterogener Koeffizienten mit deutlich höheren
Anforderungen an die Stichprobengröße einher, da bei zu wenig Beobachtungen
der Schätzer für heterogene Koeffizienten instabil wird.

Zum Glück können wir eine Modellspezifikation mit homogenen Koeffizienten $\beta$

\begin{equation}
\label{eq:homoeq}
y_{nt} = \alpha + \beta x_{nt} + \eta_n + v_{nt}
\end{equation}

als Sonderfall eines Modelles mit heterogenen Koeffizienten

\begin{equation}
\label{eq:heteroeq}
y_{nt} = \alpha + \beta_n x_{nt} + \eta_n + v_{nt}
\end{equation}

betrachten, wobei Modelle \@ref(eq:homoeq) annimmt, dass 
$\beta=\beta_n \forall n$.
Diese Annahme können wir in der Praxis relativ einfach testen indem wir 
sowohl Modell \@ref(eq:homoeq) als auch Modell \@ref(eq:heteroeq) schätzen und
die Koeffizienten über einen Chow-Test^[Der Chow Test ist ein Test, bei dem 
wir die Koeffizienten zweier Regressionsmodelle vergleichen. Unter der 
Nullhypothese gleicher Koeffizienten folgt die Teststatistik einer 
F-Verteilung mit $NT-K$ Freiheitsgraden.] vergleichen.
Bevor wir diesen Test jedoch durchführen können, müssen wir uns zunächst mit
möglichen Schätzern für Modell \@ref(eq:heteroeq) vertraut machen. 
Wie in Abschnitt \@ref(sec:panel-fixedrandom) unterscheiden wir hier 
Fixed und Random Effects Modelle.

Für den *fixed effects* Fall werden *de facto* $N$ verschiedene OLS Regressionen
mit individuellen Effekten geschätzt - genauso wie in Abschnitt
\@ref(sec:panel-fixedrandom) beschrieben. Die Schätzgleichungen sind in diesem
Falle einfach durch Gleichung \@ref(eq:heteroeq) gegeben.
Theoretisch könnten wir das manuell implementieren indem
wir für jedes einzelne Beobachtungssubjekte einen Sub-Datensatz erstellen und dort
dann die Funktion `plm::plm()` anwenden und die Koeffizienten jeweils speichern.
Das gleiche erreichen wir aber auch einfacher durch Verwendung der Funktion
`plm::pvcm()`, welche in den Argumenten sehr ähnlich zu `plm::plm()` ist, aber
die geschätzten Koeffizienten gleichfür alle Beobachtungssubjekte schätzt und
in einer Liste speichert.
Um also Modell \@ref(eq:heteroeq) zu schätzen verwenden wir einfach 
`plm::pvcm()` mit dem Argument `model="within`.
Der Nachteil ist, dass wir nicht wirklich einen Vorteil durch die Verwendung
des Panels im Bezug auf die Effizienz der Schätzung haben, da wir hier *de facto*
$NK$ Parameter mit $NT$ Datenpunkten schätzen.

```{r, echo=FALSE}
gdp_ineq_data <- data.table::fread(
  file = here::here("data/tidy/gdp_inequality.csv"), 
  colClasses = c("character", rep("double", 4))
  )
gdp_ineq_pdata <- pdata.frame(
  gdp_ineq_data, index = c("country", "year"))
```

> **Anwendungsbeispiel: Der heterogene Zusammenhang zwischen Ungleichheit und Nationaleinkommen**
> Hier verwenden wir Beispiel den Datensatz, mit dem @FlechtnerCoint 
den langfristigen Zusammenhang zwischen Einkommensungleichheit und 
Bruttoinlandsprodukt analysiert haben. 
Es handelt sich in um ein unbalanciertes Panel mit Beobachtungen aus
`r length(unique(gdp_ineq_data$country))` Ländern zwischen
`r min(gdp_ineq_data$year)` und `r max(gdp_ineq_data$year)`. 
Das Bruttoinlandsprodukt wird in pro Kopf Einheiten in PPP gemessen 
(Variable `gdp_pc_chppp`), die Einkommensungleichheit über den 
Prä- und Post-Steuer-Gini-Koeffizienten (`gini_mkt` und `gini_disp`) von 
@SoltGini.

```{r}
str(gdp_ineq_data, vec.len=3)
```

```{r, echo=FALSE}
HousePricesUS <- data.table::fread(
  file = here("data/tidy/HousePricesUS.csv"))
HousePricesUS_pdata <- pdata.frame(HousePricesUS)
```

> Das gepoolte OLS Modell und das *within*-Modell schätzen wir wie üblich mit
der Funktion `plm::plm()`, jeweils mit dem Argument `model = "pooling"`, bzw.
`model = "within"`. Für das Modell mit heterogenen Koeffizienten verwenden
wir die Funktion `plm::pvcm()`. Die Argumente sind sehr ähnlich. Im vorliegenden
Falle wollen wir auch individuelle *fixed effects* berücksichtigen und verwenden
entsprechend das Argument `model = "within"`:

```{r, echo=FALSE}
model.het <- plm::pvcm(
  formula = log(price) ~ log(income), 
  data = HousePricesUS_pdata, model = "within")
model.pool <- plm(
  formula = log(price) ~ log(income), 
  data = HousePricesUS_pdata, model = "pooling")
model.within <- plm(
  formula = log(price) ~ log(income), 
  data = HousePricesUS_pdata, model = "within")
```

```{r}
model.het <- plm::pvcm(
  formula = log(gdp_pc_chppp) ~ log(gini_disp), 
  data = gdp_ineq_pdata, model = "within")
model.pool <- plm(
  formula = log(gdp_pc_chppp) ~ log(gini_disp), 
  data = gdp_ineq_pdata, model = "pooling")
model.within <- plm(
  formula = log(gdp_pc_chppp) ~ log(gini_disp), 
  data = gdp_ineq_pdata, model = "within")
```

> Die geschätzten Koeffizienten des gepoolten und des *within* Modells können
wir über die klassische Output-Tabelle betrachten:

```{r, include=T}
texreg::screenreg(
  list("OLS (Pooled)"=model.pool, 
       "Within-Modell"=model.within)
  )
```

Bei den heterogenen Koeffizienten funktioniert das natürlich nicht.
Hier können wir uns aus dem Schätzobjekt deskriptive Statistiken ausgeben 
lassen:

```{r}
summary(model.het)
```

> Oder aber wir visualisieren die geschätzten Koeffizienten grafisch, indem 
wir sie aus dem Schätzobjet über `coef()` extrahieren:

```{r}
make_plot <- function(data_used, var_name){
  ggplot(data_used, aes_string(x = var_name)) + 
  scale_y_continuous(expand = expansion(), name = "Häufigkeit") +
  geom_histogram(bins = 8) + xlab("Geschätzter Koeffizient") + theme_bw() +
  theme(
    panel.border = element_blank(), axis.line = element_line())
}
estimates_intercept <- make_plot(
  tibble::tibble("Achsenabschnitt"=coef(model.het)[[1]]), "Achsenabschnitt")

estimates_beta <- make_plot(
  tibble::tibble(
    "Steigungskoeffizient"=coef(model.het)[[2]]), "Steigungskoeffizient")
```

```{r, echo=FALSE}
het_estimates <- ggpubr::ggarrange(
  estimates_intercept, estimates_beta, ncol = 2, labels = c("a)", "b)")
)
het_estimates <- ggpubr::annotate_figure(
  het_estimates, 
  top = ggpubr::text_grob("Geschätzte Koeffizienten"))

plot_file <- here::here(
  "figures/PanelReg/heterogene-koeffizienten-bsp.png")
save_pdf_png(
  filename = plot_file, plot = het_estimates, 
  width = 5, height = 2.5)
```

```{r, echo=FALSE, warning=FALSE}
knitr::include_graphics(plot_file, auto_pdf = T)
```

> Wir können nun die Stabilität der Koeffizienten testen indem wir das Modell
mit heterogenen Koeffizienten (`model.het`) gegen das gepoolte Modell mit
(`model.within`) oder ohne (`model.pool`) *fixed effects* vergleichen. 
Dafür verwenden wir die Funktion `plm::pooltest()`, welche die Nullhypothese
homogener Koeffizienten testet.
Für den ersten Vergleich:

```{r}
pooltest(model.pool, model.het)
```

Die Nullhypothese homogener Koeffizienten muss also klar abgelehnt werden.
Der Vergleich mit dem *fixed effects* Modell liefert ein ähnliches Ergebnis:

```{r}
pooltest(model.within, model.het)
```

> Auch hier muss die Nullhypothese von homogenen Koeffizienten verworfen werden 
und prinzipiell wäre ein Modell mit heterogenen Koeffizienten zu bevorzugen, 
so denn es denn die Datenlage hergibt.
> Beachten Sie bei den Tests auch die unterschiedlichen Freiheitsgrade der Modelle: 
Das Modell mit heterogenen Koeffizienten hat $N(T-K-1)$, das gepoolte OLS Modell
$NT-K-1$ und das *within*-Modell $N(T-1)-K$ Freiheitsgrade.

Wie oben beschrieben verlieren wir bei Verwendung eines *fixed effects* Modells
viele Freiheitsgrade, da insgesamt $NK$ Parameter geschätzt werden müssen 
($K$ Parameter jeweils für $N$ Untersuchungssubjekte).

Eine Alternative Möglichkeit heterogene Koeffizienten zu schätzen bieten sich
im *Random Effects* Kontext.
Besonders bekannt ist dabei das so genannte *Swamy Modell*, dessen 
Ausgangspunkt folgender ist:

\begin{equation}
\label{eq:swamy-base}
y_{n,t} = \gamma_n^Tz_{n,t}+v_{n,t}
\end{equation}

Wenn $\gamma_n\propto\mathcal{N}(\gamma, \Delta)$
(wobei $\Delta$ die Varianz **CHECK**), $\delta_n=\gamma_n-\gamma$ und $\epsilon=v_{n,t}+\delta_n^Tz_{n,t}$:

\begin{equation}
\label{eq:swamy-ind}
y_{n,t} = \gamma^Tz_{n,t}+\epsilon_{n,t}
\end{equation}

Die Strategie ist zunächst die Koeffizienten individuell mit OLS zu schätzen:

\begin{equation}
\label{eq:swamy-ind-ests}
\hat{\gamma}_n = (Z^T_nZ_n)^{-1}Z_n^Ty_n = \delta_n + (Z^T_nZ_n)^{-1}Z_n^Tv_n,
\end{equation}

dann den entsprechenden Mittelwert zu berechnen:

\begin{equation}
\label{eq:swamy-mean}
\bar{\hat{\gamma}} = \frac{1}{N}\sum_{n=1}^N \hat{\gamma}_n
\end{equation}

Die Varianz $Delta$ wird über die Formel

\begin{equation}
\label{eq:swamy-var}
\hat{\Delta}=\frac{1}{N-1}\sum_{n=1}^N(\hat{\gamma}_n-\gamma)^2 - \frac{1}{N}\sum_{n=1}^N\sigma_n^2(Z_n^TZ)^{-1}
\end{equation}

geschätzt (für die Herleitung siehe XXX).**CHECK SIGMA**
Um dieses Modell zu schätzen verwenden wir die Funktion `plm::pvcm()`mit dem
Argument `model = "random"`.
Eine vereinfachte Variante dieses Schätzers ist der *Panel Mean Group Estimator* 
(PMG), welcher lediglich den Durchschnitt der einzelnen OLS-Schätzer
$\hat{\gamma}_{OLS}$ darstellt:

\begin{equation}
\label{eq:pmg}
\hat{\gamma}_{PMG} = \frac{1}{N}\sum_{n=1}^N \hat{\gamma}_{OLS,n}
\end{equation}

In diesem Fall wird die Varianz des Schätzers folgendermaßen geschätzt:

\begin{equation}
\label{eq:pmg-var}
V(\hat{\gamma}_{PMG}) = \frac{1}{N(N-1)}\sum_{n=1}^N(\hat{\gamma}_{OLS,n} - \hat{\gamma}_{PMG})(\hat{\gamma}_{OLS,n} - \hat{\gamma}_{PMG})^T
\end{equation}

Der Vorteil liegt in den weniger restriktiven Annahmen: 
wir nehmen keine bestimmte Verteilung für die $\gamma_n$ an. 
Dafür ist der vereinfachte Schätzer der Varianz in kleinen Samples inkonsistent
wenn die restriktiveren Annahmen von Modell \@ref(eq:swamy-ind) gelten.
Für große $T$ konvergieren die beiden Schätzer allerdings.
Um den PMG-Schätzer zu berechnen verwenden wir die Funktion `plm::pmg()`
mit dem Argument `model="mg"`. 

> **Anwendungsbeispiel: Determinanten von Hauspreisen** Der Beispieldatensatz
enthält Informationen über Hauspreise und generelle Eigenschaften verschiedener
US-Bundesstaaten. Das Interesse von @HollyHouses galt dem Effekt dieser 
Eigenschafte auf die Hauspreise. In diesem Beispiel replizieren wir ihre
Ergebnisse bezüglich des Effekts des Gesamteinkommens.
Um die Ähnlichkeit des Swamy-Modells und dem PMG-Modell zu illustrieren schätzen
wir zunächst beide Modelle:

```{r}
HousePricesUS <- data.table::fread(
  file = here("data/tidy/HousePricesUS.csv"))
HousePricesUS_pdata <- pdata.frame(HousePricesUS)

swamymodel <- pvcm(
  formula = log(price) ~ log(income), 
  data = HousePricesUS, 
  model= "random") 
pmgmodel <- pmg(
  formula = log(price) ~ log(income), 
  data = HousePricesUS, 
  model = "mg") 
```

> Eine Inspektion der geschätzten Koeffizienten zeigt wie ähnlich sie im 
vorliegenden Falle sind:

```{r}
coefs <- cbind(coef(swamymodel), coef(pmgmodel))

dimnames(coefs)[[2]] <- c("Swamy", "PMG")
coefs
```

Der PMG-Schätzer kann übrigens leicht für den dynamischen Kontext angepasst 
werden. Wenn das Panel hier vergleichsweise lang ist (also $T\rightarrow \infty$)
ist dieser Schätzer konsistent sowohl für die Parameter als auch deren 
Standardfehler. 
Das zu schätzende Modell in diesem Falle ist:

\begin{equation}
y_{n,t}=\rho y_{n, t-1} + \delta_n^T x_{n,t} + v_{n,t}
\end{equation}

Auch dieses Modell kann einfach durch die Funktion `plm::pmg()` geschätzt werden,
nur eben mit der gelaggten unabhängigen Variable und ggf. mit einem 
Trend über `trend=TRUE`.

### Cross-Sectional Dependence in Panel-Daten {#sec:panels-csd}

* Loca vs. global

### Tests auf Einheitswurzeln und Ko-Integration {#sec:panel-unitcoint}

Eine besondere Bedeutung bei der Analyse von langen Panels spielt die Frage,
ob die im Panel zusammengefassten Zeitreihen Einheitswurzeln ausweisen und, 
wenn ja, zwischen ihnen eine Ko-Integrationsbeziehung existiert. 
Wenn die Zeitreihen nämlich Einheitswurzeln aufweisen, aber keine
Ko-Integrationsbeziehung existiert, führen Schätzungen zu sehr irreführenden
Ergebnissen. 
Bevor wir uns den einzelnen Konzepten genauer widmen, wollen wir uns die 
Bedeutung des Themas durch eine einfache Simulation vor Augen führen.

Hier $\rho=0.2$:

```{r}
autoreg <- function(rho = 0.1, T = 100){
  e <- rnorm(T)
  for (t in 2:(T)) e[t] <- e[t] + rho *e[t-1]
  e
}

tstat <- function(rho = 0.1, T = 100){
  y <- autoreg(rho, T)
  x <- autoreg(rho, T)
  z <- lm(y ~ x)
  coef(z)[2] / sqrt(diag(vcov(z))[2])
}
result <- c()
R <- 1000
for (i in 1:R) result <- c(result, tstat(rho = 0.2, T = 40))
quantile(result, c(0.025, 0.975))
```

Ergebnis wie erwartet: nur in 5 Prozent *false positives*:

```{r}
prop.table(table(abs(result) > 2))
```

Jetzt den Fall mit einer Einheitswurzel in den Zeitreihen:

```{r}
result <- c()
R <- 1000
for (i in 1:R) result <- c(result, tstat(rho = 1, T = 40))
quantile(result, c(0.025, 0.975))
prop.table(table(abs(result) > 2))
```

Wir sehen hier das Phänomen einer *spurious regression*: 
in einer Mehrheit der Fälle legen unsere Ergebnisse einen signifikanten 
Zusammenhang nahe, der aber definitiv nicht exsitiert.

Wenn man einen klassischen Test verwendet ist das nicht so super:
**ADD EXPLANATION**

```{r}
R <- 1000
T <- 100
result <- c()

for (i in 1:R){
  y <- autoreg(rho=1, T=100) 
  Dy <- y[2:T] - y[1:(T-1)] 
  Ly <- y[1:(T-1)]
  z <- lm(Dy ~ Ly)
  result <- c(result, coef(z)[2] / sqrt(diag(vcov(z))[2]))
}
prop.table(table(result < -1.64))
```

Diese kleine Simulation zeigt, dass das Testen auf Einheitswurzeln und 
eine entsprechende Reaktion absolut entscheidend ist.
Dazu müssen wir als erstes testen ob die relevanten Zeitreihen überhaupt
erst einmal eine Einheitswurzel aufweisen.
Bei den Tests auf Einheitswurzeln unterscheiden wir zwischen so genannten
*First-Generation Tests* und *Second-Generation Tests*.
Während erstere annehmen, dass es keine CSD gibt, sind letztere gegen CSD
robust.
In der Praxis sollten daher wann immer möglich letztere verwendet werden, 
allerdings ist ihre Entwicklung noch ein sehr aktives Forschungsfeld.

Sollte sich herausstellen, dass zwei Zeitreihen 
eine Einheitswurzel aufweisen ist eine Regressionsanalyse mit ihnen nur
möglich wenn sie auch ko-integriert sind.
Für zwei Variablen $x$ und $y$ gehen wir von einer Ko-Integrationsbeziehung
aus, wenn in folgender Regressionsgleichung ein $\beta$ gibt, sodass die
Fehler $\epsilon$ stationär sind:

\begin{align}
y=\alpha + \beta x + \epsilon 
\end{align}\label{eq:cointreg}

Daher ergibt sich folgende Strategie um auf eine Ko-Integrationsbeziehung zu 
testen: 

1. Wir testen ob $x$ und $y$ überhaupt Einheitswurzeln haben
2. Wir schätzen das Modell \@ref(eq:cointreg).
3. Wir testen ob die Residuen $e$ aus Schritt 2 eine Einheitswurzel aufweisen. 
Wenn nein, liegt eine Ko-Integrationsbeziehung.

* ADF - Augmented Dickey Fuller
* CADF - Cross-Sectionally Augmented Dickey Fuller
* IPS - Im, Pesaran, Shin Test
* CIPS - Cross-Sectionally Augmented Im, Pesaran, Shin Test
* CCE - Common Correlated Effects Modell (robust gegen CSD)

```{r}
# data("HousePricesUS", package = "pder") 
HousePricesUS <- data.table::fread(
  file = here("data/tidy/HousePricesUS.csv"))
php <- pdata.frame(HousePricesUS)
cipstest(log(php$price), type = "drift")
cipstest(diff(log(php$price)), type = "none")
cipstest(log(php$income), type = "drift")
cipstest(diff(log(php$income)), type = "none")
```

```{r}
php_ineq <- pdata.frame(gdp_ineq_data, index = c("country", "year"))
cipstest(php_ineq$gdp_pc_chppp, type = "drift")
cipstest(php_ineq$gini_disp, type = "drift")
cipstest(php_ineq$gini_mkt, type = "drift")

cipstest(diff(php_ineq$gdp_pc_chppp), type = "none")
cipstest(diff(php_ineq$gini_disp), type = "none")
cipstest(diff(php_ineq$gini_mkt), type = "none")
```
Hier jetzt die Schätzung durchführen um die Residuen zu erhalten:

```{r}
ccemgmod <- pcce(
  log(price) ~ log(income), 
  data=HousePricesUS, model="mg") 
summary(ccemgmod)
```

```{r}
cipstest(resid(ccemgmod), type="none")
cipstest(resid(ccemgmod), type="none")
```

The unit root hypothesis is rejected for both the residuals of the ccemg and the ccep models. The conclusion is that both models represent cointegrating regressions.

## Example 1: Flechtner Gräbner

```{r}
php_ineq <- pdata.frame(gdp_ineq_data, index = c("country", "year"))
cipstest(log(php_ineq$gdp_pc_chppp), type = "drift")
cipstest(log(php_ineq$gini_disp), type = "drift")
cipstest(log(php_ineq$gini_mkt), type = "drift")

cipstest(diff(log(php_ineq$gdp_pc_chppp)), type = "none")
cipstest(diff(log(php_ineq$gini_disp)), type = "none")
cipstest(diff(log(php_ineq$gini_mkt)), type = "none")
```

```{r}
ccemgmod_disp <- pcce(
  log(gdp_pc_chppp) ~ log(gini_disp), 
  data=gdp_ineq_data, model="mg", index = c("country", "year")) 
summary(ccemgmod_disp)

ccepmod_disp <- pcce(
  log(gdp_pc_chppp) ~ log(gini_disp), 
  data=gdp_ineq_data, model="p", index = c("country", "year")) 
summary(ccepmod_disp)
```

```{r}
cipstest(resid(ccemgmod_disp), type="none")
cipstest(resid(ccepmod_disp), type="none")
```

## Example 2: Kapeller Heimberger

Read in data:

```{r}
kaldor_data <- fread(here("data/intermed/work_data_act_corr.csv"))
php_kaldor <- pdata.frame(kaldor_data, index = c("ccode", "year"))
```

Überprüfung für die Variablen: logEXP, logRULC, logTECH
**1. Test auf Unit roots**

```{r}
cipstest(log(php_kaldor$EXP), type = "drift")
cipstest(log(php_kaldor$RULC), type = "drift")
#cipstest(log(php_kaldor$TECH), type = "drift")

cipstest(diff(log(php_kaldor$EXP)), type = "none")
cipstest(diff(log(php_kaldor$RULC)), type = "none")
#cipstest(diff(log(php_kaldor$TECH)), type = "none")
```


**2. Schätzung der cce models**:

* Problem: `TECH` geht nicht mit Log

```{r}
ccemgmod_kaldor <- pcce(
  log(EXP) ~ log(RULC) ,# + log(TECH)
  data=kaldor_data, model="mg", index = c("ccode", "year")) 
summary(ccemgmod_disp)

ccepmod_kaldor <- pcce(
  log(EXP) ~ log(RULC) ,# + log(TECH)
  data=kaldor_data, model="p", index = c("ccode", "year")) 
summary(ccepmod_disp)
```

**3. Ko-Integration**

```{r}
cipstest(resid(ccemgmod_kaldor), type="none")
cipstest(resid(ccepmod_kaldor), type="none")
```

