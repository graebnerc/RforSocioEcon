\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={R für die sozio-ökonomische Forschung},
            pdfauthor={Dr.~Claudius Gräbner},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{R für die sozio-ökonomische Forschung}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \subtitle{Version 0.4.0}
  \author{\href{http://claudius-graebner.com/}{Dr.~Claudius Gräbner}}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-11-06}

\usepackage{booktabs}
\usepackage{amsthm, amssymb, mathrsfs}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = {blue},
    citecolor = {violet}}
\usepackage[a4paper,width=150mm,top=20mm,bottom=20mm,left=20mm,right=20mm,bindingoffset=6mm]{geometry}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Willkommen}\label{willkommen}
\addcontentsline{toc}{chapter}{Willkommen}

Dieses Skript ist als Begleitung für die Lehrveranstaltung
``Wissenschaftstheorie und Einführung in die Methoden der
Sozioökonomie'' im Master ``Sozioökonomie'' an der Universität
Duisburg-Essen gedacht.

Es enthält grundlegende Informationen über die Funktion der
Programmiersprache R \citep{R-Team}.

\section*{Verhältnis zur Vorlesung}\label{verhaltnis-zur-vorlesung}
\addcontentsline{toc}{section}{Verhältnis zur Vorlesung}

Einige Kapitel beziehen sich unmittelbar auf bestimmte
Vorlesungstermine, andere sind als optionale Zusatzinformation gedacht.
Gerade Menschen ohne Vorkenntnisse in R sollten unbedingt die ersten
Kapitel vor dem vierten Vorlesungsterm lesen und verstehen. Bei Fragen
können Sie sich gerne an Claudius Gräbner wenden.

Die folgende Tabelle gibt einen Überblick über die Kapitel und die
dazugehörigen Vorlesungstermine:

\begin{longtable}[]{@{}clc@{}}
\toprule
\begin{minipage}[b]{0.24\columnwidth}\centering\strut
Kapitel\strut
\end{minipage} & \begin{minipage}[b]{0.34\columnwidth}\raggedright\strut
Zentrale Inhalte\strut
\end{minipage} & \begin{minipage}[b]{0.34\columnwidth}\centering\strut
Verwandter Vorlesungstermin\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
1: Vorbemerkungen\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Gründe für R; Besonderheiten von R\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Vorbereitung\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
2: Vorbereitung\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Installation und Einrichtung von R und R Studio,
Projektstrukturierung\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Vorbereitung\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
3: Erste Schritte in R\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Grundlegende Funktionen von R; Objekte in R; Pakete\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Vorbereitung\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
4: Ökonometrie I\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Implementierung von uni- und multivariaten linearen
Regressionsmodellen\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
T4 am 06.11.19\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
5: Datenaquise und -management\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Einlesen und Schreiben sowie Manipulation von Datensätzen; deskriptive
Statistik\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
T8 am 11.12.19\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
6: Visualisierung\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Erstellen von Grafiken\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
T8 am 11.12.19\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
7: Ökonometrie II\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Mehr Konzepte der Ökonometrie\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
T9-10 am 8.\&15.1.20\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
8: Ausblick\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Ausblick zu weiteren Anwendungsmöglichkeiten\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Optional\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
A: Einführung in Markdown\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Wissenschaftliche Texte in R Markdown schreiben\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Optional; relevant für Aufgabenblätter\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
B: Wiederholung: Wahrscheinlichkeitstheorie\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Wiederholung grundlegender Konzepte der Wahrscheinlichkeitstheorie und
ihrer Implementierung in R\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Optional; wird für die quantitativen VL vorausgesetzt\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
C: Wiederholung: Deskriptive Statistik\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Wiederholung grundlegender Konzepte der deskriptiven Statistik und ihrer
Implementierung in R\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Optional; wird für die quantitativen VL vorausgesetzt\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
C: Wiederholung: Drei grundlegende Verfahren der schließenden
Statistik\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Wiederholung von Parameterschätzung, Hypothesentests und
Konfidenzintervalle und deren Implementierung in R\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Optional; wird für die quantitativen VL vorausgesetzt\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
E: Einführung in Git und Github\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\raggedright\strut
Verwendung von Git und Github\strut
\end{minipage} & \begin{minipage}[t]{0.34\columnwidth}\centering\strut
Optional\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Das Skript ist \emph{work in progress} und jegliches Feedback ist sehr
willkommen. Dafür wird im Moodle ein extra Bereich eingerichtet.

\section*{Änderungshistorie während des
Semesters}\label{anderungshistorie-wahrend-des-semesters}
\addcontentsline{toc}{section}{Änderungshistorie während des Semesters}

\emph{An dieser Stelle werden alle wichtigen Updates des Skripts
gesammelt.} \emph{Die Versionsnummer hat folgende Struktur:
\texttt{major}.\texttt{minor}.\texttt{patch}} \emph{Neue Kapitel erhöhen
die \texttt{minor} Stelle, kleinere, aber signifikante}
\emph{Korrekturen werden als Patches gekennzeichnet.}

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.13\columnwidth}\raggedright\strut
Datum\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedright\strut
Version\strut
\end{minipage} & \begin{minipage}[b]{0.41\columnwidth}\raggedright\strut
Wichtigste Änderungen\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
19.10.19\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
0.1.0\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
Erste Version veröffentlicht\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
03.11.19\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
0.2.0\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
Markdown-Anhang hinzugefügt\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
04.11.19\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
0.3.0\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
Anhänge zur Wiederholung grundlegender Statistik hinzugefügt\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
06.11.19\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
0.4.0\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright\strut
Kapitel zu linearen Modellen hinzugefügt\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\section*{Lizenz}\label{lizenz}
\addcontentsline{toc}{section}{Lizenz}

\begin{center}\includegraphics[width=0.2\linewidth]{figures/license} \end{center}

Das gesamte Skript ist unter der
\href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative
Commons Attribution-NonCommercial-ShareAlike 4.0 International License}
lizensiert.

\chapter{Vorbemerkungen}\label{precons}

\section{Warum R?}\label{warum-r}

Im folgenden gebe ich einen kurzen Überblick über die Gründe, die uns
bewegt haben den Methodenkurs auf R aufzubauen. Die Liste ist sicherlich
nicht abschließend (siehe auch \citet{adv-r}).

\begin{itemize}
\item
  Die R Community gilt als besonders freundlich und hilfsbereit. Gerade
  weil viele Menschen, die R benutzen praktizierende
  Datenwissenschaftler*innen sind werden praktische Probleme breit und
  konstruktiv in den einschlägigen Foren diskutiert und es ist in der
  Regel leicht Lösungen für Probleme zu finde, sobald man selbst ein
  bestimmtes Level an Programmierkenntnissen erlangt hat.

  \begin{itemize}
  \tightlist
  \item
    Auch gibt es großartige Online Foren und Newsletter, die es einem
    einfacher und unterhaltsamer machen, seine R Kenntnisse stetig zu
    verbessern und zusätzlich viele neue Dinge zu lernen. Besonders
    empfehlen kann ich \href{https://www.r-bloggers.com/}{R-Bloggers},
    eine Sammlung von Blog Artikeln, die R verwenden und neben
    Inspirationen für die Verwendung von R häufig inhaltlich sehr
    interessant sind; \href{https://rweekly.org/}{rweekly}, ein
    Newsletter, der ebenfalls interessante Infos zu R enthält sowie die
    \href{https://rladies.org/}{R-Ladies Community}, die sich besonders
    das Empowerment von Minderheiten in der Programmierwelt zur Aufgabe
    gemacht hat.
  \item
    Selbstverständlich werden zahlreiche R Probleme auch auf
    \href{https://stackoverflow.com/tags/r/info}{StackOverflow}
    disktuiert. Häufig ist das der Ort, an dem man Antworten auf seine
    Fragen findet. Allerdings ist es gerade am Anfang unter Umständen
    schwierig die häufig sehr fortgeschrittenen Lösungen zu verstehen.
  \end{itemize}
\item
  R ist eine offene und freie Programmiersprache, die auf allen
  bekannten Betriebssystemen läuft. Im Gegensatz zu Programmen wie SPSS
  und STATA, für die Universitäten jedes Semester viele Tausend Euro
  bezahlen müssen und die dann umständlich über Serverlizenzen abgerufen
  werden müssen. Auch für Studierende sind die Preise alles andere als
  gering. R dagegen ist frei und inklusiv, und auch Menschen mit weniger
  Geld können sie benutzen. Gerade vor dem Hintergrund der Rolle von
  Wissenschaft in einer demokratischen und freien Gesellschaft und in
  der Kooperation mit Wissenschaftler*innen aus ärmeren Ländern ist dies
  extrem wichtig.
\item
  R verfügt über ein hervorragendes Package System. Das bedeutet, dass
  es recht einfach ist, neue Pakete zu schreiben und damit die
  Funktionalitäten von R zu erweitern. In der Kombination mit der Open
  Source Charakter von R bedeutet das, dass R nie wirklich \emph{out of
  date} ist, und dass neuere Entwicklungen der Statistik und
  Datenwissenschaften, und immer mehr auch in der VWL, recht zügig in R
  implementiert werden. Insbesondere wenn es um statistische Analysen,
  \emph{machine learning}, Visualisierungen oder Datenmanagement und
  -manipulation geht: für alles gibt es Pakete in R und irgendjemand hat
  ihr Problem mit hoher Wahrscheinlichkeit schon einmal gelöst und Sie
  können davon profitieren.

  \begin{itemize}
  \tightlist
  \item
    R ist - zusammen mit Python - mittlerweile die \emph{lingua franca}
    im Bereich Statistik und Machine Learning.
  \end{itemize}
\item
  Integration mit Git, Markdown, Latex und anderen Tools erlaubt einen
  integrierten Workflow, in dem Sie im Optimalfall euer Paper in der
  gleichen Umgebung schreiben wie den Code für eure statistische
  Analyse. Diesen Vorteil werden Sie bereits bei der Bearbeitung der
  Aufgabenzettel genießen können, da diese in teilweise in R Markdown zu
  lösen und abzugeben sind. Das bedeutet, dass Coding und Schreiben der
  Antworten im gleichen Dokument vorgenommen werden können. Auch dieses
  Skript wurde vollständig in R Markdown geschrieben.
\item
  R erlaubt sowohl objektorientierte als auch funktionale
  Programmierung.
\item
  Für besondere Aufgaben ist es recht einfach R mit high-performance
  Sprachen wie C, Fortran oder C++ zu integrieren.
\end{itemize}

\section{Besonderheiten von R}\label{besonderheiten-von-r}

R ist keine typische Programmiersprache, denn sie vor allem von
Statistiker*innen benutzt und weiterentwickelt, und nicht von
Programmierer*innen. Das hat den Vorteil, dass die Funktionen oft sehr
genau auf praktische Herausforderungen ausgerichtet sind und es für alle
typischen statistischen Probleme Lösungen in R gibt. Gleichzeitig hat
dies auch dazu geführt, dass R einige unerwünschte Eigenschaften
aufweist, da die Menschen, die Module für R programmieren keine
`genuinen' Programmierer*innen sind.

Im folgenden möchte ich einige Besonderheiten von R aufführen, damit Sie
im Laufe Ihrer R-Karriere nicht negativ von diesen Besonderheiten
überrascht werden. Während es sich für Programmier-Neulinge empfiehlt
die Liste zu einem späteren Zeitpunkt zu inspizieren sollten Menschen
mit Erfahrungen in anderen Sprachen gleich einen Blick darauf werfen.

\begin{itemize}
\item
  R wird dezentral über viele benutzergeschriebene Pakete (`libraries'
  oder `packages') konstant weiterentwickelt. Das führt wie oben erwähnt
  dazu, dass R quasi immer auf dem neuesten Stand der statistischen
  Forschung ist. Gleichzeitig kann die schiere Masse von Paketen auch
  verwirrend sein, insbesondere weil es für die gleiche Aufgabe häufig
  deutlich mehr als ein Paket gibt. Das führt zwar auch zu einer
  positiven Konkurrenz und jede*r kann sich ihren Geschmäckern gemäß für
  das eine oder andere Paket entscheiden, es bringt aber auch mögliche
  Inkonsistenzen und schwerer verständlichen Code mit sich.
\item
  Im Gegensatz zu Sprachen wie Python, die trotz einer enormen Anzahl
  von Paketen eine interne Konsistenz nicht verloren haben gibt es in R
  verschiedene `Dialekte', die teilweise inkonsistent sind und gerade
  für Anfägner durchaus verwirrend sein können. Besonders die
  Unterscheidungen des \texttt{tidyverse}, einer Gruppe von Paketen, die
  von der R Studio Company sehr stark gepusht werden und vor allem zur
  Verarbeitung von Datensätzen gedacht sind, implementieren viele
  Routinen des `klassischen R' (`base R') in einer neuen Art und Weise.
  Das Ziel ist, die Arbeit mit Datensätzen einfacher und leichter
  verständlich zu machen, allerdings wird die recht aggressive
  `Vermarktung' und die teilweise inferiore Performance des Ansatzes
  auch
  \href{https://github.com/matloff/TidyverseSkeptic}{kritisiert}.\footnote{Zum
    einen bin ich ein großer Fan von vielen tidyverse Paketen,
    gleichzeitig ist der Fokus von R Studio auf diese Pakete sehr
    gefährlich. Ich bin aber einer anderen Meinung was die
    Einsteigerfreundlichkeit vom \texttt{tidyverse} andgeht: meiner
    Meinung nach machen diese Pakete die Arbeit mit Datensätzen sehr
    einfach, und für kleine Datensätze (\textless{}500MB) benutze ich
    das \texttt{tidyverse} auch in meiner eigenen Forschung. Aufgrund
    der Einsteigerfreundlichkeit werden wir hier für die Arbeit mit
    Datensätzen trotz allem mit dem \texttt{tidyverse} arbeiten. Ich
    weise jedoch auf die kritische Diskussion im entsprechenden Kapitel
    des Skripts hin.}
\item
  Da viele der Menschen, die R Pakete herstellen keine Programmierer
  sind, sind viele Pakete von einem Programmierstandpunkt aus nicht
  sonderlich effizient oder elegant geschrieben. Gleichzeitig gibt es
  aber auch viele Ausnahmen zu dieser Regel und viele Pakete werden über
  die Zeit hinweg signifikant verbessert.
\item
  R an sich ist nicht die schnellste Programmiersprache, insbesondere
  wenn man seinen Code nicht entsprechend geschrieben hat. Auch bedarf
  eine R Session in der Regel recht viel Speicher. Hier sind selbst
  andere High-Level Sprachen wie Julia oder Python deutlich
  performanter, auch wenn Pakete wie
  \href{https://rdatatable.gitlab.io/data.table/}{data.table} diesen
  Nachteil häufig abschwächen. Zudem ist er für die meisten Probleme,
  die Sozioökonom*innen in ihrer Forschungspraxis bearbeiten,
  irrelevant.
\end{itemize}

Alles in allem ist R jedoch eine hervorragende Wahl wenn es um
quantitative sozialwissenschaftliche Forschung geht. Auch in der
Industrie ist R extrem beliebt und wird im Bereich der \emph{Data
Science} nur noch von Python ernsthaft in den Schatten gestellt.
Allerdings verwenden die meisten Menschen, die in diesem Bereich
arbeiten, ohnehin beide Sprachen, da sie unterschiedliche Vor- und
Nachteile haben. Entsprechend ist jede Minute, die Sie in das Lernen von
R investieren eine exzellente Investition, egal wo Sie in Ihrem späteren
Berufsleben einmal landen werden.

Das wichtigste am Programmieren ist in jedem Fall Spaß und die
Bereitschaft zu und die Freude an der Zusammenarbeit mit anderen. Denn
das hat R mit anderen offenen Sprachen wie Python gemeinsam:
Programmieren und das Lösen von statistischen Fragestellungen sollte
immer ein kollaboratives Gemeinschaftsprojekt sein!

\chapter{Einrichtung}\label{einrichtung}

\section{Installation von R und
R-Studio}\label{installation-von-r-und-r-studio}

Die Installation von R ist in der Regel unproblematisch. Auf der
\href{https://www.r-project.org/}{R homepage} wählt man unter dem Reiter
`Download' den Link `CRAN' aus, wählt einen Server in der Nähe und lädt
sich dann die R Software herunter. Danach folgt man den
Installationshinweisen.

Im zweiten Schritt muss noch das Programm `R-Studio' installiert werden.
Hierbei handelt es sich um eine grafische Oberfläche für R, welche uns
die Arbeit enorm erleichtern wird. Das Programm kann
\href{https://www.rstudio.com/products/rstudio/download/}{hier}
heruntergeladen werden. Bitte darauf achten `RStuio Desktop' zu
installieren.

\section{Die R Studio Oberfläche}\label{die-r-studio-oberflache}

Nach dem Installationsprozess öffnen wir R Studio zum ersten Mal. Der
folgende Screenshot zeigt die verschiedenen Elemente der Oberfläche,
deren Funktion im folgenden kurz erläutert wird. Vieles ergibt sich hier
aber auch durch \emph{learning by doing}. Im folgenden werden nur die
Bereiche der Oberfläche beschrieben, die am Anfang unmittelbar relevant
für uns sind.

\begin{center}\includegraphics[width=1\linewidth]{figures/r-studio-light-marked} \end{center}

\begin{itemize}
\item
  Der \textbf{Skriptbereich} (1) ist ein Texteditor wie Notepad - nur
  mit zusätzlichen Features wie Syntax Highlighting für R, sodass es uns
  leichter fällt R Code zu schreiben. Hier werden wir unsere Skripte
  verfassen.
\item
  Die \textbf{Konsole} (2) erlaubt es uns über R direkt mit unserem
  Computer zu interagieren. R ist eine Programmiersprache. Das bedeutet,
  wenn wir den Regeln der Sprache folgen und uns in einer für den
  Computer verständlicher Art und Weise ausdrücken, versteht der
  Computer was wir von ihm wollen und führt unsere Befehle aus. Wenn wir
  in die Konsole z.B. \texttt{2+2} eingeben, dann ist das valider R
  code. Wenn wir dann Enter drücken versteht der Computer unseren Befehl
  und führt die Berechnung aus. Die Konsole ist sehr praktisch um den
  Effekt von R Code direkt zu beobachten. Wenn wir etwas in der Console
  ausführen wollen, das wir vorher im \textbf{Skriptbereich} geschrieben
  haben, können wir den Text markieren und dann auf den Button
  \texttt{Run} (3) drücken: dann kopiert R Studio den Code in die
  Konsole und führt ihn aus.
\item
  Für den Bereich oben rechts haben wir in der Standardkonfiguration von
  R Studio drei Optionen, die wir durch Klicken auf die Reiter auswählen
  können. Der Reiter \textbf{Environment} (4) zeigt uns alle bisher
  definierten Objekte an (mehr dazu später). Der Reiter \textbf{History}
  (5) zeigt an, welchen Code wir in der Vergangenheit ausgeführt haben.
  Der Reiter \textbf{Connections} (6) braucht uns aktuell nicht zu
  interessieren.
\item
  Auch für den Bereich unten rechts haben wir mehrere Optionen: Der
  Bereich \textbf{Files} (7) zeigt uns unser Arbeitsverzeichnis mit
  allen Ordnern und Dateien an. Das ist das gleiche, was wir auch über
  den File Explorer unserer Betriebssystems sehen würden. Der Bereich
  \textbf{Plots} (8) zeigt uns eine Vorschau der Abbildungen, die wir
  durch unseren Code produzieren. Die anderen Bereiche brauchen uns
  aktuell noch nicht zu interessieren.
\item
  Wenn wir ein neues R Skript erstellen wollen, können wir das über den
  Button \textbf{Neu} (9) erledigen. Wir klicken darauf und wählen die
  Option `R Skript'. Mit den alternativen Dateiformaten brauchen wir uns
  aktuell nicht beschäftigen.
\item
  Der Botton \textbf{Neues Projekt anlegen} (10) erstellt eine neues R
  Studio Projekt - mehr dazu in Kürze.
\item
  Der Button \textbf{Öffnen} (11) öffnet Dateien im Skriptbereich.
\item
  Die beiden Buttons \textbf{Speichern} (12) und \textbf{Alles
  speichern} (13) speichern das aktuelle, bzw. alle im Skriptbereich
  geöffnenten Dateien.
\end{itemize}

Die restlichen Buttons und Fenster in R Studio werden wir im Laufe der
Zeit kennenlernen.

Es macht Sinn, sich einmal die möglichen Einstellungsmöglichkeiten für R
Studio anzuschauen und ggf. eine andere Darstellungsversion zu wählen.

\section{Einrichtung eines R
Projekts}\label{einrichtung-eines-r-projekts}

Im folgenden werden wir lernen wie man ein neues R Projekt anlegt, R
Code schreiben und ausführen kann.

Wann immer wir ein neues Programmierprojekt starten, sollten wir dafür
einen eigenen Ordner anlegen und ein so genannten `R Studio Projekt'
erstellen. Das hilft uns den Überblick über unsere Arbeit zu behalten,
und macht es einfach Code untereinander auszutauschen.

Ein Programmierprojekt kann ein Projekt für eine Hausarbeit sein, die
Mitschriften für eine Vorlesungseinheit, oder einfach der Versuch ein
bestimmtes Problem zu lösen, z.B. einen Datensatz zu visualisieren.

Die Schritte zur Erstellung eines solchen Projekts sind immer die
gleichen:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Einen Ordner für das Projekt anlegen.
\item
  Ein R-Studio Projekt in diesem Ordner erstellen.
\item
  Relevante Unterordner anlegen.
\end{enumerate}

Wir beschäftigen uns mit den Schritten gleich im Detail, müssen vorher
aber noch die folgenden Konzepte diskutieren: (1) das Konzept eines
\emph{Arbeitsverzeichnisses} (\emph{working directory}) und (2) die
Unterscheidnug zwischen \emph{absoluten} und \emph{relativen} Pfaden.

\subsection{Arbeitsverzeichnisse und
Pfade}\label{arbeitsverzeichnisse-und-pfade}

Das \textbf{Arbeitsverzeichnis} ist ein Ordner auf dem Computer, in dem
R standardmäßig sämtlichen Output speichert und von dem aus es auf
Datensätze und anderen Input zugreift. Wenn wir mit Projekten arbeiten
ist das Arbeitsverzeichnis der Ordner, in dem das R-Projektfile abgelegt
ist, ansonsten ist es euer Benutzerverzeichnis. Wir können uns das
Arbeitsverzeichnis mit der Funktion \texttt{getwd()} anzeigen lassen. In
meinem Fall ist das Arbeitsverzeichnis das folgende:

\begin{verbatim}
#> [1] "/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR"
\end{verbatim}

Wenn ich R nun sagen würde ein File unter dem Namen \texttt{test.pdf}
speichern, würde es am folgenden Ort gespeichert werden:

\begin{verbatim}
#> [1] "/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR/test.pdf"
\end{verbatim}

R geht in einem solchen Fall immer vom Arbeitsverzeichnis aus. Da wir im
vorliegenden Fall den Speicherort relativ zum Arbeitsverzeichnis
angegeben haben, sprechen wir hier von einem \textbf{relativen Pfad}.

Alternativ können wir den Speicherort auch als \textbf{absoluten Pfad}
angeben. In diesem Fall geben wir den kompletten Pfad, ausgehend vom
\href{https://de.wikipedia.org/wiki/Stammverzeichnis}{Root Verzeichnis}
des Computers, an. Wir würden R also \emph{explizit} auffordern, das
File an foldengem Ort zu speichern:

\begin{verbatim}
#> [1] "/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR/test.pdf"
\end{verbatim}

Wir werden hier \textbf{immer} relative Pfade verwenden. Relative Pfade
sind fast immer die bessere Variante, da es uns erlaubt den gleichen
Code auf verschiedenen Computern zu verwenden. Denn wiw man an den
absoluten Pfaden erkennen kann, sehen diese auf jedem Computer anders
aus und es ist dementsprechend schwierig, Code miteinander zu teilen.

Wir lernen mehr über dieses Thema wenn wir uns später mit Dateninput und
-output beschäftigen.

\subsection{Schritt 1: Projektordner
anlegen}\label{schritt-1-projektordner-anlegen}

Zuerst müssen Sie sich für einen Ordner auf Ihrem Computer entscheiden,
in dem alle Daten, die mit ihrem Projekt zu tun haben, also Daten,
Skripte, Abbildungen, etc. gespeichert werden sollen und diesen Ordner
gegebenenfalls neu erstellen. Es macht Sinn, einen solchen Ordner mit
einem informativen Namen ohne Leer- und Sonderzeichen zu versehen, z.B.
\texttt{SoSe19-Methodenkurs}.

Dieser Schritt kann theoretisch auch gemeinsam mit Schritt 2 erfolgen.

\subsection{Schritt 2: Ein R-Studio Projekt im Projektordner
erstellen}\label{schritt-2-ein-r-studio-projekt-im-projektordner-erstellen}

Wir möchten nun R Studio mitteilen den in Schritt 1 erstellten Ordner
als R Projekt zu behandeln. Damit wird nicht nur dieses Ordner als
Root-Verzeichnis festgelegt, man kann auch die Arbeitshistorie eines
Projekts leich wiederherstellen und es ist einfacher, das Projekt auf
verschiedenen Computern zu bearbeiten.

Um ein neues Projekt zu erstellen klicken Sie in R Studio auf den Button
\texttt{Neues\ Projekt} (Nr. 10 in der obigen Abbildung) und Sie sollten
folgendes Fenster sehen:

\begin{center}\includegraphics[width=0.6\linewidth]{figures/r-studio-new-project} \end{center}

Falls Sie in Schritt 1 den Projektordner bereits erstellt haben wählen
Sie hier \texttt{Existing\ Directory}, ansonsten erstellen Sie einen
neuen Projektordner gleich mit dem Projektfile mit indem Sie
\texttt{New\ Directory} auswählen.

Falls Sie \texttt{Existing\ Directory} gewählt haben, wählen Sie in
folgendem Fenster einfach den vorher erstellten Ordner aus und klickt
auf \texttt{Create\ Project}.

\begin{center}\includegraphics[width=0.6\linewidth]{figures/r-studio-new-project-exis-dir} \end{center}

Falls Sie \texttt{New\ Directory} gewählt habt landen Sie auf folgendem
Fenster:

\begin{center}\includegraphics[width=0.6\linewidth]{figures/r-studio-new-project-new-dir} \end{center}

Hier wählen Sie \texttt{New\ Project} aus, geben dem Projekt in
folgenden Fenster einen Namen (das wird der Name des Projektordners
sein), wählen den Speicherort für den Ordner aus und klicken auf
\texttt{Create\ Project}.

In beiden Fällen wurde nun ein Ordner erstellt, in dem sich ein File
\texttt{*.Rproj} befindet. Damit ist die formale Erstellung eines
Projekts abgeschlossen. Es empfiehlt sich jedoch dringend gleich eine
sinnvolle Unterordnerstruktur mit anzulegen.

\hypertarget{unterordner}{\subsection{Schritt 3: Relevante Unterordner
erstellen}\label{unterordner}}

Eine sinnvolle Unterordnerstruktur hilf (1) den Überblick über das
eigene Projekt nicht zu verlieren, (2) mit anderen über verschiedene
Computer hinweg zu kollaborieren und (3) Kollaborationsplattformen wie
Github zu verwenden und replizierbare und für andere nachvollziehbare
Forschungsarbeit zu betreiben.

Die folgende Ordnerstruktur ist eine Empfehlung. In manchen Projekten
werden Sie nicht alle hier vorgeschlagenen Unterordner brauchen, in
anderen bietet sich die Verwendung von mehr Unterordnern an.
Nichtsdestotrotz ist es ein guter Ausgangspunkt, den ich in den meisten
meiner Forschungsprojekte auch so verwende.

Insgesamt sollten die folgenden Ordner im Projektordner erstellt werden:

\begin{itemize}
\tightlist
\item
  Ein Ordner \texttt{data}, der alle Daten enthält, die im Rahmen des
  Projekts verwendet werden. Hier empfiehlt es sich zwei Unterordner
  anzulegen: Einen Ordner \texttt{raw}, der die Rohdaten enthält, so wie
  sie aus dem Internet runtergeladen wurden. Diese Rohdaten sollten
  \textbf{niemals} verändert werden, ansonsten wird Ihre Arbeit nicht
  vollständig replizierbar werden und es kommt ggf. zu irreparablen
  Schäden. Alle Veränderungen der Daten sollten durch Skripte
  dokumentiert werdenn, die die Rohdaten als Input, und einen
  modifizierten Datensatz als Output generieren. Dieser modifizierte
  Datensatz sollte dann im Unterordner \texttt{tidy} gespeichert werden.
\end{itemize}

\begin{quote}
Beispiel: Sie laden sich Daten zum BIP in Deutschland von Eurostat und
Daten zu Arbeitslosigkeit von AMECO herunter. Beiden Datensätze sollten
im Unterordner \texttt{data/raw} gespeichert werden. Mit einem Skript
lesen Sie beide Datensätze ein und erstellen den kombinierten Datensatz
\texttt{macro\_data.csv}, den Sie im Ordner \texttt{data/tidy} speichern
und für die weitere Analyse verwenden. Dadurch kann jede*r
nachvollziehen wie die von Ihnen verwendeten Daten sich aus den Rohdaten
ergeben haben und Ihre Arbeit bleibt komplett transparent.
\end{quote}

\begin{itemize}
\item
  Ein Ordner \texttt{R}, der alle R Skripte enthält, also alle
  Textdokumente, die R Code enthalten.
\item
  Ein Ordner \texttt{output}, in dem der Output ihrer Berechnungen, z.B.
  Tabellen oder Plots gespeichert werden können. Der Inhalt dieses
  Ordners sollte sich komplett mit den Inhalten der Ordner \texttt{data}
  und \texttt{R} replizieren lassen.
\item
  Ein Ordner \texttt{text}, in dem Sie Ihre Verschriftlichungen
  speichern, z.B. das eigentliche Forschungspapier, ihre Hausarbeit oder
  Ihre Vorlesungsmitschriften.
\item
  Einen Ordner \texttt{misc} in den Sie alles packen, was in keinen der
  anderen Ordner passt. Ein solcher Ordner ist wichtig und Sie sollten
  nicht zuordbare Dateien nie in den Projektordner als solchen
  speichern.
\end{itemize}

Wenn wir annehmen unser Projektordner heißt \texttt{2019-Methoden}
ergibt sich damit insgesamt folgende Ordner und Datenstruktur:

\begin{center}\includegraphics{figures/wd-structure} \end{center}

\section{Abschließende Bemerkungen}\label{abschlieende-bemerkungen}

Eine gute Ordnerstruktur ist nicht nur absolut essenziell um selbst
einen Überblick über seine Forschungsprojekte zu behalten, sondern auch
wenn man mit anderen Menschen kollaborieren möchte. In einem solchen
Fall sollte man auf jeden Fall eine Versionskontrolle wie Git und GitHub
verwenden. Wir werden uns damit im nächsten Semester genauer
beschäftigen, aber Sie werden merken, dass die Kollaboration durch eine
gut durchdachte Ordnerstruktur massiv erleichtert wird.

\hypertarget{basics}{\chapter{Erste Schritte in R}\label{basics}}

Nach diesen (wichtigen) Vorbereitungsschritten wollen wir nun mit dem
eigentlichen Programmieren anfangen. Zu diesem Zweck müssen wir uns mit
der Syntax von R vertraut machen, also mit den Regeln, denen wir folgen
müssen, wenn wir Code schreiben, damit der Computer versteht, was wir
ihm eigentlich in R sagen wollen.

\section{Befehle in R an den Computer
übermitteln}\label{befehle-in-r-an-den-computer-ubermitteln}

Grundsätzlich können wir über R Studio auf zwei Arten mit dem Computer
``kommunizieren'': über die Konsole direkt, oder indem wir im
Skriptbereich ein Skript schreiben und dies dann ausführen.

Als Beispiel für die erste Möglichkeit wollen wir mit Hilfe von R die
Zahlen \texttt{2} und \texttt{5} miteinander addieren. Zu diesem Zweck
können wir einfach \texttt{2\ +\ 2} in die Konsole eingeben, und den
Befehl mit `Enter' an den Computer senden. Da es sich beim Ausdruck
\texttt{2\ +\ 3} um korrekten R Code handelt, `versteht' der Computer
was wir von ihm wollen und gibt uns das entsprechende Ergebnis aus:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 5
\end{verbatim}

Auf diese Art und Weise könne wir R als einfachen Taschenrechner
verwenden, denn für alle einfachen mathematischen Operationen können wir
bestimmte Symbole als Operatoren verwenden. An dieser Stelle sei noch
darauf hingewiesen, dass das Symbol \texttt{\#} in R einen Kommentar
einleitet, das heißt alles was in einer Zeile nach \texttt{\#} steht
wird vom Computer ignoriert und man kann sich einfach Notizen in seinem
Code machen.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{2} \CommentTok{# Addition}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{/}\DecValTok{2} \CommentTok{# Division}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{4}\OperatorTok{*}\DecValTok{2} \CommentTok{# Multiplikation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{3}\OperatorTok{**}\DecValTok{2} \CommentTok{# Potenzierung}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 9
\end{verbatim}

Alternativ können wir die Befehle in einem Skript aufschreiben, und
dieses Skript dann ausführen. Während die Interaktion über die Konsole
sinnvoll ist um die Effekte bestimmter Befehle auszuprobieren, bietet
sich die Verwendung von Skripten an, wenn wir mit den Befehlen später
weiter arbeiten wollen, oder sie anderen Menschen zugänglich zu machen.
Den das Skript können wir als Datei auf unserem Computer speichern,
vorzugsweise im Unterordner \texttt{R} unseres R-Projekts (siehe
Abschnitt \protect\hyperlink{unterordner}{Relevante Unterordner
erstellen}), und dann später weiterverwenden.

Die Berechnungen, die wir bisland durchgeführt haben sind
zugegebenermaßen nicht sonderlich spannend. Um fortgeschrittene
Operationen in R durchführen und verstehen zu können müssen wir uns
zunächst mit den Konzepten von \textbf{Objekten}, \textbf{Funktionen}
und \textbf{Zuweisungen} beschäftigen.

\section{Objekte, Funktionen und
Zuweisungen}\label{objekte-funktionen-und-zuweisungen}

\begin{quote}
To understand computations in R, two slogans are helpful: Everything
that exists is an object. Everything that happens is a function call.
---John Chambers
\end{quote}

Mit der Aussage `Alles in R ist ein Objekt' ist gemeint, dass jede Zahl,
jede Funktion, oder jeder Buchstabe in R ein Objekt ist, das irgendwo
auf dem Speicher Ihres Rechners abgespeichert ist.

In der Berechnung \texttt{2\ +\ 3} ist die Zahl \texttt{2} genauso ein
Objekt wie die Zahl \texttt{3} und die Additionsfunktion, die durch den
Operator \texttt{+} aufgerufen wird.

Mit der Aussage `Alles was in R passiert ist ein Funktionsaufruf' ist
gemeint, dass wenn wir R eine Berechnung durchführen lassen, tun wir
dies indem wir eine Funktion aufrufen.

\textbf{Funktionen} sind Algorithmen, die bestimmte Routinen auf einen
\emph{Input} anwenden und dabei einen \emph{Output} produzieren. Die
Additionsfunktion, die wir in der Berechnung \texttt{2\ +\ 3} aufgerufen
haben hat als Input die beiden Zahlen \texttt{2} und \texttt{3}
aufgenommen, hat auf sie die Routine der Addition angewandt und als
Output die Zahl \texttt{5} ausgegeben. Der Output \texttt{5} ist dabei
in R genauso ein Objekt wie die Inputs \texttt{2} und \texttt{3}, sowie
die Funktion \texttt{+}.

Ein `Problem' ist, dass R im vorliegenden Falle den Output der
Berechnung zwar ausgibt, wir danach aber keinen Zugriff darauf mehr
haben:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 5
\end{verbatim}

Falls wir den Output weiterverwenden wollen, macht es Sinn, dem Output
Objekt einen Namen zu geben, damit wir später wieder darauf zugreifen
können. Der Prozess einem Objekt einen Namen zu Geben wird
\textbf{Zuweisung} oder \textbf{Assignment} genannt und durch die
Funktion \texttt{assign} vorgenommen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{assign}\NormalTok{(}\StringTok{"zwischenergebnis"}\NormalTok{, }\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wir können nun das Ergebnis der Berechnung \texttt{2\ +\ 3} aufrufen,
indem wir in R den Namen des Output Objekts eingeben:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zwischenergebnis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 5
\end{verbatim}

Da Zuweisungen so eine große Rolle spielen und sehr häufig vorkommen
gibt es auch für die Funktion \texttt{assign} eine Kurzschreibweise,
nämlich \texttt{\textless{}-}. Entsprechend sind die folgenden beiden
Befehle äquivalent:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{assign}\NormalTok{(}\StringTok{"zwischenergebnis"}\NormalTok{, }\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{3}\NormalTok{)}
\NormalTok{zwischenergebnis <-}\StringTok{ }\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

Entsprechend werden wir Zuweisungen immer mit dem \texttt{\textless{}-}
Operator durchführen. \footnote{Theoretisch kann \texttt{\textless{}-}
  auch andersherum verwendet werden:
  \texttt{2\ +\ 3\ -\textgreater{}\ zwischenergebnis}. Das mag zwar auf
  den ersten Blick intuitiver erscheinen, da das aus \texttt{2\ +\ 3}
  resultierende Objekt den Namen \texttt{zwischenergebnis} bekommt, also
  immer erst das Objekt erstellt wird und dann der Name zugewiesen wird,
  es führt jedoch zu deutlich weniger lesbarem Code und sollte daher nie
  verwendet werden. Ebensoweinig sollten Zuweisungen durch den
  \texttt{=} Operatur vorgenommen werden, auch wenn es im Fall
  \texttt{zwischenergebnis\ =\ 2\ +\ 3} funktionieren würde.}

Wir können in R nicht beliebig Namen vergeben. Gültige (also:
syntaktisch korrekte) Namen \ldots{}

\begin{itemize}
\tightlist
\item
  enthalten nur Buchstaben, Zahlen und die Symbole \texttt{.} und
  \texttt{\_}
\item
  fangen nicht mit \texttt{.} oder einer Zahl an!
\end{itemize}

Zudem gibt es einige Wörter, die schlicht nicht als Name verwendet
werden dürgen, z.B. \texttt{function}, \texttt{TRUE}, oder \texttt{if}.
Die gesamte Liste verbotener Worte kann mit dem Befehl
\texttt{?Reserved} ausgegeben werden.

Wenn man einen Namen vergeben möchte, der nicht mit den gerade
formulierten Regeln kompatibel ist, gibt R eine Fehlermeldung aus:

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{TRUE}\NormalTok{ <-}\StringTok{ }\DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Error in TRUE <- 5: invalid (do_set) left-hand side to assignment
\end{verbatim}

Zudem sollte man folgendes beachten:

\begin{itemize}
\tightlist
\item
  Namen sollten kurz und informativ sein; entsprechen ist
  \texttt{sample\_mean} ein guter Name, \texttt{shit15\_2} dagegen eher
  weniger
\item
  Man sollte \textbf{nie Umlaute in Namen verwenden}
\item
  R ist \emph{case sensitive}, d.h. \texttt{mean\_value} ist ein anderer
  Name als \texttt{Mean\_Value}
\item
  Auch wenn möglich, sollte man nie von R bereit gestellte Funktionen
  überschreiben. Eine Zuweisung wie \texttt{assign\ \textless{}-\ 2} ist
  zwar möglich, führt in der Regel aber zu großem Unglück, weil man
  nicht mehr ganz einfach auf die zugrundeliegende Funktion
  zurückgreifen kann.
\end{itemize}

\begin{quote}
\textbf{Hinweis}: Alle aktuellen Namenszuweisungen sind im Bereich
\texttt{Environment} in R Studio (Nr. 4 in der Abbildung oben)
aufgelistet und können durch die Funktion \texttt{ls()} angezeigt
werden.
\end{quote}

\begin{quote}
\textbf{Hinweis}: Ein Objekt kann mehrere Namen haben, aber kein Name
kann zu mehreren Objekten zeigen, da im Zweifel eine neue Zuweisung die
alte Zuweisung überschreibt:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{2} 
\NormalTok{y <-}\StringTok{ }\DecValTok{2} \CommentTok{# Das Objekt 2 hat nun zwei Namen}
\KeywordTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{4} \CommentTok{# Der Name 'x' zeigt nun zum Objekt '4', nicht mehr zu '2'}
\KeywordTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 4
\end{verbatim}

\begin{quote}
\textbf{Hinweis}: Wie Sie vielleicht bereits bemerkt haben wird nach
einer Zuweisung kein Wert sichtbar ausgegeben:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{2} \CommentTok{# Keine Zuweisung, R gibt das Ergebnis in der Konsole aus}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{2} \CommentTok{# Zuweisung, R gibt das Ergebnis in der Konsole nicht aus}
\end{Highlighting}
\end{Shaded}

\section{Zusammenfassung}\label{zusammenfassung}

\begin{itemize}
\tightlist
\item
  Wir können Befehle in R Studio an den Computer übermitteln indem wir
  (a) den R Code in die Konsole schreiben und Enter drücken oder (b) den
  Code in ein Skript schreiben und dann ausführen
\item
  Alles was in R \emph{existiert} ist ein Objekt, alles was in R
  \emph{passiert} ist ein Funktionsaufruf
\item
  Wir können einem Objekt mit Hilfe von \texttt{\textless{}-} einen
  Namen geben und dann später wieder aufrufen. Den Prozess der
  Namensgebung nennen wir \textbf{Assignment} und wir können uns alle
  aktuell von uns vergebenen Namen mit der Funktion \texttt{ls()}
  anzeigen lassen.
\item
  Eine Funktion ist ein Objekt, das auf einen Input eine bestimmte
  Routine anwendet und einen Output produziert
\end{itemize}

An dieser Stelle sei noch auf die Hilfefunktion \texttt{help()}
hingewiesen. Falls Sie Informationen über ein Objekt bekommen wollen
können Sie so weitere Informationen bekommen. Wenn Sie z.B. genauere
Informationen über die Verwendung der Funktion \texttt{assign} erhalten
wollen, können Sie Folgendes eingeben:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(assign)}
\end{Highlighting}
\end{Shaded}

\section{Grundlegende Objeke in R}\label{grundlegende-objeke-in-r}

Wir haben bereits gelernt, dass alles was in R existiert ein Objekt ist.
Wir haben aber auch schon gelernt, dass es unterschiedliche Typen von
Objekten gibt: Zahlen, wie \texttt{2} oder \texttt{3} und Funktionen wie
\texttt{assign}.\footnote{Wie wir unten lernen werden sind \texttt{2}
  und \texttt{3} in erster Linie keine Zahlen, sondern Vektoren der
  Länge 1, und gelten erst in nächster Instanz als `Zahl' (genauer:
  `double').} Tatsächlich gibt es noch viel mehr Arten von Objekten. Ein
gutes Verständnis der Objektarten ist Grundvoraussetzung später
anspruchsvolle Programmieraufgaben zu lösen. Daher wollen wir uns im
Folgenden mit den wichtigsten Objektarten in R auseinandersetzen.

\subsection{Funktionen}\label{funktionen}

Wie oben bereits kurz erwähnt handelt es sich bei Funktionen um
Algorithmen, die bestimmte Routinen auf einen \emph{Input} anwenden und
dabei einen \emph{Output} produzieren.

Die Funktion \texttt{log()} zum Beispiel nimmt als Input eine Zahl und
gibt als Output den Logarithmus dieser Zahl aus:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{log}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.6931472
\end{verbatim}

\textbf{Eine Funktion aufrufen}

In R gibt es prinzipiell vier verschiedene Arten Funktionen aufzurufen.
Nur zwei davon sind allerdings aktuell für uns relevant.

Die bei weitem wichtigste Variante ist die so genannte
\emph{Prefix-Form}. Dies ist die Form, die wir bei der überwältigenden
Anzahl von Funktionen verwenden werden. Wir schreiben hier zunächst den
Namen der Funktion (im Folgenden Beispiel \texttt{assign}), dann in
Klammern und mit Kommata getrennt die Argumente der Funktion (hier der
Name \texttt{test} und die Zahl \texttt{2}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{assign}\NormalTok{(}\StringTok{"test"}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ein hin und wieder auftretende Form ist die so genannte
\emph{Infix-Form}. Hier wird der Funktionsname zwischen die Argumente
geschrieben. Dies ist, wie wir oben bereits bemerkt haben, bei vielen
mathematischen Funktionen wie \texttt{+}, \texttt{-} oder \texttt{/} der
Fall. Streng genommen ist die die Infix-Form aber nur eine
\emph{Abkürzung}, denn jeder Funktionsaufruf in Infix-Form kann auch in
Prefix-Form geschrieben werden, wie folgendes Beispiel zeigt:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{`}\DataTypeTok{+}\StringTok{`}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 5
\end{verbatim}

\textbf{Die Argumente einer Funktion}

Die Argumente einer Funktion stellen zum einen den \emph{Input} für die
in der Funktion implementierten Routine dar.

Die Funktion \texttt{sum} zum Beispiel nimmt als Argumente eine
beliebige Anzahl an Zahlen (ihr `Input') und berechnet die Summe dieser
Zahlen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 10
\end{verbatim}

Darüber hinaus akzeptiert \texttt{sum()} noch ein \emph{optionales
Argument}, \texttt{na.rm}, welches entweder den Wert \texttt{TRUE} oder
\texttt{FALSE} annehmen kann. Wenn wir das Argument nicht explizit
spezifizieren nimmt es automatisch \texttt{FALSE} als den Standardwert
an.

Dieses optionale Argument ist kein klassischer Input, sondern
kontrolliert das genaue Verhalten der Funktion. Im Falle von
\texttt{sum()} werden fehlende Werte, so genannte \texttt{NA} (siehe
unten) ignoriert bevor die Summe der Inputs gebildet wird wenn
\texttt{na.rm} den Wert \texttt{TRUE} hat:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\OtherTok{NA}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\OtherTok{NA}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 10
\end{verbatim}

Wenn wir wissen wollen, welche Argumente eine Funktion akzeptiert ist es
immer eine gute Idee über die Funktion \texttt{help()} einen Blick in
die Dokumentation zu werfen!

Im Falle von \texttt{sum()} sehen wir hier sofort, dass die Funktion
neben den zu addierenden Zahlen ein optionales Argument \texttt{na.rm}
akzeptiert, welches den Standardwert \texttt{FALSE} annimmt.

\textbf{Eigene Funktionen definieren}

Sehr häufig möchten wir selbst Funktionen definieren. Das können wir mit
dem reservierten Keyword \texttt{function} machen. Als Beispiel wollen
wir eine Funktion \texttt{pythagoras} definieren, die als Argumente die
Seitenlängen der Katheten eines rechtwinkligen Dreiecks annimmt und über
den \href{https://de.wikipedia.org/wiki/Satz_des_Pythagoras}{Satz des
Pythagoras} die Länge der Hypothenuse bestimmt:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pythagoras <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(kathete_}\DecValTok{1}\NormalTok{, kathete_}\DecValTok{2}\NormalTok{)\{}
\NormalTok{  hypo_quadrat <-}\StringTok{ }\NormalTok{kathete_}\DecValTok{1}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{kathete_}\DecValTok{2}\OperatorTok{**}\DecValTok{2}
\NormalTok{  l_hypothenuse <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(hypo_quadrat) }\CommentTok{# sqrt() zieht die Quadratwurzel}
  \KeywordTok{return}\NormalTok{(l_hypothenuse)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Wir definieren eine Funktion durch die Funktion \texttt{function()}. In
der Regel beginnen wir die Definition indem wir der zu erstellenden mit
einem Namen assoziieren (hier: `pythagoras') damit wir sie später auch
verwenden können.

Die Argumente für \texttt{function} sind dann die Argumente, welche die
zu definierende Funktion annehmen soll, in diesem Fall
\texttt{kathete\_1} und \texttt{kathete\_2}. Danach beginnen wir den
`function body', also den Code für die Routine, welche die Funktion
ausführen soll, mit einer geschweiften Klammer.

Innerhalb des \emph{function bodies} wird dann die entsprechende Routine
implementiert. Im vorliegenden Beispiel definieren wir zunächst die
Summe der Werte von \texttt{kathete\_1} und \texttt{kathete\_2} als ein
Zwischenergebnis, welches hier \texttt{hypo\_quadrat} genannt wird. Dies
ist der häufig unter \(c^2=a^2 + b^2\) bekannte Teil des Satz von
Pythagoras. Da wir an der `normalen' Länge der Hypothenuse interesssiert
sind, ziehen wir mit der Funktion \texttt{sqrt()} noch die Wurzel von
\texttt{hypo\_quadrat}, und geben dem resultierenden Objekt den Namen
\texttt{l\_hypothenuse}, welches in der letzten Zeile mit Hilfe des
Keywords \texttt{return} als der Wert definiert wird, den die Funktion
als Output ausgibt.\footnote{Das ist strikt genommen nicht notwendig,
  aber der Übersichtlichkeit werden wir immer \texttt{return} verwenden.
  Eine interessante Debatte darüber ob man \texttt{return} verwenden
  sollte oder nicht findet sich
  \href{https://stackoverflow.com/questions/11738823/explicitly-calling-return-in-a-function-or-not}{hier}.}

Am Ende der Routine kann man mit dem Keyword \texttt{return} explizit
machen welchen Wert die Funktion als Output ausgeben soll. Wenn wir die
Funktion nun aufrufen wird die oben definierte Routine ausgeführt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pythagoras}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 4.472136
\end{verbatim}

Beachten Sie, dass alle Objet Namen, die innerhalb des \emph{function
bodies} verwendet werden gehen nach dem Funktionsaufruf
verloren:\footnote{Das liegt daran, dass Funktionen ihr eigenes
  \href{https://adv-r.hadley.nz/environments.html}{environment} haben.}
Deswegen kommt es im vorliegenden Falle zu einem Fehler, da
\texttt{hypo\_quadrat} nur innerhalb des \emph{function bodies}
existiert:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pythagoras <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(kathete_}\DecValTok{1}\NormalTok{, kathete_}\DecValTok{2}\NormalTok{)\{}
\NormalTok{  hypo_quadrat <-}\StringTok{ }\NormalTok{kathete_}\DecValTok{1}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{kathete_}\DecValTok{2}\OperatorTok{**}\DecValTok{2}
\NormalTok{  l_hypothenuse <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(hypo_quadrat) }\CommentTok{# sqrt() zieht die Quadratwurzel}
  \KeywordTok{return}\NormalTok{(l_hypothenuse)}
\NormalTok{\}}
\NormalTok{x <-}\StringTok{ }\KeywordTok{pythagoras}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{hypo_quadrat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Error in eval(expr, envir, enclos): object 'hypo_quadrat' not found
\end{verbatim}

Es ist immer eine gute Idee, die selbst definierten Funktionen zu
dokumentieren - nicht nur wenn wir sie auch anderen zur Verfügung
stellen wollen, sondern auch damit wir selbst nach einer möglichen Pause
unseren Code noch gut verstehen können. Nichts ist frustrierender als
nach einer mehrwöchigen Pause viele Stunden investieren zu müssen, den
eigens programmierten Code zu entschlüsseln!

Die Dokumentation von Funktionen kann mit Hilfe von einfachen
Kommentaren erfolgen, ich empfehle jedoch sofort sich die
\href{https://r-pkgs.org/man.html\#man-functions}{hier beschriebenen
Konventionen} anzugewöhnen. In diesem Falle würde eine Dokumentation
unserer Funktion \texttt{pythagoras} folgendermaßen aussehen:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#' Berechne die Länge der Hypothenuse in einem rechtwinkligen Dreieck}
\CommentTok{#' }
\CommentTok{#' Diese Funktion nimmt als Argumente die Längen der beiden Katheten eines}
\CommentTok{#'  rechtwinkligen Dreiecks und berechnet daraus die Länge der Hypothenuse.}
\CommentTok{#' @param kathete_1 Die Länge der ersten Kathete}
\CommentTok{#' @param kathete_2 Die Länge der zweiten Kathete}
\CommentTok{#' @return Die Länge der Hypothenuse des durch a und b definierten }
\CommentTok{#'  rechtwinkligen Dreieckst}
\NormalTok{pythagoras <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(kathete_}\DecValTok{1}\NormalTok{, kathete_}\DecValTok{2}\NormalTok{)\{}
\NormalTok{  hypo_quadrat <-}\StringTok{ }\NormalTok{kathete_}\DecValTok{1}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{kathete_}\DecValTok{2}\OperatorTok{**}\DecValTok{2}
\NormalTok{  l_hypothenuse <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(hypo_quadrat) }\CommentTok{# sqrt() zieht die Quadratwurzel}
  \KeywordTok{return}\NormalTok{(l_hypothenuse)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Die Dokumentation wird also direkt vor die Definition der Funktion
gesetzt. In der ersten Zeile gibt man der Funktion einen maximal
einzeiligen Titel, der nicht länger als 80 Zeichen sein sollte und die
Funktion prägnant beschreibt.

Dann, nach einer Lehrzeile wird genauer beschrieben was die Funktion
macht. Danach werden die Argumente der Funktion beschrieben. Für jedes
Argument beginnen wir die Reihe mit \texttt{@param}, gefolgt von dem
Namen des Arguments und dann einer kurzen Beschreibung.

Nach den Argumenten beschreiben wir noch kurz was der Output der
Funktion ist. Diese Zeile wird mit \texttt{@return} begonnen.

Die Dokumentation einer Funktion sollte also zumindest die Parameter und
die Art des Outputs erklären.

\textbf{Gründe für die Verwendung eigener Funktionen}

Eigene Funktionen zu definieren ist in der Praxis extrem hilfreich und
es ist empfehlenswert Routinen, die mehrere Male verwendet werden
grundsätzlich als Funktionen zu schreiben. Dafür gibt es mehrere Gründe:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Der Code wird kürzer und transparenter.} Zwar ist kurzer Code
  nicht notwendigerweise leichter zu verstehen als langer, aber
  Funktionen können besonders gut dokumentiert werden (am besten indem
  man den hier beschriebenen Konventionen folgt).
\item
  \textbf{Funktionen bieten Struktur.} Funktionen fassen in der Regel
  Ihre Vorstellung davon zusammen, wie ein bestimmtes Problem zu lösen
  ist. Da man sich diese Gedanken nicht ständig neu machen möchte ist es
  sinnvoll sie einmalig in einer Funktion zusammen zu fassen.
\item
  \textbf{Funktionen erleichtern Korrekturen.} Wenn Sie merken, dass Sie
  in der Implementierung einer Routine einen Fehler gemacht haben müssen
  Sie im besten Falle nur einmal die Definition der Funktion korrigieren
  - im schlimmsten Falle müssen sie in ihrem Code nach der Routine
  suchen und sie in jedem einzelnen Anwendungsfall erneut korrigieren.
\end{enumerate}

Es gibt noch viele weitere Gründe dafür, Funktionen häufig zu verwenden.
Viele hängen mit dem Entwicklerprinzip
\href{https://de.wikipedia.org/wiki/Don\%E2\%80\%99t_repeat_yourself}{DRY}
(``Don't Repeat Yourself'') zusammen.

\subsection{Vektoren}\label{vektoren}

Vektoren sind einer der wichtigsten Objektypen in R. Quasi alle Daten
mit denen wir in R arbeiten werden als Vektoren behandelt.

Was Vektoren angeht gibt es wiederum die wichtige \textbf{Unterscheidung
von atomaren Vektoren und Listen}. Beide bestehen ihrerseits aus
Objekten und sie unterscheiden sich dadurch, dass atomare Vektoren nur
aus Objekten des gleichen Typs bestehen können, Listen dagegen auch
Objekte unterschiedlichen Typs beinhalten können.

Entsprechend kann jeder atomare Vektor einem Typ zugeordnet werden, je
nachdem welchen Typ seine Bestandteile haben. Hier sind insbesondere
vier Typen relevant:

\begin{itemize}
\tightlist
\item
  \texttt{logical} (logische Werte): es gibt zwei logische Werte,
  \texttt{TRUE} und \texttt{FALSE}, welche auch mit \texttt{T} oder
  \texttt{F} abgekürzt werden können
\item
  \texttt{integer} (ganze Zahlen): das sollte im Prinzip selbsterklärend
  sein, allerding muss den ganzen Zahlen in R immer der Buchstabe
  \texttt{L} folgen, damit die Zahl tatsächlich als ganze Zahl
  interpretiert wird.\footnote{Diese auf den ersten Blick merkwürdige
    Syntax hat historische Gründe: als der integer Typ in die R
    Programmiersprache eingeführt wurde war er sehr stark an den Typ
    \texttt{long\ integer} in der Programmiersprache `C' angelehnt. In C
    wurde ein solcher `long integer' mit dem Suffix `l' oder `L'
    definiert, diese Regel wurde aus Kompatibilitätsgründen auch für R
    übernommen, jedoch nur mit `L', da man Angst hatte, dass `l' mit `i'
    verwechselt wird, was in R für die imaginäre Komponente komplexer
    Zahlen verwendet wird.} Beispiele sind \texttt{1L}, \texttt{400L}
  oder \texttt{10L}.\\
\item
  \texttt{double} (Dezimalzahlen): auch das sollte selbsterklärend sein;
  Beispiele wären \texttt{1.5}, \texttt{0.0}, oder \texttt{-500.32}.
\item
  Ganze Zahlen und Dezimalzahlen werden häufig unter der Kategorie
  \texttt{numeric} zusammengefasst. Dies ist in der Praxis aber quasi
  nie hilfreich und man sollte diese Kategorie möglichst nie verwenden.
\item
  Wörter (\texttt{character}): sie sind dadurch gekennzeichnet, dass sie
  auch Buchstaben enthalten können und am Anfang und Ende ein \texttt{"}
  haben. Beispiele hier wären \texttt{"Hallo"}, \texttt{"500"} oder
  \texttt{"1\_2\_Drei"}.
\item
  Es gibt noch zwei weitere besondere `Typen', die strikt gesehen keine
  atomaren Vektoren darstellen, allerdings in diesem Kontext schon
  häufig auftauchen: \texttt{NULL}, was strikt genommen ein eigener
  Datentyp ist und immer die Länge 0 hat, sowie \texttt{NA}, das einen
  fehlenden Wert darstellt
\end{itemize}

Hieraus ergibt sich folgende Aufteilung für Vektoren:

\begin{center}\includegraphics[width=0.8\linewidth]{figures/vector-classification} \end{center}

Wir werden nun die einzelnen Typen genauer betrachten. Vorher wollen wir
jedoch noch die Funktion \texttt{typeof} einführen. Sie hilft uns in der
Praxis den Typ eines Objekts herauszufinden. Dafür rufen wir einfach die
Funktion \texttt{typeof} mit dem zu untersuchenden Objekt oder dessen
Namen auf:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(2L)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "integer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\FloatTok{22.0}
\KeywordTok{typeof}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "double"
\end{verbatim}

Wir können auch explizit testen ob ein Objekt ein Objekt bestimmten Typs
ist. Die generelle Syntax hierfür ist: \texttt{is.*()}, also z.B.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\FloatTok{1.0}
\KeywordTok{is.integer}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.double}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

Diese Funktion gibt als Output also immer einen logischen Wert aus, je
nachdem ob die Inputs des entsprechenden Typs sind oder nicht.

Bestimmte Objekte können in einen anderen Typ transformiert werden. Hier
spricht man von \texttt{coercion} und die generelle Syntax hierfür ist:
\texttt{as.*()}, also z.B.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "2"}
\KeywordTok{print}\NormalTok{(}
  \KeywordTok{typeof}\NormalTok{(x)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{as.double}\NormalTok{(x)}
\KeywordTok{print}\NormalTok{(}
  \KeywordTok{typeof}\NormalTok{(x)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "double"
\end{verbatim}

Allerdings ist eine Transformation nicht immer möglicht:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.double}\NormalTok{(}\StringTok{"Hallo"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Warning: NAs introduced by coercion
\end{verbatim}

\begin{verbatim}
#> [1] NA
\end{verbatim}

Da R nicht weiß wie man aus dem Wort `Hallo' eine Dezimalzahl machen
soll, transformiert er das Wort in einen `Fehlenden Wert', der in R als
\texttt{NA} bekannt ist und unten noch genauer diskutiert wird.

Für die Grundtypen ergibt sich folgende logische Hierachie an trivialen
Transformationen: \texttt{logical} → \texttt{integer} → \texttt{double}
→ \texttt{character}, d.h. man kann eine Dezimalzahl ohne Probleme in
ein Wort transformieren, aber nicht umgekehrt:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{2}
\NormalTok{y <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(x)}
\KeywordTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "2"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{as.double}\NormalTok{(y) }\CommentTok{# Das funktioniert}
\KeywordTok{print}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k <-}\StringTok{ }\KeywordTok{as.double}\NormalTok{(}\StringTok{"Hallo"}\NormalTok{) }\CommentTok{# Das nicht}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Warning: NAs introduced by coercion
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NA
\end{verbatim}

Da nicht immer ganz klar ist wann R bei Transformationen entgegen der
gerade eingeführten Hierachie eine Warnung ausgibt und wann nicht sollte
man hier immer besondere Vorsicht walten lassen!

Zudem ist bei jeder Transformation Vorsicht geboten, da sie häufig
Eigenschaften der Objekte implizit verändert. So führt eine
Transformation von einer Dezimalzahl hin zu einer ganzen Zahl teils zu
unerwartetem Rundungsverhalten:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\FloatTok{1.99}
\KeywordTok{as.integer}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1
\end{verbatim}

Auch führen Transformationen, die der eben genannten Hierachie
zuwiderlaufen, nicht zwangsweise zu Fehlern, sondern `lediglich' zu
unerwarteten Änderungen, die in jedem Fall vermieden werden sollten:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{as.logical}\NormalTok{(}\DecValTok{99}\NormalTok{)}
\KeywordTok{print}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

Häufig transformieren Funktionen ihre Argumente automatisch, was
meistens hilfreich ist, manchmal aber auch gefährlich sein kann:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{1L }\CommentTok{# Integer}
\NormalTok{y <-}\StringTok{ }\FloatTok{2.0} \CommentTok{# Double}
\NormalTok{z <-}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{y}
\KeywordTok{typeof}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "double"
\end{verbatim}

Interessanterweise werden logische Werte ebenfalls transformiert:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\OtherTok{TRUE}
\NormalTok{y <-}\StringTok{ }\OtherTok{FALSE}
\NormalTok{z <-}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{y }\CommentTok{# TRUE wird zu 1, FALSE zu 0}
\KeywordTok{print}\NormalTok{(z) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1
\end{verbatim}

Daher sollte man immer den Überblick behalten, mit welchen Objekttypen
man gerade arbeitet.

Hier noch ein kurzer Überblick zu den Test- und Transformationsbefehlen:

\begin{longtable}[]{@{}lll@{}}
\toprule
Typ & Test & Transformation\tabularnewline
\midrule
\endhead
logical & \texttt{is.logical} & \texttt{as.logical}\tabularnewline
double & \texttt{is.double} & \texttt{as.double}\tabularnewline
integer & \texttt{is.integer} & \texttt{as.integer}\tabularnewline
character & \texttt{is.character} & \texttt{as.character}\tabularnewline
function & \texttt{is.function} & \texttt{as.function}\tabularnewline
NA & \texttt{is.na} & NA\tabularnewline
NULL & \texttt{is.null} & \texttt{as.null}\tabularnewline
\bottomrule
\end{longtable}

Ein letzter Hinweis zu \textbf{Skalaren}. Unter Skalaren verstehen wir
in der Regel `einzelne Zahlen', z.B. \texttt{2}. Dieses Konzept gibt es
in R nicht. \texttt{2} ist ein Vektor der Länge 1. Wir unterscheiden
also vom Typ her nicht zwischen einem Vektor, der nur ein oder mehrere
Elemente hat.

\textbf{Hinweis:} Um längere Vektoren zu erstellen, verwenden wir die
Funktion \texttt{c()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1 2 3
\end{verbatim}

Dabei können auch Vektoren miteinander verbunden werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{3} \CommentTok{# Shortcut für: x <- c(1, 2, 3)}
\NormalTok{y <-}\StringTok{ }\DecValTok{4}\OperatorTok{:}\DecValTok{6}
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(x, y)}
\NormalTok{z}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1 2 3 4 5 6
\end{verbatim}

Da atomare Vektoren immer nur Objekte des gleichen Typs enthalten
können, könnte man erwarten, dass es zu einem Fehler kommt, wenn wir
Objete unterschiedlichen Type kombinieren wollen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{"Hallo"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Tatsächlich transformiert R die Objekte allerdings nach der oben
beschriebenen Hierachie \texttt{logical} → \texttt{integer} →
\texttt{double} → \texttt{character}. Da hier keine Warnung oder kein
Fehler ausgegeben wird, sind derlei Transformationen eine gefährliche
Fehlerquelle!

\textbf{Hinweis:} Die Länge eines Vektors kann mit der Funktion
\texttt{length} bestimmt werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{  }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{len_x <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
\NormalTok{len_x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 3
\end{verbatim}

\subsection{Logische Werte (logical)}\label{logische-werte-logical}

Die logischen Werte \texttt{TRUE} und \texttt{FALSE} sind häufig das
Ergebnis von logischen Abfragen, z.B. `Ist 2 größer als 1?'. Solche
Abfragen kommen in der Forschungspraxis häufig vor und es macht Sinn,
sich mit den häufigsten logischen Operatoren vertraut zu machen:

\begin{longtable}[]{@{}ccl@{}}
\toprule
Operator & Funktion in R & Beispiel\tabularnewline
\midrule
\endhead
größer & \texttt{\textgreater{}} &
\texttt{2\textgreater{}1}\tabularnewline
kleiner & \texttt{\textless{}} & \texttt{2\textless{}4}\tabularnewline
gleich & \texttt{==} & \texttt{4==3}\tabularnewline
größer gleich & \texttt{\textgreater{}=} &
\texttt{8\textgreater{}=8}\tabularnewline
kleiner gleich & \texttt{\textless{}=} &
\texttt{5\textless{}=9}\tabularnewline
nicht gleich & \texttt{!=} & \texttt{4!=5}\tabularnewline
und & \texttt{\&} &
\texttt{x\textless{}90\ \&\ x\textgreater{}55}\tabularnewline
oder & \texttt{\textbar{}} &
\texttt{x\textless{}90\ \textbar{}\ x\textgreater{}55}\tabularnewline
entweder oder & \texttt{xor()} &
\texttt{xor(2\textless{}1,\ 2\textgreater{}1)}\tabularnewline
nicht & \texttt{!} & \texttt{!(x==2)}\tabularnewline
ist wahr & \texttt{isTRUE()} &
\texttt{isTRUE(1\textgreater{}2)}\tabularnewline
\bottomrule
\end{longtable}

Das Ergebnis eines solches Tests ist immer ein logischer Wert:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{4}
\NormalTok{y <-}\StringTok{ }\NormalTok{x }\OperatorTok{==}\StringTok{ }\DecValTok{8}
\KeywordTok{typeof}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "logical"
\end{verbatim}

Es können auch längere Vektoren getestet werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{3}
\NormalTok{x}\OperatorTok{<}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE FALSE FALSE
\end{verbatim}

Tests können beliebig miteinander verknüpft werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{1L}
\NormalTok{x}\OperatorTok{>}\DecValTok{2} \OperatorTok{|}\StringTok{ }\NormalTok{x}\OperatorTok{<}\DecValTok{2} \OperatorTok{&}\StringTok{ }\NormalTok{(}\KeywordTok{is.double}\NormalTok{(x) }\OperatorTok{&}\StringTok{ }\NormalTok{x}\OperatorTok{!=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] FALSE
\end{verbatim}

Da für viele mathematischen Operationen \texttt{TRUE} als die Zahl
\texttt{1} interpretiert wird, ist es einfach zu testen wie häufig eine
bestimmte Bedingung erfüllt ist:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{50} 
\NormalTok{smaller_}\DecValTok{20}\NormalTok{ <-}\StringTok{ }\NormalTok{x}\OperatorTok{<}\DecValTok{20} 
\KeywordTok{print}\NormalTok{(}
  \KeywordTok{sum}\NormalTok{(smaller_}\DecValTok{20}\NormalTok{) }\CommentTok{# Wie viele Elemente sind kleiner als 20?}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 19
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}
  \KeywordTok{sum}\NormalTok{(smaller_}\DecValTok{20}\OperatorTok{/}\KeywordTok{length}\NormalTok{(x)) }\CommentTok{# Wie hoch ist der Anteil von diesen Elementen?}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.38
\end{verbatim}

\subsection{Wörter (character)}\label{worter-character}

Wörter werden in R dadurch gebildet, dass an ihrem Anfang und Ende das
Symbol \texttt{\textquotesingle{}} oder \texttt{""} steht:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "Hallo"}
\KeywordTok{typeof}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ 'Auf Wiedersehen'}
\KeywordTok{typeof}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "character"
\end{verbatim}

Wie andere Vektoren können sie mit der Funktion \texttt{c()} verbunden
werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(x, }\StringTok{" und "}\NormalTok{, y)}
\NormalTok{z}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Hallo"           " und "           "Auf Wiedersehen"
\end{verbatim}

Nützlich ist in diesem Zusammenhang die Funktion \texttt{paste()}, die
Elemente von mehreren Vektoren in Wörter transformiert und verbindet:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{10}
\NormalTok{y <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"Versuch Nr."}\NormalTok{, x)}
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "Versuch Nr. 1"  "Versuch Nr. 2"  "Versuch Nr. 3"  "Versuch Nr. 4" 
#>  [5] "Versuch Nr. 5"  "Versuch Nr. 6"  "Versuch Nr. 7"  "Versuch Nr. 8" 
#>  [9] "Versuch Nr. 9"  "Versuch Nr. 10"
\end{verbatim}

\texttt{paste()} akzeptiert ein optionales Argument \texttt{sep}, mit
dem wir den Wert angeben können, der zwischen die zu verbindenden
Elemente gesetzt wird:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tag_nr <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{10}
\NormalTok{x_axis <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"Tag"}\NormalTok{, tag_nr, }\DataTypeTok{sep =} \StringTok{": "}\NormalTok{)}
\NormalTok{x_axis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "Tag: 1"  "Tag: 2"  "Tag: 3"  "Tag: 4"  "Tag: 5"  "Tag: 6"  "Tag: 7" 
#>  [8] "Tag: 8"  "Tag: 9"  "Tag: 10"
\end{verbatim}

\begin{quote}
Hinweis: Hier haben wir ein Beispiel für das so genannte `Recycling'
gesehen: da der Vektor \texttt{c("Tag")} kürzer war als der Vektor
\texttt{tag\_nr} wird \texttt{c("Tag")} einfach kopiert damit die
Operation mit \texttt{paste()} Sinn ergibt. Recycling ist oft praktisch,
aber manchmal auch schädlich, nämlich dann, wenn man eigentlich davon
ausgeht eine Operation mit zwei gleich langen Vektoren durchzuführen,
dies aber tatsächlich nicht tut. In einem solchen Fall führt Recycling
dazu, dass keine Fehlermeldung ausgegeben wird. Ein Beispiel dafür gibt
folgender Code, in dem die Intention klar die Verbindung aller
Wochentage zu Zahlen ist und einfach ein Wochentag vergessen wurde:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tage <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"Tag "}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{, }\StringTok{":"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\NormalTok{tag_namen <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Montag"}\NormalTok{, }\StringTok{"Dienstag"}\NormalTok{, }\StringTok{"Mittwoch"}\NormalTok{, }\StringTok{"Donnerstag"}\NormalTok{, }\StringTok{"Freitag"}\NormalTok{, }\StringTok{"Samstag"}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(tage, tag_namen)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Tag 1: Montag"     "Tag 2: Dienstag"   "Tag 3: Mittwoch"  
#> [4] "Tag 4: Donnerstag" "Tag 5: Freitag"    "Tag 6: Samstag"   
#> [7] "Tag 7: Montag"
\end{verbatim}

\subsection{Fehlende Werte und NULL}\label{fehlende-werte-und-null}

Fehlende Werte werden in R als \texttt{NA} kodiert. \texttt{NA} erfüllt
gerade in statistischen Anwendungen eine wichtige Rolle, da ein
bestimmter Platz in einem Vektor aktuell fehlend sein müsste, aber als
Platz dennoch existieren muss.

\begin{quote}
\textbf{Beispiel:} Der Vektor \texttt{x} enthält einen logischen Wert,
der zeigt ob eine Person die Fragen auf einem Fragebogen richtig
beantwortet hat. Wenn die Person die dritte Frage auf dem Fragebogen
nicht beantwortet hat, sollte dies durch \texttt{NA} kenntlich gemacht
werden. Einfach den Wert komplett wegzulassen macht es im Nachhinein
unmöglich festzustellen \emph{welche} Frage die Person nicht beantwortet
hat.
\end{quote}

Die meisten Operationen die \texttt{NA} als einen Input bekommen geben
auch als Output \texttt{NA} aus, weil unklar ist wie die Operation mit
unterschiedlichen Werten für den fehlenden Wert ausgehen würde:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{5} \OperatorTok{+}\StringTok{ }\OtherTok{NA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NA
\end{verbatim}

Einzige Ausnahmen sind Operationen, die unabhängig vom fehlenden Wert
einen bestimmten Wert annehmen:

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{NA} \OperatorTok{|}\StringTok{ }\OtherTok{TRUE} \CommentTok{# Gibt immer TRUE, unabhängig vom Wert für NA}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

Um zu testen ob ein Vektor \texttt{x} fehlende Werte enthält sollte die
Funktion \texttt{is.na} verwendet werden, und nicht etwa der Ausdruck
\texttt{x==NA}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DecValTok{5}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\KeywordTok{print}\NormalTok{(x }\OperatorTok{==}\StringTok{ }\OtherTok{NA}\NormalTok{) }\CommentTok{# Unklar, da man nicht weiß, ob alle NA für den gleichen Wert stehen}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NA NA NA NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}
  \KeywordTok{is.na}\NormalTok{(x)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE FALSE  TRUE FALSE
\end{verbatim}

Wenn eine Operation einen nicht zu definierenden Wert ausgibt, ist das
Ergebnis nicht \texttt{NA} sondern \texttt{NaN} (\emph{not a number}):

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{0} \OperatorTok{/}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NaN
\end{verbatim}

Eine weitere Besonderheit ist \texttt{NULL}, welches in der Regel als
Vektor der Länge 0 gilt, aber häufig zu besonderen Zwecken verwendet
wird:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\OtherTok{NULL}
\KeywordTok{length}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0
\end{verbatim}

\subsection{Indizierung und Ersetzung}\label{indizierung-und-ersetzung}

Einzelne Elemente von atomare Vektoren können mit eckigen Klammern
extrahiert werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{)}
\NormalTok{x[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 2
\end{verbatim}

Auf diese Weise können auch bestimmte Elemente modifiziert werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{)}
\NormalTok{x[}\DecValTok{2}\NormalTok{] <-}\StringTok{ }\DecValTok{99}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  2 99  6
\end{verbatim}

Es kann auch mehr als ein Element extrahiert werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  2 99
\end{verbatim}

Negative Indizes sind auch möglich, diese eliminieren die entsprechenden
Elemente:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 99  6
\end{verbatim}

Um das letzte Element eines Vektors zu bekommen verwendet man einen
Umweg über die Funktion \texttt{length()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{length}\NormalTok{(x)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 6
\end{verbatim}

\subsection{Nützliche Funktionen für atomare
Vektoren}\label{nutzliche-funktionen-fur-atomare-vektoren}

Hier sollen nur einige Funktionen erwähnt werden, die im Kontext von
atomaren Vektoren besonders praktisch sind,\footnote{Für viele typische
  Aufgaben gibt es in R bereits eine vordefinierte Funktion. Am
  einfachsten findet man diese durch googlen.} inbesondere wenn es darum
geht solche Vektoren herzustellen, bzw. Rechenoperationen mit ihnen
durchzuführen.

\textbf{Herstellung von atomaren Vektoren}:

Eine Sequenz ganzer Zahlen wird in der Regel sehr häufig gebraucht.
Entsprechend gibt es den hilfreichen Shortcut\texttt{:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{10}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1]  1  2  3  4  5  6  7  8  9 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\DecValTok{10}\OperatorTok{:}\DecValTok{1}
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] 10  9  8  7  6  5  4  3  2  1
\end{verbatim}

Häufig möchten wir jedoch eine kompliziertere Sequenz bauen. In dem Fall
hilft uns die allgemeinere Funktion \texttt{seq()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\KeywordTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1]  1  2  3  4  5  6  7  8  9 10
\end{verbatim}

In diesem Fall ist \texttt{seq()} äquivalent zu \texttt{:}. \texttt{seq}
erlaubt aber mehrere optionale Argumente: so können wir mit \texttt{by}
die Schrittlänge zwischen den einzelnen Zahlen definieren.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.5}\NormalTok{)}
\KeywordTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5
#> [15]  8.0  8.5  9.0  9.5 10.0
\end{verbatim}

Wenn wir die Länge des resultierenden Vektors festlegen wollen und die
Schrittlänge von R automatisch festgelegt werden soll, können wir dies
mit dem Argument \texttt{length.out} machen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{4}\NormalTok{)}
\KeywordTok{print}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 2 4 6 8
\end{verbatim}

Und wenn wir einen Vektor in der Länge eines anderen Vektors erstellen
wollen, bietet sich das Argument \texttt{along.with} an. Dies wird
häufig für das Erstellen von Indexvektoren verwendet. In einem solchen
Fall müssen wir die Indexzahlen nicht direkt angeben:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z_index <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{along.with =}\NormalTok{ z)}
\KeywordTok{print}\NormalTok{(z_index)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1 2 3 4
\end{verbatim}

Auch häufig möchten wir einen bestimmten Wert wiederholen. Das geht mit
der Funktion \texttt{rep}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\KeywordTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NA NA NA NA NA
\end{verbatim}

\textbf{Rechenoperationen}

Es gibt eine Reihe von Operationen, die wir sehr häufig gemeinsam mit
Vektoren anwenden. Häufig interessiert und die \textbf{Länge} eines
Vektors. Dafür können wir die Funktion \texttt{length()} verwenden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\KeywordTok{length}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 4
\end{verbatim}

Wenn wir den \textbf{größten} oder \textbf{kleinsten Wert} eines Vektors
erfahren möchten geht das mit den Funktionen \texttt{min()} und
\texttt{max()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{min}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 4
\end{verbatim}

Beide Funktionen besitzen ein optionales Argument \texttt{na.rm}, das
entweder \texttt{TRUE} oder \texttt{FALSE} sein kann. Im Fallse von
\texttt{TRUE} werden alle \texttt{NA} Werte für die Rechenoperation
entfernt:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\OtherTok{NA}\NormalTok{)}
\KeywordTok{min}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{min}\NormalTok{(y, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1
\end{verbatim}

Den \textbf{Mittelwert} bzw die \textbf{Varianz/Standardabweichung} der
Elemente bekommen wir mit \texttt{mean()}, \texttt{var()}, bzw.
\texttt{sd()}, wobei alle Funktionen auch das optionale Argument
\texttt{na.rm} akzeptieren:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 2.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(y, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1.666667
\end{verbatim}

Ebenfalls häufig sind wir an der \textbf{Summe}, bzw, dem
\textbf{Produkt} aller Elemente des Vektors interessiert. \texttt{sum()}
und \texttt{prod()} helfen weiter und auch sie kennen das optionale
Argument \texttt{na.rm}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prod}\NormalTok{(y, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 24
\end{verbatim}

\subsection{Listen}\label{listen}

Im Gegensatz zu atomaren Vektoren können Listen Objekte verschiedenen
Typs enthalten. Sie werden mit der Funktion \texttt{list()} erstellt:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \StringTok{"a"}\NormalTok{,}
  \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),}
  \OtherTok{FALSE}
\NormalTok{)}
\KeywordTok{typeof}\NormalTok{(l_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l_}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "a"
#> 
#> [[2]]
#> [1] 1 2 3
#> 
#> [[3]]
#> [1] FALSE
\end{verbatim}

Wir können Listen mit der Funktion \texttt{str()} inspizieren. In diesem
Fall erhalten wir unmittelbar Informationen über die Art der Elemente:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(l_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> List of 3
#>  $ : chr "a"
#>  $ : num [1:3] 1 2 3
#>  $ : logi FALSE
\end{verbatim}

Die einzelnen Elemente einer Liste können auch benannt werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \StringTok{"erstes_element"}\NormalTok{ =}\StringTok{ "a"}\NormalTok{,}
  \StringTok{"zweites_element"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),}
  \StringTok{"drittes_element"}\NormalTok{ =}\StringTok{ }\OtherTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Die Namen aller Elemente in der Liste erhalten wir mit der Funktion
\texttt{names()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(l_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "erstes_element"  "zweites_element" "drittes_element"
\end{verbatim}

Um einzelne Elemente einer Liste auszulesen müssen wir \texttt{{[}{[}}
anstatt \texttt{{[}} verwemden. Wir können dann entweder Elemente nach
ihrer Position oder ihren Namen auswählen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l_}\DecValTok{2}\NormalTok{[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "a"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l_}\DecValTok{2}\NormalTok{[[}\StringTok{"erstes_element"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "a"
\end{verbatim}

Im folgenden wollen wir uns noch mit zwei speziellen Typen beschäftigen,
die weniger fundamental als die bislang diskutierten sind, jedoch häufig
in der alltäglichen Arbeit vorkommen: Matrizen und Data Frames.

\subsection{Matrizen}\label{matrizen}

Bei Matrizen handelt es sich um zweidimensionale Objekte mit Zeilen und
Spalten, bei denen es sich jeweils um atomare Vektoren handelt.

\textbf{Erstellen von Matrizen}

Matrizen werden mit der Funktion \texttt{matrix()}erstellt. Diese
Funktion nimmt als erstes Argument die Elemente der Matrix und dann die
Spezifikation der Anzahl von Zeilen (\texttt{nrow}) und/oder der Anzahl
von Spalten (\texttt{ncol}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{11}\OperatorTok{:}\DecValTok{20}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{5}\NormalTok{)}
\NormalTok{m_}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      [,1] [,2]
#> [1,]   11   16
#> [2,]   12   17
#> [3,]   13   18
#> [4,]   14   19
#> [5,]   15   20
\end{verbatim}

Wie können die Zeilen, Spalten und einzelne Werte folgendermaßen
extrahieren und ggf. Ersetzungen vornehmen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m_}\DecValTok{1}\NormalTok{[,}\DecValTok{1}\NormalTok{] }\CommentTok{# Erste Spalte}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 11 12 13 14 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m_}\DecValTok{1}\NormalTok{[}\DecValTok{1}\NormalTok{,] }\CommentTok{# Erste Zeile}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 11 16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m_}\DecValTok{1}\NormalTok{[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\CommentTok{# Element [2,2]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 17
\end{verbatim}

\begin{quote}
\textbf{Optionaler Hinweis:} Matrizen sind weniger `fundamantal' als
atomare Vektoren. Entsprechend gibt uns \texttt{typeof()} für eine
Matrix auch den Typ der enthaltenen atomaren Vektoren an:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(m_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "integer"
\end{verbatim}

\begin{quote}
Um zu testen ob es sich bei einem Objekt um eine Matrix handelt
verwenden wir entsprechend \texttt{is.matrix()}:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.matrix}\NormalTok{(m_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.matrix}\NormalTok{(}\FloatTok{2.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] FALSE
\end{verbatim}

\textbf{Matrizenalgebra}

Matrizenalgebra spielt in vielen statistischen Anwendungen eine wichtige
Rolle. In R ist es sehr einfach die typischen Rechenoperationen für
Matrizen zu implementieren. Hier nur ein paar Beispiele, für die wir die
folgenden Matrizen verwenden:

\[A = \left( 
\begin{array}{rrr}                                
1 & 6 \\                                               
5 & 3 \\                                               
\end{array}
\right) \quad B = \left( 
\begin{array}{rrr}                                
0 & 2 \\                                               
4 & 8 \\                                               
\end{array}\right)\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrix_a <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{matrix_b <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{8}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Skalar-Addition: \[4+\boldsymbol{A}=
\left( 
\begin{array}{rrr}                                
4+a_{11} & 4+a_{21} \\                                               
4+a_{12} & 4+a_{22} \\                                               
\end{array}
\right)\]

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{4}\OperatorTok{+}\NormalTok{matrix_a}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      [,1] [,2]
#> [1,]    5   10
#> [2,]    9    7
\end{verbatim}

Matrizen-Addition: \[\boldsymbol{A}+\boldsymbol{B}=
\left(
\begin{array}{rrr}                                
a_{11} + b_{11} & a_{21} + b_{21}\\                                               
a_{12} + b_{12} & a_{22} + b_{22}\\                                               
\end{array}
\right)\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrix_a }\OperatorTok{+}\StringTok{ }\NormalTok{matrix_b}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      [,1] [,2]
#> [1,]    1    8
#> [2,]    9   11
\end{verbatim}

Skalar-Multiplikation: \[2\cdot\boldsymbol{A}=
\left( 
\begin{array}{rrr}                                
2\cdot a_{11} & 2\cdot a_{21} \\                                               
2\cdot a_{12} & 2\cdot a_{22} \\                                               
\end{array}
\right)\]

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{*}\NormalTok{matrix_a}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      [,1] [,2]
#> [1,]    2   12
#> [2,]   10    6
\end{verbatim}

Elementenweise Matrix Multiplikation (auch `Hadamard-Produkt'):
\[\boldsymbol{A}\odot\boldsymbol{B}=
\left(
\begin{array}{rrr}                                
a_{11}\cdot b_{11} & a_{21}\cdot b_{21}\\                                               
a_{12}\cdot b_{12} & a_{22}\cdot b_{22}\\                                               
\end{array}
\right)\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrix_a }\OperatorTok{*}\StringTok{ }\NormalTok{matrix_b}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      [,1] [,2]
#> [1,]    0   12
#> [2,]   20   24
\end{verbatim}

Matrizen-Multiplikation: \[\boldsymbol{A}\cdot\boldsymbol{B}=
\left(
\begin{array}{rrr}                                
a_{11}\cdot b_{11} + a_{12}\cdot b_{21} & a_{11}\cdot b_{21}+a_{12}\cdot b_{22}\\                     a_{21}\cdot b_{11} + a_{22}\cdot b_{21} & a_{21}\cdot b_{12}+a_{22}\cdot b_{22}\\                     
\end{array}
\right)\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrix_a }\OperatorTok{%*%}\StringTok{ }\NormalTok{matrix_b}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      [,1] [,2]
#> [1,]   24   50
#> [2,]   12   34
\end{verbatim}

Die Inverse einer Matrix \(\boldsymbol{A}\), \(\boldsymbol{A}^{-1}\),
ist definiert sodass gilt
\[\boldsymbol{A}\boldsymbol{A}^{-1}=\boldsymbol{I}\] Sie kann in R mit
der Funktion \texttt{solve()} identifiziert werden:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{solve}\NormalTok{(matrix_a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>            [,1]        [,2]
#> [1,] -0.1111111  0.22222222
#> [2,]  0.1851852 -0.03703704
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrix_a }\OperatorTok{%*%}\StringTok{ }\KeywordTok{solve}\NormalTok{(matrix_a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      [,1]         [,2]
#> [1,]    1 2.775558e-17
#> [2,]    0 1.000000e+00
\end{verbatim}

Die minimalen Abweichungen sind auf machinelle Rundungsfehler
zurückzuführen und treten häufig auf.

Es gibt im Internet zahlreiche gute Überblicksartikel zum Thema
Matrizenalgebra in R, z.B.
\href{https://www.statmethods.net/advstats/matrix.html}{hier} oder in
größerem Umfang
\href{https://www.math.uh.edu/~jmorgan/Math6397/day13/LinearAlgebraR-Handout.pdf}{hier}.

\subsection{Data Frames}\label{data-frames}

Der \texttt{data.frame} ist eine besondere Art von Liste und ist ein in
der Datenanalyse regelmäßig auftretender Datentyp. Gegensatz zu einer
normalen Liste müssen bei einem \texttt{data.frame} alle Elemente die
gleiche Länge aufweisen. Das heißt man kann sich einen
\texttt{data.frame} als eine rechteckig angeordnete Liste vorstellen.

Wegen der engen Verwandschaft können wir einen \texttt{data.frame}
direkt aus einer Liste erstellen indem wir die Funktion
\texttt{as.data.frame()} verwenden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \StringTok{"a"}\NormalTok{ =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{,}
  \StringTok{"b"}\NormalTok{ =}\StringTok{ }\DecValTok{4}\OperatorTok{:}\DecValTok{6}\NormalTok{,}
  \StringTok{"c"}\NormalTok{ =}\StringTok{ }\DecValTok{7}\OperatorTok{:}\DecValTok{9}
\NormalTok{)}
\NormalTok{df_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(l_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wenn wir R nach dem Typ von \texttt{df\_3} fragen, sehen wir, dass es
sich weiterhin um eine Liste handelt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(df_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "list"
\end{verbatim}

Allerdings können wir testen ob \texttt{df\_3} ein \texttt{data.frame}
ist indem wir \texttt{is.data.frame} benutzen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.data.frame}\NormalTok{(df_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.data.frame}\NormalTok{(l_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] FALSE
\end{verbatim}

Wenn wir \texttt{df\_3} ausgeben sehen wir unmittelbar den Unterschied
zu klassischen Liste:\footnote{Gerade bei sehr großen Data Frames möchte
  man oft nur die ersten paar Elemente inspizieren. Das ist mit der
  Funktion \texttt{head()} möglich.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l_}\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> $a
#> [1] 1 2 3
#> 
#> $b
#> [1] 4 5 6
#> 
#> $c
#> [1] 7 8 9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_}\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>   a b c
#> 1 1 4 7
#> 2 2 5 8
#> 3 3 6 9
\end{verbatim}

Die andere Möglichkeit einen \texttt{data.frame} zu erstellen ist direkt
über die Funktion \texttt{data.frame()}, wobei es hier in der Regel
ratsam ist das optionale Argument \texttt{stringsAsFactors} auf
\texttt{FALSE} zu setzen, da sonst Wörter in so genannte Faktoren
umgewandelt werden:\footnote{Zur Geschichte dieses wirklich ärgerlichen
  Verhaltens siehe
  \href{https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/}{diesen
  Blog}.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_}\DecValTok{4}\NormalTok{ <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \StringTok{"gender"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\DecValTok{3}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"female"}\NormalTok{, }\DecValTok{2}\NormalTok{)),}
  \StringTok{"height"}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{89}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{66}\NormalTok{, }\DecValTok{50}\NormalTok{),}
  \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}
\NormalTok{)}
\NormalTok{df_}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>   gender height
#> 1   male     89
#> 2   male     75
#> 3   male     80
#> 4 female     66
#> 5 female     50
\end{verbatim}

Data Frames sind das klassische Objekt um eingelesene Daten zu
repräsentieren. Wenn Sie sich z.B. Daten zum BIP in Deutschland aus dem
Internet runterladen und diese Daten dann in R einlesen, werden diese
Daten zunächst einmal als \texttt{data.frame} repräsentiert.\footnote{Das
  ist nicht ganz korrekt, weil es mittlerweilse Erweiterungen gibt,
  welche den \texttt{data.frame} mit effizienteren Objekten ersetzen,
  z.B. dem \texttt{tibble} oder dem \texttt{data.table}. Der Umgang mit
  diesen Objekten ist jedoch sehr ähnlich zum \texttt{data.frame}.}
Diese Repräsentation erlaubt dann eine einfache Analyse und Manipulation
der Daten.

Zwar gibt es eine eigene Vorlesung zur Bearbeitung von Daten, wir wollen
aber schon hier einige zentrale Befehle im Zusammenhang von Data Frames
einführen.

An dieser Stelle sei jedoch schon angemerkt, dass um Zeilen, Spalten
oder einzelne Elemente auszuwählen verwenden die gleichen Befehle wie
bei Matrizen verwendet werdenkönnen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_}\DecValTok{4}\NormalTok{[, }\DecValTok{1}\NormalTok{] }\CommentTok{# erste Spalte}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "male"   "male"   "male"   "female" "female"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_}\DecValTok{4}\NormalTok{[, }\DecValTok{2}\NormalTok{] }\CommentTok{# Werte der zweiten Spalte}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 89 75 80 66 50
\end{verbatim}

Die Abfrage funktioniert nicht nur mit Indices, sondern auch mit
Spaltennamen:\footnote{Anstelle von \texttt{{[}{[}} kann auch der
  Shortcut \texttt{\$} verwendet werden. Das werden wir aufgrund der
  größeren Transparenz von \texttt{{[}{[}} hier jedoch nicht verwenden.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_}\DecValTok{4}\NormalTok{[[}\StringTok{"gender"}\NormalTok{]] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "male"   "male"   "male"   "female" "female"
\end{verbatim}

Wenn wir \texttt{{[}} anstatt von \texttt{{[}{[}} verwenden erhalten wir
als Output einen (reduzierten) Data Frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_}\DecValTok{4}\NormalTok{[}\StringTok{"gender"}\NormalTok{] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>   gender
#> 1   male
#> 2   male
#> 3   male
#> 4 female
#> 5 female
\end{verbatim}

Es können auch mehrere Zeilen ausgewählt werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_}\DecValTok{4}\NormalTok{[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, ] }\CommentTok{# Die ersten beiden Zeilen}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>   gender height
#> 1   male     89
#> 2   male     75
\end{verbatim}

Oder einzelne Werte:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_}\DecValTok{4}\NormalTok{[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{] }\CommentTok{# Zweiter Wert der zweiten Spalte}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 75
\end{verbatim}

Dies können wir uns zu Nutze machen um den Typ der einzelnen Spalten
herauszufinden:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(df_}\DecValTok{4}\NormalTok{[[}\StringTok{"gender"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "character"
\end{verbatim}

\section{Pakete}\label{pakete}

Bei Paketen handelt es sich um eine Kombination aus R Code, Daten,
Dokumentationen und Tests. Sie sind der beste Weg, reproduzierbaren Code
zu erstellen und frei zugänglich zu machen. Zwar werden Pakete häufig
der Öffentlichkeit zugänglich gemacht, z.B. über GitHub oder CRAN. Es
ist aber genauso hilfreich, Pakete für den privaten Gerbrauch zu
schreiben, z.B. um für bestimmte Routinen Funktionen zu programmieren,
zu dokumentieren und in verschiedenen Projekten verfügbar zu
machen.\footnote{\citet{Packages} bietet eine exzellente Einführung in
  das Programmieren von R Paketen.}

Die Tatsache, dass viele Menschen statistische Probleme lösen indem sie
bestimmte Routinen entwickeln, diese dann generalisieren und über Pakete
der ganzen R Community frei verfügbar machen, ist einer der Hauptgründe
für den Erfolg und die breite Anwendbarkeit von R.

Wenn man R startet haben wir Zugriff auf eine gewisse Anzahl von
Funktionen, vordefinierten Variablen und Datensätzen. Die Gesamtheit
dieser Objekte wird in der Regel \texttt{base\ R} genannt, weil wir alle
Funktionalitäten ohne Weiteres nutzen können.

Die Funktion \texttt{assign}, zum Beispiel, ist Teil von
\texttt{base\ R}: wir starten R und können Sie ohne Weiteres verwenden.

Im Prinzip kann so gut wie jedwede statistische Prozedur in
\texttt{base\ R} implementiert werden. Dies ist aber häufig
zeitaufwendig und fehleranfällig: wie wir am Beispiel von Funktionen
gelernt haben, sollten häufig verwendete Routinen im Rahmen von einer
Funktion implementiert werden, die dann immer wieder angewendet werden
kann. Das reduziert nicht nur Fehler, sondern macht den Code besser
verständlich.

Pakete folgen dem gleichen Prinzip, nur tragen sie die Idee noch weiter:
hier wollen wir die Funktionen auch über ein einzelnes R Projekt hinaus
nutzbar machen, sodass sie nicht in jedem Projekt neu definiert werden
müssen, sondern zentral nutzbar gemacht und dokumentiert werden.

Um ein Paket in R zu nutzen, muss es zunächst installiert werden. Für
Pakete, die auf der zentralen R Pakete Plattform CRAN verfügbar sind,
geht dies mit der Funktion \texttt{install.packages}. Wenn wir z.B. das
Paket \texttt{data.table} installieren wollen geht das mit dem folgenden
Befehl:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"data.table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Das Paket \texttt{data.table} enthält viele Objekte, welche die Arbeit
mit großen Datensätzen enorm erleichtern. Darunter ist eine verbesserte
Version des \texttt{data.frame}, der \texttt{data.table}. Wir können
einen \texttt{data.frame} mit Hilfe der Funktion
\texttt{as.data.table()} in einen \texttt{data.table} umwandeln.

Allerdings haben wir selbst nach erfolgreicher Installation von
\texttt{data.table} nicht direkt Zugriff auf diese Funktion:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{a=}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}
  \DataTypeTok{b=}\DecValTok{21}\OperatorTok{:}\DecValTok{25}
\NormalTok{)}
\KeywordTok{as.data.table}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Error in as.data.table(x): could not find function "as.data.table"
\end{verbatim}

Wir haben zwei Möglichkeiten auf die Objekte im Paket
\texttt{data.table} zuzugreifen: zum einen können wir mit dem Operator
\texttt{::} arbeiten:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\NormalTok{data.table}\OperatorTok{::}\KeywordTok{as.data.table}\NormalTok{(x)}
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>    a  b
#> 1: 1 21
#> 2: 2 22
#> 3: 3 23
#> 4: 4 24
#> 5: 5 25
\end{verbatim}

Wir schreiben also den Namen des Pakets, direkt gefolgt von \texttt{::}
und dann den Namen des Objets aus dem Paket, das wir vewendent wollen.

Zwar ist das der transparenteste und sauberste Weg auf Objekte aus
anderen Paketen zuzugreifen, allerdings kann es auch nervig sein wenn
man häufig oder sehr viele Objekte aus dem gleichen Paket verwendet. Wir
können alle Objekte eines Paketes direkt zugänglich machen indem wir die
Funktion \texttt{library()} verwenden.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{as.data.table}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

Der Übersicht halber sollte das für alle in einem Skript verwendeten
Pakete ganz am Anfang des Skripts gemacht werden. So sieht man auch
unmittelbar welche Pakete für das Skript installiert sein müssen.

Grundsätzlich sollte man in jedem Skript nur die Pakete mit
\texttt{library()} einlesen, die auch tatsächlich verwendet werden.
Ansonsten lädt man unnötigerweise viele Objekte und verliert den
Überblick woher eine bestimmte Funktion eigentlich kommt. Außerdem ist
es schwieriger für andere das Skript zu verwenden, weil unter Umständen
viele Pakete unnötigerweise installiert werden müssen.

Da Pakete dezentral von verschiedensten Menschen hergestellt werden,
besteht die Gefahr, dass Objekte in unterschiedlichen Paketen den
gleichen Namen bekommen. Da in R ein Name nur zu einem Objekt gehören
kann, werden beim Einladen mehrerer Pakete eventuell Namen
überschrieben, oder `maskiert'. Dies wird am Anfang beim Einlesen der
Pakete mitgeteilt, gerät aber leicht in Vergessenheit und kann zu sehr
kryptischen Fehlermeldungen führen.

Wir wollen das kurz anhand der beiden Pakete \texttt{dplyr} und
\texttt{plm} illustrieren:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(plm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Attaching package: 'plm'
\end{verbatim}

\begin{verbatim}
#> The following objects are masked from 'package:dplyr':
#> 
#>     between, lag, lead
\end{verbatim}

\begin{verbatim}
#> The following object is masked from 'package:data.table':
#> 
#>     between
\end{verbatim}

In beiden Paketen gibt es Objekte mit den Namen \texttt{between},
\texttt{lag} und \texttt{lead}. Bei der Verwendung von \texttt{library}
maskiert das später eingelesene Paket die Objekte des früheren. Wir
können das illustrieren indem wir den Namen des Objekts eingeben:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lead}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> function (x, k = 1, ...) 
#> {
#>     UseMethod("lead")
#> }
#> <bytecode: 0x7f9be67d3fb0>
#> <environment: namespace:plm>
\end{verbatim}

Aus der letzten Zeile wird ersichtlich, dass \texttt{lead} hier aus dem
Paket \texttt{plm} kommt.

Wenn wir die Funktion aus \texttt{dplyr} verwenden wollen, müssen wir
\texttt{::} verwenden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\NormalTok{lead}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> function (x, n = 1L, default = NA, order_by = NULL, ...) 
#> {
#>     if (!is.null(order_by)) {
#>         return(with_order(order_by, lead, x, n = n, default = default))
#>     }
#>     if (length(n) != 1 || !is.numeric(n) || n < 0) {
#>         bad_args("n", "must be a nonnegative integer scalar, ", 
#>             "not {friendly_type_of(n)} of length {length(n)}")
#>     }
#>     if (n == 0) 
#>         return(x)
#>     xlen <- length(x)
#>     n <- pmin(n, xlen)
#>     out <- c(x[-seq_len(n)], rep(default, n))
#>     attributes(out) <- attributes(x)
#>     out
#> }
#> <bytecode: 0x7f9be6885360>
#> <environment: namespace:dplyr>
\end{verbatim}

Wenn es zu Maskierungen kommt ist es aber der Transparenz wegen besser
in beiden Fällen \texttt{::} zu verwenden, also \texttt{plm::lead} und
\texttt{dplyr::lead}.

\begin{quote}
\textbf{Hinweis}: Alle von Konflikten betroffenen Objekte können mit der
Funktion \texttt{conflicts()} angezeigt werden.
\end{quote}

\begin{quote}
\textbf{Optionale Info}: Um zu überprüfen in welcher Reihenfolge R nach
Objekten sucht, kann die Funktion \texttt{search} verwendet werden. Wenn
ein Objekt aufgerufen wird schaut R zuerst im ersten Element des Vektors
nach, der globalen Umgebung. Wenn das Objekt dort nicht gefunden wird,
schaut es im zweiten, etc. Wie man hier auch erkennen kann, werden
einige Pakete standardmäßig eingelesen. Wenn ein Objekt nirgends
gefunden wird gibt R einen Fehler aus. Im vorliegenden Falle zeigt uns
die Funktion, dass er erst im Paket \texttt{plm} nach der Funktion
\texttt{lead()} sucht, und nicht im Paket \texttt{dplyr}:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{search}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] ".GlobalEnv"         "package:plm"        "package:dplyr"     
#>  [4] "package:data.table" "package:stats"      "package:graphics"  
#>  [7] "package:grDevices"  "package:utils"      "package:datasets"  
#> [10] "package:methods"    "Autoloads"          "package:base"
\end{verbatim}

\begin{quote}
\textbf{Weiterführender Hinweis} Um das Maskieren besser zu verstehen
sollte man sich mit dem Konzept von \emph{namespaces} und
\emph{environments} auseinandersetzen. Eine gute Erklärung bietet
\citet{Packages}.
\end{quote}

\begin{quote}
\textbf{Weiterführender Hinweis} Das Paket \texttt{conflicted} führt
dazu, dass R immer einen fehler ausgibt wenn nicht eindeutige
Objektnamen verwendet werden.
\end{quote}

\section{Kurzer Exkurs zum Einlesen und Schreiben von
Daten}\label{kurzer-exkurs-zum-einlesen-und-schreiben-von-daten}

Zum Abschluss wollen wir noch kurz einige Befehle zum Einlesen von Daten
einführen. Später werden wir uns ein ganzes Kapitel mit dem Einlesen und
Schreiben von Daten beschäftigen, da dies in der Regel einen nicht
unbeträchtlichen Teil der quantitativen Forschungsarbeit in Anspruch
nimmt. An dieser Stelle wollen wir aber nur lernen, wie man einen
Datensatz in R einliest.

R kann zahlreiche verschiedene Dateiformate einlesen, z.B. \texttt{csv},
\texttt{dta} oder \texttt{txt}, auch wenn für manche Formate bestimmte
Pakete geladen sein müssen.

Das gerade für kleinere Datensätze mit Abstand beste Format ist in der
Regel \texttt{csv}, da es von zahlreichen Programmen und auf allen
Betriebssystemen gelesen und geschrieben werden kann.

Für die Beispiele hier nehmen wir folgende Ordnerstruktur an:

\begin{center}\includegraphics[width=0.5\linewidth]{figures/chap3-data-folder} \end{center}

Um die Daten einzulesen verwenden wir das Paket \texttt{tidyverse}, die
wir später genauer kennen lernen werden. Sie enthält viele nützliche
Funktionen zur Arbeit mit Datensätzen. Zudem verwende ich das Paket
\texttt{here} um relative Pfade immer von meinem Arbeitsverzeichnis aus
angeben zu können.\footnote{Das ist notwendig, da dieses Skript in R
  Markdown geschrieben ist und das Arbeitsverzeichnis automatisch auf
  den Ordner ändert, in dem das .Rmd file liegt. Mehr Information zum
  Schreiben von R Markdown finden Sie im Anhang. Dieser wird auch in der
  Vorlesung besprochen.}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(here)}
\end{Highlighting}
\end{Shaded}

Nehmen wir an, die Datei \texttt{Rohdaten.csv} sähe folgendermaßen aus:

\begin{verbatim}
Auto,Verbrauch,Zylinder,PS
Ford Pantera L,15.8,8,264
Ferrari Dino,19.7,6,175
Maserati Bora,15,8,335
Volvo 142E,21.4,4,109
\end{verbatim}

Wie in einer typischen csv Datei sind die Spalten hier mit einem Komma
getrennt. Um diese Datei einzulesen verwenden wir die Funktion
\texttt{read\_csv} mit dem Dateipfad als erstes Argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auto_daten <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"data/raw/Rohdaten.csv"}\NormalTok{))}
\NormalTok{auto_daten}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 4
#>   Auto           Verbrauch Zylinder    PS
#>   <chr>              <dbl>    <dbl> <dbl>
#> 1 Ford Pantera L      15.8        8   264
#> 2 Ferrari Dino        19.7        6   175
#> 3 Maserati Bora       15          8   335
#> 4 Volvo 142E          21.4        4   109
\end{verbatim}

Wir haben nun einen Datensatz in R, mit dem wir dann weitere Analysen
anstellen können. Nehmen wir einmal an, wir wollen eine weitere Spalte
hinzufügen (Verbrauch/PS) und dann den Datensatz im Ordner
\texttt{data/tidy} speichern. Ohne auf die Modifikation des Data Frames
einzugehen können wir die Funktion \texttt{write\_csv} verwenden um den
Datensatz zu speichern. Hierzu geben wir den neuen Data Frame als
erstes, und den Pfad als zweites Argument an:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auto_daten_neu <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(auto_daten, }\DataTypeTok{Verbrauch_pro_PS=}\NormalTok{Verbrauch}\OperatorTok{/}\NormalTok{PS)}
\KeywordTok{write_csv}\NormalTok{(auto_daten_neu, }\KeywordTok{here}\NormalTok{(}\StringTok{"data/tidy/NeueDaten.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Es wird ein späteres Kapitel (und einen späteren Vorlesungstermin)
geben, in dem wir uns im Detail mit dem Lesen, Schreiben und
Manipulieren von Datensätzen beschäftigen.

\chapter{Lineare statistische Modelle in R}\label{linmodel}

\section{Einleitung und Überblick}\label{einleitung-und-uberblick}

\subsection{Einführung in die lineare
Regression}\label{einfuhrung-in-die-lineare-regression}

Zentrales Lernziel dieses Kapitels ist der Umgang mit einfachen linearen
Regressionsmodellen in R. Dabei werden die Inhalte des Anhangs
\protect\hyperlink{stat-rep}{Wiederholung grundlegender statistischer
Konzepte} als bekannt vorausgesetzt. Schauen Sie als erstes in diesem
Abschnitt nach wenn Sie ein hier verwendetes Konzept nicht verstehen und
konsultieren Sie ansonsten ein Statistiklehrbuch (und freundliche
Kommiliton*inen) ihrer Wahl.

In diesem Kapitel werden die folgenden R Pakete verwendet:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(latex2exp)}
\KeywordTok{library}\NormalTok{(icaeDesign)}
\KeywordTok{library}\NormalTok{(ggpubr)}
\end{Highlighting}
\end{Shaded}

Ziel solcher Modelle ist es, ausgehend von einem Datensatz ein lineares
Modell zu schätzen. Ein solches lineares Modell hat in der Regel die
Form

\[Y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + u_i\]
und soll uns helfen den linearen Zusammenhang zwischen den Variablen in
\(x_i\) und \(Y_i\) zu verstehen. Dazu müssen wir die Parameter
\(\beta_i\) \emph{schätzen}, denn \(\beta_i\) gibt uns Informationen
über den Zusammenhang zwischen \(x_i\) und \(Y_i\).

Sobald wir konkrete Werte für \(\beta_i\) geschätzt haben, können wir im
Optimalfall von unseren Daten auf eine größere Population schließen und
Vorhersagen für zukünftiges Verhalten des untersuchten Systems treffen.
Damit das funktioniert, müssen jedoch einige Annahmen erfüllt sein, und
in diesem Kapitel geht es nicht nur darum, die geschätzten Werte
\(\hat{\beta}_i\) zu identifizieren, sondern auch die der Regression
zugrundeliegenden Annahmen zu überprüfen.

Bevor wir uns Schritt für Schritt mit der Regression auseinandersetzen
wollen wir uns noch ein konkretes Beispiel anschauen.

\subsection{Einführungsbeispiel}\label{einfuhrungsbeispiel}

\begin{quote}
\textbf{Beispiel: Konsum und Nationaleinkommen} Wir sind daran
interessiert wie zusätzliches Einkommen auf die Konsumausgaben in einer
Volkswirtschaft auswirken. Daher stellen wir folgendes Modell auf:
\end{quote}

\[C_i = \beta_0 + \beta_1 Y_i + u_i\]

\begin{quote}
wobei \(C_i\) für die Konsumausgaben und \(Y_i\) für das BIP steht.
Diese Gleichung stellt unser statistisches Modell dar. Es hat zwei
Parameter, \(\beta_0\) und \(\beta_1\), die wir mit Hilfe unserer Daten
schätzen möchten. Wir laden uns also Daten zum Haushaltseinkommen und
zum BIP aus dem Internet herunter und inspizieren die Daten zunächst
visuell:
\end{quote}

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{quote}
Der Zusammenhang scheint gut zu unserem linearen Modell oben zu passen,
sodass wir das Modell mit Hilfe der Daten schätzen um konkrete Werte für
\(\beta_0\) und \(\beta_1\) zu identifizieren:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{schaetzung_bip <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Konsum}\OperatorTok{~}\NormalTok{BIP, }\DataTypeTok{data =}\NormalTok{ daten)}
\NormalTok{schaetzung_bip}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> lm(formula = Konsum ~ BIP, data = daten)
#> 
#> Coefficients:
#> (Intercept)          BIP  
#>   -184.0780       0.7064
\end{verbatim}

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{quote}
In dieser Abbildung korrespondiert \(\beta_0\) zum Achensabschnitt und
\(\beta_1\) zur Steigung der Konsumgerade. Wir können \(\beta_0\) als
die Konsumausgaben interpretieren, wenn das BIP Null betragen würde, und
\(\beta_1\) als die marginale Konsumquote, also den Betrag, um den die
Konsumausgaben steigen, wenn das BIP um ein Euro steigt. Die geschätzten
Werte für \(\beta_0\) und \(\beta_1\) sind hier \(-184\) und \(0.7\).
Auf dieser Basis können wir auch ausrechnen, wie hoch die Konsumausgaben
in einer Volkswirtschaftslehre mit einem BIP von 8000 wäre, indem wir
uns einfach an der geschätzten Gerade bis zu diesem Betrag fortbewegen.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\NormalTok{schaetzung_bip[[}\StringTok{"coefficients"}\NormalTok{]][}\DecValTok{1}\NormalTok{]}
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{schaetzung_bip[[}\StringTok{"coefficients"}\NormalTok{]][}\DecValTok{2}\NormalTok{]}
\KeywordTok{unname}\NormalTok{(beta_}\DecValTok{0} \OperatorTok{+}\StringTok{ }\NormalTok{beta_}\DecValTok{1}\OperatorTok{*}\DecValTok{8000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 5467.186
\end{verbatim}

\begin{quote}
Im aktuellen Beispiel wären das also \texttt{5467.19} Euro.
\end{quote}

\subsection{Überblick über die Inhalte des
Kapitels}\label{uberblick-uber-die-inhalte-des-kapitels}

Im folgenden werden wir uns zunächst mit den
\protect\hyperlink{lin-grundlagen}{formalen Grundlagen} der linearen
Einfachregression, also der Regression mit einer \(x\)-Variable, und
ihrer Implementierung in R beschäftigen. Insbesondere wir die Methode
der kleinsten Quadrate und die dafür notwendigen Annahmen eingeführt.

Danach werden wir typische \protect\hyperlink{lin-kennzahlen}{Kennzahlen
einer Regression} diskutieren und lernen, wie wir die Güte einer
Regression beurteilen können. Dieser Abschnitt enthält Auführeungen zum
\(R^2\), Standardfehlern von Schätzern, Konfidenzintervallen und
Residuenanalysen. Vieles ist eine Anwendung der im
\protect\hyperlink{stat-rep}{Anhang zur schließenden Statistik}
beschriebenen Konzepte.

Nachdem wir den \protect\hyperlink{stat-ablauf}{Ablauf einer
Regressionsanalyse} kurz zusammengefasst haben, generalisieren wir das
Gelernte noch für den \protect\hyperlink{lin-multi}{multiplen Fall},
also den Fall wenn wir mehr als eine \(x\)-Variable in unserem Modell
verwenden.

Am Schluss finden Sie ein konkretes
\protect\hyperlink{lin-beispiel}{Anwendungsbeispiel}, bei dem wir eine
lineare Regression von Anfang an implementieren. (\emph{Hinweis: dieser
Abschnitt wird später ergänzt})

\hypertarget{lin-grundlagen}{\section{Grundlagen der einfachen linearen
Regression}\label{lin-grundlagen}}

\subsection{Grundlegende Begriffe}\label{grundlegende-begriffe}

Wir betrachten zunächst den Fall der einfachen linearen Regression, das
heißt wir untersuchen den Zusammenhang zwischen zwei Variablen, sodass
unser theoretisches Modell folgendermaßen aussieht:

\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]

Alles was auf der linken Seite vom \texttt{=} steht bezeichnen wir als
die LHS (engl. \emph{left hand side}), alles auf der rechten Seite als
RHS (engl. \emph{right hand side}).

Wir bezeichnen \(Y_i\) als die \textbf{abhängige Variable} (auch:
\emph{Zielvariable} oder \emph{erklärte Variable}). Das ist die
Variable, die wir typischerweise erklären wollen. Im Eingangbeispiel
waren das die Konsumausgaben.

Wir bezeichnen \(x_i\) als die \textbf{unabhängige Variable} (auch:
\emph{erklärende Variable}). Das ist die Variable, mit der wir die
abhängige Variable erklären wollen. Im Eingangsbeispiel war das das BIP,
denn wir wollten über das BIP erklären wie viel Geld in einem Land für
Konsum ausgegeben wird.

Jetzt ist es natürlich so, dass wir die erklärenden Variablen nie ganz
genau beobachten können. Beim BIP sind z.B. Messfehler unvermeidlich,
und auch ansonsten wird es sicher einige Unvollkommenheiten im
Zusammenhang zwischen der erkärenden Variable und der abhängigen
Variable geben. Um der Tatsäche Rechnung zu tragen, dass der
Zusammenhang zwischen \(x_i\) und \(Y_i\) nicht exakt ist, führen wir
auf der rechten Seite der Gleichung noch die \textbf{Fehlerterme}
\(\epsilon_i\) ein.

Wir müssen für unser Modell annehmen, dass die Fehlerterme nur einen
nicht-systematischen Effekt auf \(Y_i\) haben, ansonsten müssten wir sie
explizit in unser Modell als erklärende Variable aufnehmen (dazu später
mehr). Sie absorbieren quasi alle Einflüsse auf \(Y_i\), die nicht über
\(x_i\) wirken. Damit wir die Funktion richtig schätzen können nehmen
wir für die Fehler ein bestimmtes Wahrscheinlichkeitsmodell an. In der
Regel nimmt man an, die Fehler seien i.i.d.\footnote{\(i.i.d.\) steht
  für \textit{identically and independently distributed}, d.h. die
  Fehler sind unabhängig voneinender und folgen alle der gleichen
  Verteilung.} normalverteilt mit Erwartungswert 0:
\(\epsilon_i \ i.i.d. \propto \mathcal{N}(0, \sigma^2)\).

Nun macht auch die Groß- und Kleinschreibung in der Gleichung mehr Sinn:
die \(x_i\) nehmen wir als beobachtete Größen hin und behandeln sie
nicht als Zufallsvariablen (ZV).\footnote{Wenn Sie Schwierigkeiten mit
  dem Konzept einer ZV haben, schauen Sie doch mal in den
  \protect\hyperlink{stat-stoch}{Anhang zur Wahrscheinlichkeitstheorie}.}
Die \(\epsilon_i\) sind als ZV definiert und da wir \(Y_i\) als eine
Funktion von \(x_i\) und \(\epsilon_i\) interpretieren sind die \(Y_i\)
auch ZV - und dementsprechend groß geschrieben. Die Fehlerterme werden
per Konvention nie groß geschrieben - wahrscheinlich weil sich das für
Fehler nicht gehört. Wer es ganz genau nehmen würde, müsste sie aber
auch groß schreiben, denn sie sind als ZV definiert und diese werden
eigentlich groß geschrieben.

Die Annahme von \(\mathbb{E}(\epsilon_i)=0\), also die Annahme, dass der
Erwartungswert für jeden Fehler gleich Null ist, ist neben der Annahme,
dass wir einen linearen Zusammenhang modellieren zentral: wir gehen
davon aus, dass unser Modell im Mittel stimmt. Unter dieser Annahme gibt
es keine \emph{systematischen} Abweichungen der \(Y_i\) von der über
\(\beta_0\) und \(\beta_1\) definierten Regressionsgeraden. Das ist
allerding nur der Fall, wenn bestimmte Annahmen erfüllt sind (dazu
später mehr).

\subsection{Schätzung mit der
Kleinste-Quadrate-Methode}\label{schatzung-mit-der-kleinste-quadrate-methode}

Nachdem wir unser Modell aufgestellt haben, möchten wir nun die
Parameter \(\beta_0\) und \(\beta_1\) \emph{schätzen}. Dazu brauchen wir
einen \emph{Schätzer}, also eine Funktion, die uns für die Daten, die
wir haben, den optimalen Wert für den gesuchten Parameter
gibt.\footnote{Wenn Ihnen das Konzept eines Schätzers sehr fremd ist,
  schauen Sie doch mal in den \protect\hyperlink{stat-rep}{Anhang zur
  schließenden Statistik}.} Wir suchen also nach den Werten für
\(\beta_0\) und \(\beta_1\) sodass die resultierende Gerade möglichst
nahe an allen \(Y_i\) Werten in folgendem Graph ist:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-8-1.pdf}

Wenn wir das händisch machen würden, könnten wir versuchen die Abstände
zwischen den einzelnen \(Y_i\) und der Regressionsgerade messen und
letztere so lange herumschieben, bis die Summe der Abstände möglichst
klein ist. In gewisser Weise ist das genau das, was wir in der Praxis
auch machen. Nur arbeiten wir nicht mit den Abständen als solchen, denn
dann würden sich positive und negative Abstände ja ausgleichen. Daher
quadrieren wir die Abstände, bevor wir sie summieren. Daher ist die
gängigste Methode, Werte für \(\beta_0\) und \(\beta_1\) zu finden auch
als \textbf{Kleinste-Quadrate Methode} (engl. \emph{ordinary least
squares} - OLS) bekannt.\footnote{Warum summiert man nicht die
  Absolutwerte der Abweichungen, sondern ihre quadrierten Werte? Das hat
  technische Gründe: mit quadrierten Werten lässt sich einfach leichter
  rechnen als mit Absolutwerten.} Die dadurch definierten Schätzer
\(\hat{\beta}_0\) und \(\hat{\beta}_1\) sind entsprechend als
\emph{OLS-Schätzer} bekannt.

Wir bezeichnen die Abweichung von \(Y_i\) zu Regressionsgeraden als
\emph{Residuum} \(r_i\). Wie in der Abbildung zu sehen ist, gilt für die
Abweichung von der Regressionsgeraden für die einzelnen \(Y_i\):
\(r_i=(Y_i-\hat{\beta}_0-\hat{\beta}_1x_i)\). Wir suchen also nach den
Werten für \(\hat{\beta}_0\) und \(\hat{\beta}_1\) für die die Summe
aller Residuen minimal ist:

\[\hat{\beta}_0, \hat{\beta}_1 =\text{argmin}_{\beta_0, \beta_1} \sum_{i=1}^n(Y_i-\beta_0-\beta_1x_i)^2\]

Dabei bedeutet \(\text{argmin}_{\beta_0, \beta_1}\): wähle die Werte für
\(\beta_0\) und \(\beta_1\), welche den nachfolgenden Ausdruck
minimieren.

Diesen Ausdruck kann man analytisch so lange umformen bis
gilt:\footnote{Jede*r Interessierte findet die genaue Herleitung sehr
  einfach im Internet oder in der Kursliteratur.}

\[\hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\]
und

\[\hat{\beta_0}=\bar{y}-\hat{\beta}_1\bar{x}\]

Zum Glück gibt es in R die Funktion \texttt{lm()}, welche diese
Berechnungen für uns übernimmt. Wir wollen dennoch anhand eines
Minimalbeispiels die Werte selber schätzen, um unser Ergebnis dann
später mit dem Ergebnis von \texttt{lm()} zu vergleichen.

Dazu betrachten wir folgenden (artifiziellen) Datensatz:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datensatz}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>     x    y
#> 1 0.1 2.58
#> 2 0.2 3.05
#> 3 0.3 4.98
#> 4 0.4 3.63
#> 5 0.5 3.83
\end{verbatim}

Zuerst berechnen wir \(\hat{\beta}_1\):

\[\hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\]

Dazu brauchen wir zunächst \(\bar{x}\), das ist in diesem Fall
\texttt{0.3}, und \(\bar{y}\), in unserem Fall \texttt{3.614}. Dann
können wir bereits rechnen:

\[\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=(0.1-0.3)(2.58-3.614)+(0.2-0.3)(3.05-3.614)\\+(0.3-0.3)(4.98-3.614)+(0.4-0.3)(3.63-3.614)+(0.5-0.3)(3.83-3.614)=0.308\]
und

\[\sum_{i=1}^n(x_i-\bar{x})^2=(0.1-0.3)^2+(0.2-0.3)^2+(0.3-0.3)^2+(0.4-0.3)^2+(0.5-0.3)^2=0.1\]
Daher:

\[\hat{\beta_1}=\frac{0.308}{0.1}=3.08\]

Entsprechend ergibt sich für \(\hat{\beta}_0\):

\[\hat{\beta_0}=\bar{y}-\hat{\beta}_1\bar{x}=3.614-3.08\cdot 0.3=2.69\]

In R können wir für diese Rechnung wie gesagt die Funktion \texttt{lm()}
verwenden. In der Praxis sind für uns vor allem die folgenden zwei
Argumente von \texttt{lm()} relevant: \texttt{formula} und
\texttt{data}.

Über \texttt{data} informieren wir \texttt{lm} über den Datensatz, der
für die Schätzung verwendet werden soll. Dieser Datensatz muss als
\texttt{data.frame} oder vergleichbares Objekt vorliegen.

Über \texttt{formula} teilen wir \texttt{lm} dann die zu schätzende
Formel mit. Die LHS und RHS werden dabei mit dem Symbol
\texttt{\textasciitilde{}} abgegrenzt. Wir können die Formel entweder
direkt als \texttt{y\textasciitilde{}x} an \texttt{lm()} übergeben, oder
wir speichern sie vorher als \texttt{character} und verwenden die
Funktion \texttt{as.formula()}. Entsprechend sind die folgenden beiden
Befehle äquivalent:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x, }\DataTypeTok{data =}\NormalTok{ datensatz)}
\NormalTok{reg_formel <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\StringTok{"y~x"}\NormalTok{)}
\KeywordTok{lm}\NormalTok{(reg_formel, }\DataTypeTok{data =}\NormalTok{ datensatz)}
\end{Highlighting}
\end{Shaded}

Der Output von \texttt{lm()} ist eine Liste mit mehreren interessanten
Informationen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{schaetzung <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x, }\DataTypeTok{data =}\NormalTok{ datensatz)}
\KeywordTok{typeof}\NormalTok{(schaetzung)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{schaetzung}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> lm(formula = y ~ x, data = datensatz)
#> 
#> Coefficients:
#> (Intercept)            x  
#>        2.69         3.08
\end{verbatim}

Die von \texttt{lm()} produzierte Liste enthält also die basalsten
Informationen über unsere Schätzung. Wir sehen unmittelbar, dass wir
vorher richtig gerechnet haben, da wir die gleichen Werte herausbekommen
haben.

Wenn wir noch genauer wissen wollen, wie die Ergebnisliste aufgebaut
ist, können wir die Funktion \texttt{str()} verwenden:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(schaetzung)}
\end{Highlighting}
\end{Shaded}

Da die Liste aber tatsächlich sehr lang ist, wird dieser Code hier nicht
ausgeführt. Es sei aber darauf hingewiesen, dass wir die geschätzen
Werte auf folgende Art und Weise direkt ausgeben lassen können:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{schaetzung[[}\StringTok{"coefficients"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> (Intercept)           x 
#>        2.69        3.08
\end{verbatim}

Dies ist in der Praxis häufig nützlich, z.B. wenn wir wie in der
Einleitung Werte mit Hilfe unseres Modell vorhersagen wollen. Zum
Abschluss hier noch einmal die Daten mit der von uns gerade berechneten
Regressionsgeraden:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-15-1.pdf}

Zwar wissen wir jetzt, wie wir eine einfache lineare Regression
schätzen, allerdings hört die Arbeit hier nicht auf! Unsere bisherige
Tätigkeiten korrespondieren zu der im
\protect\hyperlink{stat-rep}{Anhang zur schließenden Statistik}
beschriebenen \emph{Parameterschätzung}. Wir wollen aber auch noch die
anderen beiden Verfahren, Hypothesentests und Konfidenzintervalle,
abdecken und lernen wie wir die Güte unserer Schätzung besser
einschätzen können.

Zuvor wollen wir aber noch einmal genauer überprüfen, welche Annahmen
genau erfüllt sein müssen, damit die OLS-Prozedur auch funktioniert.

\subsection{Annahmen für den OLS
Schätzer}\label{annahmen-fur-den-ols-schatzer}

Das lineare Regressionsmodell wird sehr häufig in der sozioökonomischen
Forschung verwendet. Wie jedes statistisches Modell basiert es jedoch
auf bestimmten Annahmen, aus denen sich der sinnvolle Anwendungsbereich
des Modells ergibt. Wann immer wir die lineare Regression verwenden
sollten wir daher kritisch prüfen ob die entsprechenden Annahmen für den
Anwendungsfall plausibel sind. Daher wollen wir uns im Folgenden etwas
genauer mit diesen Annahmen vertraut machen.

Zwar schwankt die genaue Anzahl der Annahmen je nach Formulierung, in
der Essenz handelt es sich aber um folgende:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{A1: Erwartungswert der Fehler gleich Null}:
  \(\mathbb{E}(\epsilon=0)\) Diese Annahmen setzt voraus, dass
  \(\epsilon\) keine Struktur hat und im Mittel gleich Null ist. Das ist
  plausibel, denn würden wir Informationen über eine Struktur in
  \(\epsilon\) haben, bedeutet das, dass wir eine weitere erklärende
  Variable in das Modell aufnehmen könnten, welche diese Struktur
  explizit macht, oder die lineare Strukur des Modells ändern könnten.
  Die Annahme impliziert auch, dass der Zusammenhang zwischen der
  erklärten und erklärenden Variablen auch tatsächlich linear ist. Grob
  ausgedrückt: wir nehmen hier an, dass wir unser Modell clever
  spezifiziert haben.
\item
  \textbf{A2: Unabhängigkeit der Fehler mit den erklärenden Variablen}:
  Das bedeutet, dass es keinen systematischen Zusammenhang zwischen den
  Fehlern und den erklärenden Variablen gibt. Die Annahme wäre verletzt,
  wenn für größere Werte von \(x\) die Messgenauigkeit drastisch in eine
  Richtung hin abnehmen würde.
\item
  \textbf{A3: Konstante Varianz der Fehlerterme} Diese Annahme wird auch
  \textbf{Homoskedastizität} genannt und sagt einfach:
  \(Var(\epsilon_i)=\sigma^2\forall i\)
\item
  \textbf{A4: Keine Autokorrelation der Fehlerterme} Die Annahme
  verlangt, dass die Fehler nicht untereinander korreliert sind:
  \(Cov(\epsilon_i, \epsilon_j)=0 \forall i,j\). Das kann vor allem ein
  Problem sein, wenn die gleichen erklärenden Variablen zu
  unterschiedlichen Zeitpunkten gemessen werden.
\item
  \textbf{A5: Normalverteilung der Fehlerterme:}
  \(\epsilon\propto\mathcal{N}(0,\sigma^2)\) Niese Annahme ist notwendig
  für die Hypothesentests und Berechnung von Konfidenzintervallen für
  die Schätzer.
\end{enumerate}

Diese Annahmen bilden die Grundlage für das so genannte
\textbf{Gauss-Markov-Theorem}, gemäß dem der OLS-Schätzer für lineare
der beste konsistente Schätzer ist, den wir finden können. In anderen
Worten: OLS ist der BLUE - \emph{Best Linear Unbiased Estimator}. Mit
konsistent ist gemeint, dass die Schätzer im Mittel den wahren Wert
\(\beta_i\) treffen, also der Erwartungswert jedes Schätzers
\(\hat{\beta}_i\) der wahre Werte \(\beta\) ist. Mit ``der beste''
meinen wir ``den effizientesten'' im Sinne einer minimalen Varianz. Was
mit der Varianz eines Schätzers gemeint wird, wird weiter unten erklärt.

Dabei gilt, dass der OLS-Schätzer bereits unter den Annahmen \textbf{A1}
und \textbf{A2} erwartungstreu ist. Die weiteren Annahmen sind notwendig
um die Effizienz sicherzustellen, und die Standardfehler für die
Schätzer berechnen zu können. Es gibt Varianten von OLS in der man die
Abhängigkeit von diesen Annahmen reduzieren kann. Wir lernen solcherlei
Methoden später in der Veranstaltung kennen.

Das bedeutet aber auch, dass wann immer eine oder mehrere Annahmen
verletzt ist, wir unseren Ergebnissen nur bedingt vertrauen können und
einige Ergebnisse und Kennzahlen unserer Regression möglicherweise
irreführend sind. Daher sollte zu jeder Regressionsanalyse die
Überprüfung der Annahmen dazugehören. Die notwendigen Methoden dazu
lernen wir weiter unten kennen.

Es ist wichtig zu beachten, dass wir eine Regression mit OLS schätzen
können und keine Fehlermeldungen bekommen, obwohl die Annahmen für OLS
nicht erfüllt sind. In diesem Fall sind die Ergebnisse jedoch
möglicherweise irreführend. Daher ist es immer wichtig, die Korrektheit
der Annahmen zu überprüfen und weitere Kennzahlen der Regression zu
betrachten Methoden dafür lernen wir nun genauer kennen.

\hypertarget{lin-kennzahlen}{\section{Kennzahlen in der linearen
Regression}\label{lin-kennzahlen}}

\subsection{\texorpdfstring{Erklärte Varianz und das
\(R^2\)}{Erklärte Varianz und das R\^{}2}}\label{erklarte-varianz-und-das-r2}

Als erstes wollen wir fragen, `wie gut' unser geschätztes Modell unsere
Daten erklären kann. In der ökonometrischen Praxis ist ein Weg diese
Frage zu beantworten zu fragen, wie viel `Variation' der abhängigen
Variable \(Y_i\) durch die Regression erklärt wird. Als Maß für die
Variation wird dabei die Summe der quadrierten Abweichungen von \(Y_i\)
von seinem Mittelwert verwendet, auch \(TSS\) (für engl. \emph{Total Sum
of Squares} - `Summe der Quadrate der Totalen Abweichungen') genannt:

\[TSS=\sum_{i=1}^n(Y_i-\bar{Y})^2\] In R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tss <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((datensatz}\OperatorTok{$}\NormalTok{y }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(datensatz}\OperatorTok{$}\NormalTok{y))}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\NormalTok{tss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 3.30012
\end{verbatim}

Diese Werte sind in folgender Abbildung für unseren Beispieldatensatz
von oben grafisch dargestellt:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-17-1.pdf}

Die TSS wollen wir nun aufteilen in eine Komponente, die in unserer
Regression erklärt wird, und eine Komponente, die nicht erklärt werden
kann. Bei letzterer handelt es sich um die Abweichungen der geschätzten
Werte \(\hat{Y_i}\) und den tatsächlichen Werten \(Y_i\), den oben
definierten Residuen \(r_i\). Entsprechend definieren wir die
\emph{Residual Sum of Squares (RSS)} (dt.: \emph{Residuenquadratsumme})
als:

\[RSS=\sum_i^n\epsilon_i^2\] In R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rss <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(schaetzung[[}\StringTok{"residuals"}\NormalTok{]]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\NormalTok{rss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 2.35148
\end{verbatim}

Diese sehen wir hier:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-19-1.pdf}

Was noch fehlt sind die \emph{Explained Sum of Squares (ESS)} (dt.
\emph{Summe der Quadrate der Erklärten Abweichungen}), also die
Variation in der abhängigen Variable, die durch die Regression erklärt
wird. Dabei handelt es sich um die quadrierte Differenz zwischen
\(\bar{Y}\) und den geschätzten Werten \(\hat{Y}\):

\[ESS=\sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2\] Diese ergibt sich in R als:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ess <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((schaetzung[[}\StringTok{"fitted.values"}\NormalTok{]] }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(datensatz}\OperatorTok{$}\NormalTok{y))}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\NormalTok{ess}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.94864
\end{verbatim}

Und grafisch:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-21-1.pdf}

Für die drei gerade eingeführten Teile der Gesamtvarianz gilt im
Übrigen:

\[TSS=ESS+RSS\]

Aus diesen Werten können wir nun das \textbf{Bestimmtheitsmaß} \(R^2\)
berechnen, welches Informationen darüber gibt, welchen Anteil der
Variation in \(Y_i\) durch unser Modell erklärt wird:

\[R^2=\frac{ESS}{TSS}=1-\frac{RSS}{TSS}\]

Wir können das für unseren Anwendungsfall natürlich händisch berechnen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r_sq_manual <-}\StringTok{ }\NormalTok{ess }\OperatorTok{/}\StringTok{ }\NormalTok{tss}
\NormalTok{r_sq_manual}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.2874562
\end{verbatim}

Leider wird diese Größe im Output von \texttt{lm()} direkt nicht
ausgegeben. Wir können aber einen ausführlicheren Output unserer
Regression mit der Funktion \texttt{summary()} erstellen, dort ist das
\(R^2\) dann auch enthalten:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{info_schaetzung <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(schaetzung)}
\NormalTok{info_schaetzung[[}\StringTok{"r.squared"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.2874562
\end{verbatim}

In unserem Fall erklärt unser Modell als ca. 28\% der Gesamtvarianz der
erklärten Variable. In einer sozialwissenschaftlichen Anwendung wäre das
nicht so wenig, denn aufgrund der vielen Faktoren, die hier eine Rolle
spielen, darf man keine zu hohen Werte für \(R^2\) erwarten. Vielmehr
legen sehr hohe Werte eine gewisse Skepsis nahe, ob nicht eher ein
tautologischer Zusammenhang geschätzt wurde.

Ein großer Nachteil vom \(R^2\) ist, dass es größer wird sobald wir
einfach mehr erklärende Variablen in unsere Regression aufnehmen. Warum?
Eine neue Variable kann unmöglich \(TSS\) verändern (denn die
erklärenden Variablen kommen in der Formel für TSS nicht vor), aber
erhöht immer zumindest ein bisschen die ESS. Wenn unser alleiniges Ziel
also die Maximierung von \(R^2\) wäre, dann müssten wir einfach ganz
viele erklärenden Variablen in unser Modell aufnehmen. Das kann ja nicht
Sinn sozioökonomischer Forschung sein!

Zur Lösung dieses Problems wurde das adjustierte \(R^2\) entwickelt, was
bei Regressionen auch standardmäßig angegeben wird. Hier korrigieren wir
das \(R^2\) mit Hilfe der \textbf{Freiheitsgrade} (engl. \emph{degrees
of freedom}). Die Freiheitsgerade sind die Differenz zwischen
Beobachtungen und Anzahl der zu schätzenden Parameter und werden in der
Regel mit \(df\) bezeichnet. In unserem Fall hier ist \(N=5\) und
\(K=3\), da mit \(\beta_0\) und \(\beta_1\) zwei Parameter geschätzt
werden.

Das adjustierte \(R^2\), häufig als \(\bar{R}^2\) bezeichnet, ist
definiert als:

\[\bar{R}^2=1-\frac{\sum_{i=1}^n\epsilon^2/(N-K-1)}{\sum_{i=1}^n(Y_i-\bar{Y})^2/(N-1)}\]
Um dieses Maß aus unserem Ergebnisobjekts auszugeben schreiben wir:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{info_schaetzung[[}\StringTok{"adj.r.squared"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.04994162
\end{verbatim}

Leider hat es keine so eindeutige Interpretation wie das \(R^2\), aber
es sollte immer gemeinsam mit letzterem beachtet werden. Häufig
vergleicht man das \(\bar{R}^2\) vor und nach der Inklusion einer
weiteren erklärenden Variable. Wenn \(\bar{R}^2\) steigt geht man häufig
davon aus, dass sich die Inklusion auszahlt, allerdings sind das `nur'
Konventionen. Man sollte nie eine Variabel ohne gute theoretische
Begründung aufnehmen!

\subsection{Hypothesentests und statistische
Signifikanz}\label{hypothesentests-und-statistische-signifikanz}

Wie sicher können wir uns mit den geschätzten Parametern für \(\beta_0\)
und \(\beta_1\) sein? Wenn z.B. \(\hat{\beta}_1>0\) bedeutet das
wirklich, dass wir einen positiven Effekt gefunden haben? Immerhin sind
ja unsere Fehler ZV und vielleicht haben wir einfach zufällig eine
Stichprobe erhoben, wo der Effekt von \(x_1\) positiv erscheint,
tatsächlich aber kein Effekt existiert? Um die Unsicherheit, die mit der
Parameterschätzung einhergeht, zu quantifizieren können wir uns die
Annahme, dass unsere Fehler normalverteilt sind, zu Nutze machen und
testen wir plausibel die tatsächliche Existenz eines Effekts ist.

Wir verlassen nun also das Gebiet der reinen Parameterschätzung und
beschäftigen uns mit Hypothesentests und Konfidenzintervallen für unsere
Schätzer \(\hat{\beta}_0\) und \(\hat{\beta}_1\). Das ist analog zu den
im \protect\hyperlink{stat-rep}{Anhang zur schließenden Statistik}
besprochenen Herangehensweisen.

Wir wissen bereits, dass es sich bei unseren Schätzern \(\hat{\beta}_0\)
und \(\hat{\beta}_1\) um ZV handelt. Aber welcher Verteilung folgen sie?
Tatsächlich können wir das aus unseren oben getroffenen Annahmen direkt
ableiten:

\[\hat{\beta}_0 \propto \mathcal{N}\left(\beta_0, \sigma^2\left( \frac{1}{n} +
\frac{\bar{x}^2}{SS_X}\right) \right), \quad SS_X=\sum_{i=1}^n(x_i-\bar{x})^2\\
\hat{\beta}_1 = \mathcal{N}\left(\beta_1, \frac{\sigma^2}{SS_X}\right)\]

An der Tatsache, dass \(\mathbb{E}(\hat{\beta_i})=\beta_i\) sehen wir,
dass die Schätzer \emph{erwartungstreu} sind. Es ist dann plausibel die
\emph{Genauigkeit} oder \emph{Effizienz} eines Schätzers durch seine
Varianz zu messen: wenn ein Schätzer eine große Varianz hat bedeutet
das, dass wir bei dem einzelnen Schätzwert eine große Unsicherheit
haben, ob der Schätzer tatsächlich nahe an seinem Erwartungswert liegt.
Am besten kann man das an einem simulierten Beispiel illustrieren.

\begin{quote}
\textbf{Beispiel: Die Varianz von \(\hat{\beta}_1\)}: Im folgenden
kreieren wir einen künstlichen Datensatz, bei dem wir den wahren DGP
kennen. Dieser wahre DGP wird beschrieben durch:
\end{quote}

\[Y_i=\beta_0+\beta_1 x_i + \epsilon_i, \quad \epsilon_i\propto\mathcal{N}(0,5)\]

\begin{quote}
Wenn wir nun mit diesem DGP mehrere Datensätze kreieren, sieht natürlich
jeder Datensatz anders aus. Schließlich sind die \(\epsilon_i\)
zufällig. Dennoch wissen wir, dass, da unsere Schätzer erwartungstreu
sind, sie im Mittel die wahren Werte von \(\beta_0\) und \(\beta_1\)
treffen sollten. Aber wie sehr streuen die geschätzten Werte um diesen
wahren Wert? Zunächst erstellen wir den künstlichen Datensatz. Dazu
spezifizieren wir zunächst die Grundstruktur des DGT:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{true_DGP <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, b0, b1)\{}
\NormalTok{  y <-}\StringTok{ }\NormalTok{b0 }\OperatorTok{+}\StringTok{ }\NormalTok{b1}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(x), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{)}
  \KeywordTok{return}\NormalTok{(y)}
\NormalTok{\}}
\NormalTok{beta_0_wahr <-}\StringTok{ }\DecValTok{3}
\NormalTok{beta_1_wahr <-}\StringTok{ }\DecValTok{2}
\NormalTok{sample_size <-}\StringTok{ }\DecValTok{100}
\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(sample_size, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Nun erstellen wir mit Hilfe einer Schleife 1000 Realisierungen der
Daten. Wir können uns das wie 1000 Erhebungen vorstellen. Für jede
dieser Realisierungen schätzen wir dann die lineare Regressionsgleichung
von oben:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n_datensaetze <-}\StringTok{ }\DecValTok{1000}
\NormalTok{beta_0_estimates <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, n_datensaetze)}
\NormalTok{beta_1_estimates <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, n_datensaetze)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_datensaetze)\{}
\NormalTok{  daten_satz <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{x =}\NormalTok{ x,}
    \DataTypeTok{y =} \KeywordTok{true_DGP}\NormalTok{(x, beta_0_wahr, beta_1_wahr)}
\NormalTok{  )}
\NormalTok{  schaetzung_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x, }\DataTypeTok{data =}\NormalTok{ daten_satz)}
\NormalTok{  beta_0_estimates[i] <-}\StringTok{ }\NormalTok{schaetzung_}\DecValTok{2}\NormalTok{[[}\StringTok{"coefficients"}\NormalTok{]][}\DecValTok{1}\NormalTok{]}
\NormalTok{  beta_1_estimates[i] <-}\StringTok{ }\NormalTok{schaetzung_}\DecValTok{2}\NormalTok{[[}\StringTok{"coefficients"}\NormalTok{]][}\DecValTok{2}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Nun können wir die Streuung der Schätzer direkt visualisieren:
\end{quote}

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-27-1.pdf}

\begin{quote}
Wie wir sehen, treffen die Schätzer im Mittel den richtigen Wert,
streuen aber auch. Die Varianz gibt dabei die Breite des jeweiligen
Histograms an und je strärker die relativen Häufigkeiten des geschätzten
Wertes um den wahren Wert konzentriert sind, also desto geringer die
Varianz, desto genauer ist der Schätzer.
\end{quote}

Ein Maß für die Genauigkeit eines Schätzers ist sein
\textbf{Standardfehler}. Für \(\hat{\beta}_1\) ist dieser wie oben
beschrieben definiert als \(\frac{\sigma}{\sqrt{SS_X}}\). Da \(\sigma\)
(die Varianz der Fehler) nicht bekannt ist, müssen wir sie aus den Daten
schätzen. Das geht mit \(\frac{1}{n-2}\sum_{i=1}^n\epsilon_i^2\), wobei
die detaillierte Herleitung hier nicht diskutiert wird. Grundsätzlich
handelt es sich hier um die empirische Varianz. Das \(n-2\) kommt von
den um zwei reduzierten Freiheitsgraden dieser Schätzung.

Dieser Standardfehler ist ein erstes Maß für die Genauigkeit des
Schätzers. Er wird aufgrund seiner Wichtigkeit auch in der Summary jeder
Schätzung angegeben. Hier betrachten wir die Schätzung aus dem
einführenden Beispiel:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(schaetzung_bip)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> lm(formula = Konsum ~ BIP, data = daten)
#> 
#> Residuals:
#>     Min      1Q  Median      3Q     Max 
#> -39.330  -8.601   1.761  14.769  31.306 
#> 
#> Coefficients:
#>               Estimate Std. Error t value Pr(>|t|)    
#> (Intercept) -1.841e+02  4.626e+01  -3.979  0.00157 ** 
#> BIP          7.064e-01  7.827e-03  90.247  < 2e-16 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 20.29 on 13 degrees of freedom
#> Multiple R-squared:  0.9984, Adjusted R-squared:  0.9983 
#> F-statistic:  8145 on 1 and 13 DF,  p-value: < 2.2e-16
\end{verbatim}

Sie sind hier unter \texttt{Std.\ Error} zu finden. Wir können diese
Information jedoch noch weiter verwenden und Hypothesen im Zusammenhang
mit den Schätzern testen. Eine besonders relevante Frage ist immer ob
ein bestimmter \emph{Schätzer} signifikant von 0 verschieden ist. Dazu
können wir fragen: ``Wie wahrscheinlich ist es, gegeben der Daten, dass
\(\beta_i\) gleich Null ist?''.

Das ist die klassische Frage für einen Hypothesentest\footnote{Lesen Sie
  noch einmal im \protect\hyperlink{stat-rep}{Anhang zur schließenden
  Statistik} nach, wenn Sie nicht mehr wissen was ein Hypothesentest
  ist.} mit \(H_0: \beta_0=0\) und \(H_1: \beta_0 \neq 0\).

Für einen Hypothesentest brauchen wir zunächst eine Teststatistik, also
die Verteilung für den Schätzer wenn \(H_0\) wahr wäre. Da wir annehmen,
dass die Fehlerterme in unserem Fall normalverteilt sind, ist das in
unserem Falle eine \(t\)-Verteilung mit \(n-2\)
Freiheitsgraden.\footnote{Warum jetzt genau eine \(t\)-Verteilung und
  keine Normalverteilung? Das liegt daran, dass wir die Varianz unserer
  Fehler \(\sigma\) nicht beobachten können und durch \(\hat{\sigma}\)
  geschätzt haben. Das führt dazu, dass die resultierende Teststatistik
  nicht mehr normalverteilt ist. Mir zunehmendem Stichprobenumfang wird
  die Abweichung immer irrelevanter, jedoch ist die t-Verteilung so
  einfach zu handhaben, dass man sie eigentlich immer benutzen kann.}
Damit können wir überprüfen wie wahrscheinlich unser Schätzwert unter
der \(H_0\) wäre. Wenn er sehr unwahrscheinlich wäre, würden wir \(H_0\)
verwerfen.

Die Wahrscheinlichkeit, dass wir unseren Schätzer unseren Schätzer
gefunden hätten, wenn \(H_0\) wahr wäre wird durch den \(p\)-Wert des
Schätzers angegeben. Dieser findet sich in der Spalte
\texttt{Pr(\textgreater{}\textbar{}t\textbar{})}. In unserem Fall mit
\(\hat{\beta}_1\) ist dieser Wert mit \(2\cdot 10^{-16}\) extrem klein.
Das bedeutet, wenn \(H_0: \beta_1=0\) wahr wäre, würden wir unseren Wert
für \(\hat{\beta}_1\) mit einer Wahrscheinlichkeit nahe Null beobachten.
Es erscheint daher sehr unplausibel, dass \(\beta_1=0\). Tatsächlich
würden wir diese Hypothese auf quasi jedem beliebigen Signifikanzniveau
verwerfen. Daher ist der Schätzer in der Zusammenfassung mit drei
Sternen gekennzeichnet:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(schaetzung_bip)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> lm(formula = Konsum ~ BIP, data = daten)
#> 
#> Residuals:
#>     Min      1Q  Median      3Q     Max 
#> -39.330  -8.601   1.761  14.769  31.306 
#> 
#> Coefficients:
#>               Estimate Std. Error t value Pr(>|t|)    
#> (Intercept) -1.841e+02  4.626e+01  -3.979  0.00157 ** 
#> BIP          7.064e-01  7.827e-03  90.247  < 2e-16 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 20.29 on 13 degrees of freedom
#> Multiple R-squared:  0.9984, Adjusted R-squared:  0.9983 
#> F-statistic:  8145 on 1 and 13 DF,  p-value: < 2.2e-16
\end{verbatim}

Grundsätzlich gilt, dass wir \(H_0: \beta_i = 0\) auf dem
\(\alpha\)-Signifikanzniveau verwerfen können wenn \(p<1-\alpha\). Wenn
wir \(H_0: \beta_i\) auf dem Signifikanzniveau von mindestens
\(\alpha=0.05\) verwerfen können, sprechen wir von einem signfikanten
Ergebnis. In unserem Beispiel der Konsumfunktion sind also sowohl die
Schätzer \(\beta_0\) und \(\beta_1\) hochsignifikant und wir können,
under den oben getroffenen Annahmen, mit großer Sicherheit davon
ausgehen, dass beide von Null verschieden sind.

Dabei ist jedoch wichtig darauf hinzuweisen, dass \emph{statistische
Signifikanz} nicht mit \emph{sozioökonomischer Relevanz} zu tun hat: ein
Effekt kann hochsignifikant, aber extrem klein sein. Dennoch ist die
Signifikanz eine wichtige und häufig verwendete Kennzahl für jede
lineare Regression. Gleichzeitig ist die wissenschaftliche Praxis, nur
Studien mit signifikanten Ergebnissen ernst zu nehmen, sehr
problematisch, Stichwork
\href{https://de.wikipedia.org/wiki/P-Hacking}{p-Hacking}. Wir
diskutieren dieses Problem später im Rahmen der Veranstaltung

\subsection{Konfidenzintervalle für die
Schätzer}\label{konfidenzintervalle-fur-die-schatzer}

Ausgehend von den Überlegungen zur Signifikanz können wir nun
\textbf{Konfidenzintervalle} für unsere Schätzer konstruieren. Wie im
\protect\hyperlink{stat-rep}{Anhang zur schließenden Statistik} genauer
erläutert besteht ein ein Konfidenzintervall \(I_{\alpha}\) aus allen
geschätzten Parameterwerten, für die wir bei einem zweiseitigen
Hypothesentest zum Signifikanzniveau \(\alpha\) die Nullhypothese
\(\beta_i=0\) nicht verwerfen werden können.

Um diese Intervalle für eine Schätzun in R zu konstruieren verwenden wir
die Funktion \texttt{confint}, die als erstes Argument das geschätzte
Modell und als Argument \texttt{level} das Signifikanzniveau
\(1-\alpha\) akzeptiert:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(schaetzung_bip, }\DataTypeTok{level=}\FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>                    2.5 %      97.5 %
#> (Intercept) -284.0209372 -84.1350533
#> BIP            0.6894978   0.7233183
\end{verbatim}

Für \(\hat{\beta}_1\) ist das 95\%-Konfidenzintervall also
\([0.69, 0.72]\). Das bedeutet, dass wenn der zugrundeliegende
Datengenerierungsprozess sehr häufig wiederholt werden würde, dann würde
95\% der so für \(\hat{\beta}_1\) berechneten 95\%-Konfidenzintervalle
\(\beta_1\) enthalten.

\subsection{Zur Rolle der
Stichprobengröße}\label{zur-rolle-der-stichprobengroe}

Um die Rolle der Stichprobengröße besser beurteilen zu können, verwenden
wir hier einen künstlich hergestellten Datensatz für den wir die
`wahren' Werte \(\beta_0\) und \(\beta_1\) kennen:\footnote{Die Befehle
  sollten Ihnen weitgehen bekannt sein. Die Funktion \texttt{set.seed()}
  verwenden wir um den
  \href{https://de.wikipedia.org/wiki/Mersenne-Twister}{Zufallszahlengenerator
  von R} so zu kalibrieren, dass bei jedem Durchlaufen des Skripts die
  gleichen Realisierungen der ZV gezogen werden und die Ergebnisse somit
  reproduzierbar sind.}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{wahres_b0 <-}\StringTok{ }\DecValTok{3}
\NormalTok{wahres_b1 <-}\StringTok{ }\FloatTok{1.4}

\NormalTok{stichproben_n <-}\StringTok{ }\DecValTok{50}
\NormalTok{x <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\NormalTok{stichproben_n }\OperatorTok{*}\StringTok{ }\FloatTok{0.1}
\NormalTok{fehler <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(stichproben_n, }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{3}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, stichproben_n)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{stichproben_n)\{}
\NormalTok{  y[i] <-}\StringTok{ }\NormalTok{wahres_b0 }\OperatorTok{+}\StringTok{ }\NormalTok{wahres_b1}\OperatorTok{*}\NormalTok{x[i] }\OperatorTok{+}\StringTok{ }\NormalTok{fehler[i]}
\NormalTok{\}}
\NormalTok{datensatz <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ x,}
  \DataTypeTok{y =}\NormalTok{ y}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wie wir im folgenden sehen ist die geschätzte Gerade nicht exakt
deckungsgleich zur `wahren' Gerade, aber doch durchaus nahe dran:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-32-1.pdf}

Grundsätzlich gilt, dass die erwartete Deckung der beiden dann höher ist
wenn (1) die Annahmen für die einfache lineare Regression gut erfüllt
sind und (2) die Stichprobe groß ist. Im Moment sind wir in einer
Luxussituation, da wir die `wahre' Gerade kennen: wir haben ja den
Datensatz für den wir die Gerade schätzen selbst erstellt. In der Praxis
bleibt uns nichts anderes üblich als (1) so gut es geht zu überprüfen
und die restliche Unsicherheit so gut es geht zu quantifizieren. Im
folgenden wollen wir uns genauer anschauen welche Methoden uns dafür zur
Verfügung stehen. Vorher wollen wir uns aber noch ansehen, wie eine
größere Stichprobe die Schätzgenauigkeit beeinflusst, wobei wir die
formale Begründung warum das so ist bis ans Ende des Kapitels
aufschieben:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-34-1.pdf}

\subsection{Residuenanalyse}\label{residuenanalyse}

Eine sehr hilfreiche Art, die Modellannahmen von oben zu überprüfen ist
die Analyse der Residuen. Diese sind im Ergebnisobjekt der Regression
gespeichert:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{schaetzung_bip[[}\StringTok{"residuals"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

Wir wollen nun die Residuen verwenden um die folgenden Annahmen unseres
Regressionsmodells zu überprüfen:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{A1:} \(\mathbb{E}(\epsilon_i)=0\)
\item
  \textbf{A3: } \(Var(\epsilon_i)=\sigma^2\forall i\)
\item
  \textbf{A4: } \(Cov(\epsilon_i, \epsilon_j)=0 \forall i,j\)
\item
  \textbf{A5:} \(\epsilon \propto \mathcal{N}(0, \sigma^2)\)
\end{enumerate}

Um die ersten drei Annahmen zu überpüfen bilden wir die \(\epsilon_i\)
gegen \(\hat{Y}\) ab und erhalten so den so genannten
\textbf{Tukey-Anscombe-Plot}:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-36-1.pdf}

Hier geht es nun darum eine Struktur zu erkennen. Wenn alle Annahmen
korrekt sind, sehen wir nur eine unstrukturierte Punktewolke. In dem
vorliegenden Fall können wir aufgrund der wenigen Datenpunkte den Plot
aber nur mit großer Schwerierigkeit interpretieren - auch deswegen sind
große Stichproben immer Besser. Es scheint aber so zu sein, dass die
Varianz der Fehler mit \(x_i\) steigt, also A3 möglicherweise verletzt
ist - zum Glück kann man den OLS-Schätzer leicht modifizieren um damit
umzugehen. Ansonsten ist keine Struktur zumindest unmittelbar
ersichtlich.

Als nächstes wollen die Annahme normalverteilter Residuen überprüfen.
Das geht mit dem so genannten \textbf{Q-Q-Plot}:

\includegraphics{Chap-linmodels_files/figure-latex/unnamed-chunk-37-1.pdf}

Bei normalverteilten Residuen würden die Punkte möglichst exakt auf der
Linie liegen. Das ist hier nur bedingt der Fall, deswegen sollten wir
skeptisch bezüglich aller Ergebnisse sein, die auf der
Normalverteilungsannahme aufbauen, also auf den Standardfehlern,
\(p\)-Werten und den Konfidenzintervallen.

Natürlich gibt es auch noch weitere Probleme, die bei einer linearen
Regression auftreten können. So ist es immer ein Problem, wenn wir eine
wichtige erklärende Variable in unserem Modell vergessen haben
(\textbf{omitted variable bias}), da deren Effekt dann durch die
Fehlerterme abgefangen wird und zu einer Verletzung von \textbf{A1} und
\textbf{A2} führt.

Ein großes Problem stellt auch die so genannte \textbf{Simultanität}
dar: diese tritt auf, wenn zwischen erklärter und erklärender Variable
ein wechelseitiges kausales Verhältnis besteht. Wir sprechen dann auch
von einem \textbf{Endogenitätsproblem}, welches leider sehr häufig
auftritt und letztendlich vor allem theoretisch identifiziert werden
muss.

\hypertarget{stat-ablauf}{\section{Zum Ablauf einer
Regression}\label{stat-ablauf}}

Insgesamt ergibt sich aus den eben beschriebenen Schritten also
folgendes Vorgehen bei einer Regression:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Aufstellen des statistischen Modells
\item
  Erheben und Aufbereitung der Daten
\item
  Schätzen des Modells
\item
  Überprüfung der Modellannahmen durch die Residuenanalyse
\item
  Inspektion der relevanten Kennzahlen wie \(R^2\) und der statistischen
  Signifikanz der geschätzten Werte; falls relevant: Angabe von
  Konfidenzintervallen
\end{enumerate}

\hypertarget{lin-multi}{\section{Multiple lineare
Regression}\label{lin-multi}}

Zum Abschluss wollen wir noch das bislang besprochene für den Fall von
mehreren erklärenden Variablen generalisieren. In der Praxis werden Sie
nämlich so gut wie immer mehr als eine erklärende Variable verwenden.
Zwar sind die resultierenden Plots häufig nicht so einfach zu
interpretieren wie im Fall der einfachen Regression, das Prinzip ist
jedoch quasi das gleiche. Zudem ist die Implementierung in R nicht
wirklich schwieriger.

Im folgenden wollen wir einen Beispieldatensatz verwenden, in dem
Informationen über Informationen über die Preise von ökonomischen
Journalen gesammelt sind:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{journal_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"data/tidy/journaldaten.csv"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Titel, Preis, Seitenanzahl, Zitationen)}
\KeywordTok{head}\NormalTok{(journal_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>                                                  Titel Preis Seitenanzahl
#> 1:                   Asian-Pacific Economic Literature   123          440
#> 2:           South African Journal of Economic History    20          309
#> 3:                             Computational Economics   443          567
#> 4: MOCT-MOST Economic Policy in Transitional Economics   276          520
#> 5:                          Journal of Socio-Economics   295          791
#> 6:                                    Labour Economics   344          609
#>    Zitationen
#> 1:         21
#> 2:         22
#> 3:         22
#> 4:         22
#> 5:         24
#> 6:         24
\end{verbatim}

In einer einfachen linearen Regression könnten wir z.B. folgendes Modell
schätzen:

\[PREIS_i = \beta_0 + \beta_1 SEITEN + \epsilon\]

Das würden wir mit folgendem Befehl in R implementieren:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Preis}\OperatorTok{~}\NormalTok{Seitenanzahl, }\DataTypeTok{data=}\NormalTok{journal_data)}
\KeywordTok{summary}\NormalTok{(reg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> lm(formula = Preis ~ Seitenanzahl, data = journal_data)
#> 
#> Residuals:
#>      Min       1Q   Median       3Q      Max 
#> -1157.56  -190.54   -40.72   179.59  1329.30 
#> 
#> Coefficients:
#>              Estimate Std. Error t value Pr(>|t|)    
#> (Intercept)  56.74315   53.85199   1.054    0.293    
#> Seitenanzahl  0.43610    0.05757   7.575 1.89e-12 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 336.5 on 178 degrees of freedom
#> Multiple R-squared:  0.2438, Adjusted R-squared:  0.2395 
#> F-statistic: 57.38 on 1 and 178 DF,  p-value: 1.888e-12
\end{verbatim}

Allerdings macht es auch Sinn anzunehmen, dass beliebte Journale teurer
sind. Daher würden wir gerne die Anzahl der Zitationen in das obige
Modell als zweite erklärende Variable aufnehmen. In diesem Fall würden
wir mit einem \emph{multiplen} linearen Modell arbeiten:

\[PREIS_i = \beta_0 + \beta_1 SEITEN + \beta_2 ZITATE + \epsilon\]

Tatsächlich ist die einzige Änderungen, die wir auf der technischen
Seite machen müssen, die Inklusion der neuen erklärenden Variable in die
Schätzgleichung:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Preis}\OperatorTok{~}\NormalTok{Seitenanzahl }\OperatorTok{+}\StringTok{ }\NormalTok{Seitenanzahl }\OperatorTok{+}\StringTok{ }\NormalTok{Zitationen, }\DataTypeTok{data=}\NormalTok{journal_data)}
\end{Highlighting}
\end{Shaded}

Hierbei ist zu beachten, dass das \texttt{+} nicht im additiven Sinne
gemeint ist, sondern in der Logik einer Regressionsgleichung.

Wenn wir uns die Zusammenfassung dieses Objekts anschauen, sehen wir
einen sehr ähnlichen Output als für den einfachen linearen Fall, nur
dass wir eine weitere Zeile für die neue erklärende Variable haben:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(reg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> lm(formula = Preis ~ Seitenanzahl + Seitenanzahl + Zitationen, 
#>     data = journal_data)
#> 
#> Residuals:
#>      Min       1Q   Median       3Q      Max 
#> -1346.70  -173.48   -38.83   138.32  1259.00 
#> 
#> Coefficients:
#>              Estimate Std. Error t value Pr(>|t|)    
#> (Intercept)  -3.72002   52.80969  -0.070    0.944    
#> Seitenanzahl  0.59413    0.06477   9.173  < 2e-16 ***
#> Zitationen   -0.10872    0.02393  -4.544 1.02e-05 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 319.3 on 177 degrees of freedom
#> Multiple R-squared:  0.3228, Adjusted R-squared:  0.3151 
#> F-statistic: 42.18 on 2 and 177 DF,  p-value: 1.049e-15
\end{verbatim}

Zwei Punkte sind bei der multiplen Regression zu beachten: Erstens sind
die geschätzten Effekte als \textbf{isolierte Effekte} zu
interpretieren, also in einer Situation in der alle anderen erklärenden
Variablen fix gehalten werden. Das ist die berühmte \emph{ceteris
paribus} Formen.

Der geschätzte Wert für \texttt{Seitenanzahl} sagt uns dementsprechend:
``\emph{Ceteris paribus}, also alle anderen Einflussfaktoren fix
gehalten, geht ein um eine Seite dickeres Journal mit einem um \(0.6\)
Dollar höherem Abo-Preis einher.'' Beachten Sie den relevanten
Unterschied zur einfachen Regression, die sehr wahrscheinlich unter dem
oben angesprochenen \emph{omitted variable bias} gelitten hat.

Der zweite zu beachtende Aspekt bezieht sich auf die Korrelation der
verschiedenen erklärenden Variablen. Die Annahmen für OLS schließen an
sich nur so genannte \emph{perfekte Kollinearität} aus, das heißt die
Situation in der eine erklärende Variable eine perfekte lineare
Transformation einer anderen erklärenden Variable ist. Problematisch
sind aber auch schon geringere, aber immer noch hohe Korrelationen: denn
je stärker die erklärenden Variablen untereinander korrelieren, desto
größer werden die Standardfehler unserer Schätzer. Mit diesem Problem
werden wir uns später in der Veranstaltung noch genauer
auseinandersetzen.

\section{Anwendungsbeispiel}\label{anwendungsbeispiel}

Dieser Abschnitt wir zeitnah ergänzt.

\appendix


\chapter{Eine kurze Einführung in R Markdown}\label{markdown}

Hier gibt es eine kurze Einführung in \texttt{R\ Markdown}. Wir
beschränken uns dabei auf die grundlegende Idee von Markdown, da die
konkrete Syntax im Internet an zahlreichen Stellen wunderbar erläutert
ist und man das konkrete Schreiben am besten in der Anwendung lernt.

\section{Markdown vs.~R-Markdown}\label{markdown-vs.r-markdown}

Bei \texttt{Markdown} handelt es sich um eine sehr einfache
Auszeichnungssprache, d.h. eine Programmiersprache, mit der schön
formatierte Texte erstellt werden können und die gleichzeitig auch für
Menschen sehr einfach lesbar ist. Dateien, die in Markdown geschrieben
sind, sind gewöhnlicherweise an der Endung \texttt{.md} zu erkennen.

R-Markdown stellt man sich am besten als eine Kombination von Markdown
und R vor: R-Markdown Dateien, die immer durch die Dateiendung
\texttt{.Rmd} gekennzeichnet sind, bestehen sowohl aus Markdown-Code,
als auch aus R-Code. Das bedeutet, dass man sein Forschungsprojekt
gleichzeitig erklären und durchführen kann. Im Prinzip können ganze
Forschungspapiere in R-Markdown verfasst werden und damit vollständig
reproduzierbar gestaltet werden.

\section{Installation von R-Markdown}\label{installation-von-r-markdown}

Für den Fall, dass Sie mit R-Studio arbeiten brauchen Sie lediglich das
Paket \texttt{rmarkdown} zu installieren:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{'rmarkdown'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Das Standardformat für R-Markdown Dokumente ist html. Wenn Sie aber auch
PDF Dokumente erstellen wollen, müssen Sie auf Ihrem Computer
\href{https://www.latex-project.org/}{LaTex} installieren. Hierfür
finden sich zahlreiche Anleitungen im Internet (z.B.
\href{https://www.latex-tutorial.com/installation/}{hier} oder
\href{https://www.latex-project.org/get/}{hier}).

\section{Der R-Markdown Workflow}\label{der-r-markdown-workflow}

\subsection{Ein neues R-Markdown Dokument
erstellen}\label{ein-neues-r-markdown-dokument-erstellen}

R-Studio macht es Ihnen sehr leicht R-Markdown Dokumente zu erstellen.
Klicken Sie einfach auf den Button \texttt{Neu} und wählen dort dann
\texttt{R\ Markdown} aus, wie auf folgendem Screenshot zu sehen ist:

\begin{center}\includegraphics[width=0.75\linewidth]{/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR/figures/A-Markdown-NewFile} \end{center}

Im folgenden Fenster können Sie getrost die Standardeinstellungen so wie
vorgeschlagen belassen, da Sie alles später noch sehr leicht ändern
können.

Sie sehen nun eine Datei, das bereits einigen Beispielcode enthält und
damit schon einen Großteil der Syntax illustriert.

Ein R-Markdown Dokument besteht in der Regel aus zwei Teilen: dem
Titelblock und dem darunter folgenden Dokumentenkörper:

\begin{center}\includegraphics[width=0.75\linewidth]{/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR/figures/A-Markdown-Title} \end{center}

\subsection{Der Titelblock}\label{der-titelblock}

Der Titelblock ist immer durch zwei Zeilen mit dem Inhalt
``\texttt{-\/-\/-}'' oben und unten abgegrenzt. Die Syntax des
Titelblocks folgt der Sprache
\href{https://de.wikipedia.org/wiki/YAML}{YAML}, aber das hat wenig
praktische Relevanz. Im Titelblock werden alle globalen Einstellungen
für das Dokument vorgenommen. Für einfache Dokumente muss nur wenig an
den Standardeinstellungen geändert werden, aber im Laufe der Zeit werden
Sie merken, dass Sie über den YAML-Block Ihr Dokument zu ganz großen
Teilen individualisieren können. In der Regel finden Sie alle Antworten
durch Googlen, daher werde ich hier nicht weiter auf den Header
eingehen.

\subsection{Der Textkörper}\label{der-textkorper}

Der Textkörper besteht aus normalem Text, welcher in der Markdown Syntax
geschrieben ist, und so genannten \emph{Chunks}. Für die wirklich
einfache Syntax für normalen Text gibt es zahlreiche gute Anleitungen im
Internet, z.B. dieses eingängige
\href{https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf}{Cheat
Sheet}.

Innerhalb der Chunks können Sie Code in einer beliebigen
Programmiersprache schreiben, insbesondere auch in R. Die Syntax
unterscheidet sich dabei überhaupt nicht von einem normalen R Skript.

Um einen Chunk zu Ihrem Dokument hinzuzufügen klicken Sie oben rechts im
Skripbereich auf `Insert' und wählen \texttt{R} aus:

\begin{center}\includegraphics[width=0.75\linewidth]{/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR/figures/A-Markdown-Chunk1} \end{center}

Daraufhin wird an der Stelle des Cursors ein Chunk in Ihr Dokument
eingefügt. Dieser Chunk wird in der ersten und letzten Zeile durch
\texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}} begrenzt.
In der ersten Zeile wird zusätzlich innerhalb von geschweiften Klammern
die Programmiersprache des Chunks definiert:

\begin{center}\includegraphics[width=0.75\linewidth]{/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR/figures/A-Markdown-Chunk3} \end{center}

Darüber hinaus kann das Ausführverhalten für den Chunk durch weitere
Argumente innerhalb der geschweiften Klammer weiter spezifiziert werden.

Häufig möchten Sie z.B., dass der Code im Chunk zwar im Dokument
angezeigt, aber nicht ausgeführt werden soll. Dies können Sie durch die
Option \texttt{eval=FALSE} erreichen. In diesem Fall sähe Ihr Chunk so
aus:

\begin{center}\includegraphics[width=0.75\linewidth]{/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR/figures/A-Markdown-Chunk4} \end{center}

In diesem Beispiel wird die Zuweisung \texttt{x\ \textless{}-\ 4} bei
der Kompillierung des Dokuments nicht ausgeführt.

Eine gute Übersicht über die Optionen, die Ihnen offen stehen, finden
Sie
\href{https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}{hier}
oder durch Googlen.

Sie können einzelne Chunks auch schon vor dem Kompillieren des Dokuments
ausführen indem Sie auf das Play-Zeichen oben links beim Chunk drücken.
Damit erhalten Sie eine Vorschau auf das Ergebnis.

\subsection{Kompillieren von
Dokumenten}\label{kompillieren-von-dokumenten}

Der Prozess, der aus dem Quellcode ihres Dokuments (also allem was in
der \texttt{.Rmd} Datei geschrieben ist) das fertige Dokument erstellt,
wird \emph{Kompillieren} genannt. Dabei wird aus dem \texttt{.Rmd}
Dokument ein gut lesbares \texttt{.html} oder \texttt{.pdf} Dokument
erstellt, wobei alle Chunks normal ausgeführt werden (es sei denn dies
wird durch die Option \texttt{eval=FALSE} verhindert).

Grundsätzlich gibt es zwei Möglichkeiten ein Dokument zu kompillieren:
über die entsprechende R-Funktion, oder über den Knit-Button in
R-Studio.

Die klassische Variante verwendet die Funktion \texttt{render()} aus dem
Paket \texttt{rmarkdown}. Die wichtigsten Argumente sind dabei die
folgenden: \texttt{input} spezifiziert die zu kompillierende
\texttt{.Rmd}-Datei, \texttt{output\_format} das für den Output
gewünschte Format\footnote{R-Markdown Dateien können in sehr viele
  verschiedene Formate kompilliert werden, das am häufigsten verwendete
  Format ist jedoch \texttt{html}. Eine Übersicht finden Sie
  \href{https://bookdown.org/yihui/rmarkdown/output-formats.html}{hier}.}
und \texttt{output\_file} den Pfad und den Namen der zu erstellenden
Outputdatei.

Wenn Sie also das Dokument \texttt{FirstMarkdown.Rmd} kompillieren
wollen und den Output unter \texttt{Output/OurMarkdown.html} als
html-Datei speichern wollen, dann können Sie das mit folgendem Code,
vorausgesetzt die Datei \texttt{FirstMarkdown.Rmd} liegt im Unterordner
\texttt{R}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{render}\NormalTok{(}\DataTypeTok{input =} \StringTok{"R/FirstMarkdown.Rmd"}\NormalTok{, }
       \DataTypeTok{output_format =} \StringTok{"html"}\NormalTok{, }
       \DataTypeTok{output_file =} \StringTok{"output/FirstMarkdown.html"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Weitere Informationen zu den Parametern finden Sie wie immer über die
\texttt{help()} Funktion. Alternativ können Sie auch den Button
\texttt{Knit} in der R-Studio Oberfläche verwenden. Das ist in der Regel
bequemer, lässt aber weniger Individualisierung zu.

\section{Relative Pfade in
Markdown-Dokumenten}\label{relative-pfade-in-markdown-dokumenten}

Der problematischste Teil beim Arbeiten mit R-Markdown ist der Umgang
mit relativen Pfaden. Um das Problem zu illustrieren nehmen wir einmal
folgende Ordnerstruktur an, wobei der Ordner \texttt{MarkdownProject}
unser Arbeitsverzeichnis ist:

\begin{center}\includegraphics[height=0.25\textheight]{/Users/claudius/work-claudius/general/paper-projects/packages/SocioEconMethodsR/figures/A-Markdown-Ordnerstruktur} \end{center}

Das Problem ist nun, dass wenn Sie eine R-Markdown Datei kompillieren,
diese Datei alle Pfade \textbf{nicht} ausgehend von Ihrem
Arbeitsverzeichnis interpretiert, sondern vom \emph{Speicherort} der
\texttt{.Rmd}-Datei. Das ist natürlich hochproblematisch, denn stellen
Sie sich vor, Sie möchten in Ihrem R-Markdown-Skript die Datei
\texttt{data/BIP-Data.csv} einlesen. Normalerweise würden Sie dafür den
folgenden Code verwenden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bip_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{"data/BIP-Data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Zwar würde der Code in einem R-Skript, z.B. in \texttt{R/R-Skript.R}
perfekt funktionieren. In einem R-Markdown Dokument, das nicht im
Arbeitsverzeichnis direkt gespeichert ist, jedoch nicht. Da in
R-Markdown-Dokumenten alle Pfade relativ des Speicherorts des Dokuments
interpretiert werden, müssten wir hier schreiben:\footnote{Mit
  \texttt{../} bewegt man sich bei einem relativen Pfad einen Ordner
  nach oben.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bip_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{"../data/BIP-Data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Das wäre allerdings unschön, weil wir dann unterschiedlichen Codes in
Skripten und in R-Markdown-Dokumenten verwenden müssten und das Ganze
dadurch deutlich verwirrender werden würde.

Es wäre also schön, wenn R automatisch wüsste, was das
Arbeitsverzeichnis des aktuellen Projekts ist und dieses automatisch
berücksichtigt, unabhängig davon ob wir mit einem \texttt{.R} oder
\texttt{.Rmd} Dokument arbeiten und wo dieses Dokument innerhalb unserer
Projekt-Struktur gespeichert ist.

Zum Glück können wir genau das mit Hilfe des Pakets
\href{https://github.com/jennybc/here_here}{here} erreichen.\footnote{Tatsächlich
  ist \texttt{here} dermaßen praktisch, dass ich empfehle grundsätzlich
  alle Pfade in jedem Projekt - ob R-Markdown oder nicht - mit Hilfe von
  \texttt{here} anzugeben.} Das Paket enthält eine Funktion
\texttt{here()} die als Argument einen Dateinamen oder einen relativen
Pfad akzeptiert, und daraus einen absoluten Pfad auf dem Computer, auf
dem der Code gerade ausgeführt wird, konstruiert.

Wir können also unseren Code von oben einfach folgendermaßen
umschreiben:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bip_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"data/BIP-Data.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

In dieser Form funktioniert er sowohl in \texttt{.R} als auch
\texttt{.Rmd} Dateien ohne Probleme.

\begin{quote}
\textbf{Hinweis I:} Die Funktion \texttt{here()} verwendet verschiedene
Heuristiken um das Arbeitsverzeichnis des aktuellen Projekt
herauszufinden. Darunter fällt auch das Suchen nach einer
\texttt{.Rproj} Datei. Überhaupt funktionieren die Heuristiken in der
Regel wunderbar und können für Ihren konkreten Fall über die Funktion
\texttt{dr\_here()} angezeigt werden. Um ganz sicherzugehen sollte man
aber immer in das Arbeitsverzeichnis eine Datei \texttt{.here} ablegen.
Diese kann manuell, oder über die Funktion \texttt{set\_here()},
erstellt werden.
\end{quote}

\begin{quote}
\textbf{Hinweis II:} Die Verwendung von \texttt{here()} ist essenziell,
wenn Ihre R-Markdown Dokumente auf mehreren Computern funktionieren
sollen. Daher ist die Verwendung in den Arbeitsblättern
\textbf{verpflichtend}.
\end{quote}

\section{Weitere Quellen}\label{weitere-quellen}

Eine gute Übersicht über die häufigsten Befehle enthält dieses
\href{https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf}{Cheat
Sheet}. Eine sehr umfangreiche Einführung bietet das Online-Buch
\href{https://bookdown.org/yihui/rmarkdown/}{R Markdown: The Definitive
Guide}. Aber auch darüber hinaus finden sich im Internet zahlreiche
Beispiele für die R-Markdown-Syntax. Dieses Skript wurde übrigens in
\href{https://bookdown.org/yihui/bookdown/}{R Bookdown}, einer
Erweiterung von R-Markdown für Bücher, geschrieben.

\hypertarget{stat-stoch}{\chapter{Wiederholung:
Wahrscheinlichkeitstheorie}\label{stat-stoch}}

In diesem Kapitel werden Grundlagen der Wahrscheinlichkeitstheorie
wiederholt. Die zentralen Themen sind dabei:

\begin{itemize}
\tightlist
\item
  Der Zusammenhang zwischen Wahrscheinlichkeitstheorie und Statistik
\item
  Grundbegriffe der Wahrscheinlichkeitstheorie und Statistik
\item
  Zufallsvariablen
\item
  Diskrete und stetige Verteilungen
\end{itemize}

Grundkonzepte der deskriptiven und schließenden Statistik (insb.
Parameterschätzung, Hypothesentests und die Berechnung von
Konfidenzintervallen) werden in den beiden Anhängen
\protect\hyperlink{desk-stat}{zur deskriptiven} und
\protect\hyperlink{stat-rep}{schließenden Statistik} wiederholt.

Für den Code in diesem Kapitel werden die folgenden Pakete verwendet:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(ggpubr)}
\KeywordTok{library}\NormalTok{(latex2exp)}
\KeywordTok{library}\NormalTok{(icaeDesign)}
\KeywordTok{library}\NormalTok{(data.table)}
\end{Highlighting}
\end{Shaded}

\section{Einleitung: Wahrscheinlichkeitstheorie und
Statistik}\label{einleitung-wahrscheinlichkeitstheorie-und-statistik}

Statistik und Wahrscheinlichkeitstheorie sind untrennbar miteinander
verbunden. In der Wahrscheinlichkeitstheorie beschäftigt man sich mit
Modellen von Zufallsprozessen, also Prozessen, deren Ausgang nicht exakt
vorhersehbar ist. Häufig spricht man von \emph{Zufallsexperimenten}.

Die Wahrscheinlichkeitstheorie entwickelt dabei Modelle, welche diese
Zufallsexperimenten und deren mögliche Ausgänge beschreiben und dabei
den möglichen Ausgängen Wahrscheinlichkeiten zuordnern. Diese Modelle
werden \emph{Wahrscheinlichkeitsmodelle} genannt.

In der Statistik versuchen wir anhand von beobachteten Daten
herauszufinden, welches Wahrscheinlichkeitsmodell gut geeignet ist, den
die Daten generierenden Prozess (\emph{data generating process} - DGP)
zu beschreiben. Das ist der Grund warum man für Statistik auch immer
Kenntnisse der Wahrscheinlichkeitstheorie braucht.

\begin{quote}
Kurz gesagt: in der Wahrscheinlichkeitstheorie wollen wir mit Hilfe von
Wahrscheinlichkeitsmodellen Daten vorhersagen, in der Statistik mit
Hilfe bekannter Daten Rückschlüsse auf die zugrundeliegenden
Wahrscheinlichkeitsmodelle ziehen.
\end{quote}

\section{Grundbegriffe der
Wahrscheinlichkeitstheorie}\label{grundbegriffe-der-wahrscheinlichkeitstheorie}

Ein wahrscheinlichkeitstheoretisches Modell besteht \emph{immer} aus den
folgenden drei Komponenten:

\textbf{Ergebnisraum}: diese Menge \(\Omega\) enthält alle möglichen
Ergebnisse des modellierten Zufallsexperiments. Das einzelne Ergebnis
bezeichnen wir mit \(\omega\).

\begin{quote}
\textbf{Beispiel:} Handelt es sich bei dem Zufallsexperiment um das
Werfen eines normalen sechseitigen Würfels gilt
\(\Omega=\{1,2,3,4,5,6\}\). Wenn der Würfen gefallen ist, bezeichnen wir
die oben liegende Zahl als das Ergebnis \(\omega\) des Würfelwurfs,
wobei hier gilt \(\omega_1=\) ``Der Würfel zeigt 1'', u.s.w.
\end{quote}

\textbf{Ereignisse:} unter Ereignissen \(A, B, C,...\) verstehen wir die
Teilmengen des Ergebnisraums. Ein Ereignis enthält ein oder mehrere
Elemente des Ergebnisraums. Enthält ein Ereignis genau ein Element,
sprechen wir von einem \emph{Elementarereignis}.

\begin{quote}
\textbf{Beispiel:} ``Es wird eine gerade Zahl gewürfelt'' ist ein
mögliches Ereignis im oben beschriebenen Zufallsexperiment. Das Ereignis
- nennen wir es hier \(A\) - tritt ein, wenn ein Würfelwurf mit dem
Ergebnis ``2'', ``4'' oder ``6'' endet. Also:
\(A=\{\omega_2, \omega_4, \omega_6\}\) Das Ereignis \(B\) ``Es wird eine
2 gewürfelt'' tritt nur ein, wenn das Ergebnis des Würfelwurfs eine 2
ist: \(B=\{\omega_2\}\). Entsprechend nennen wir es ein
\emph{Elementarereignis}.
\end{quote}

Da es sich bei Ereignissen um Mengen handelt können wir die typischen
mengentheoretischen Konzepte wie `Vereinigung', `Differenz' oder
`Komplement' zu ihrer Beschreibung verwenden:

\begin{longtable}[]{@{}lll@{}}
\toprule
Konzept & Symbol & Übersetzung\tabularnewline
\midrule
\endhead
Schnittmenge & \(A\cap B\) & \(A\) und \(B\)\tabularnewline
Vereinigung & \(A\cup B\) & \(A\) und/oder \(B\)\tabularnewline
Komplement & \(A^c\) & Nicht \(A\)\tabularnewline
Differenz & \(A \setminus B = A\cap B^c\) & \(A\) ohne
\(B\)\tabularnewline
\bottomrule
\end{longtable}

\textbf{Wahrscheinlichkeiten}: jedem \emph{Ereignis} \(A\) wird eine
Wahrscheinlichkeit \(\mathbb{P}(A)\) zugeordnet. Wahrscheinlichkeiten
können aber nicht beliebige Zahlen sein. Vielmehr müssen sie im Einklang
mit den drei \emph{Axiomen von Kolmogorow} stehen:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Für jedes Ereignis \(A\) gilt: \(0\leq\mathbb{P}(A)\leq1\)
\item
  Das sichere Ereignis \(\Omega\) umfasst den ganzen Ergebnisraum und es
  gilt entsprechend \(\mathbb{P}(\Omega)=1\).
\item
  Es gilt: \(\mathbb{P}(A\cup B) = \mathbb{P}(A)+\mathbb{P}(B)\) falls
  \(A\cap B=\emptyset\), also wenn sich A und B gegenseitig
  ausschließen.
\end{enumerate}

Aus diesen Axiomen lassen sich eine ganze Menge Sätze heraus ableiten,
auf die wir im folgenden aber nicht besonders eingehen wollen. Die
Grundidee ist aber, bestimmten Ereignissen von Anfang an bestimmte
Wahrscheinlichkeiten zuzuordnen, und die Wahrscheinlichkeiten für andere
Ereignisse dann aus den eben beschriebenen Regeln abzuleiten.

Je nach Art des Ergebnisraums \(\Omega\) unterscheiden wir zwei
grundsätzlich verschiedene Arten von Wahrscheinlichkeitsmodellen: ist
\(\Omega\) \textbf{abzählbar} handelt es sich um ein \textbf{diskretes
Wahrscheinlichkeitsmodell}. Der Würfelwurf oder ein Münzwurf sind
hierfür Beispiele: die Menge der möglichen Ergebnisse ist hier klar
abzählbar.\footnote{Wir nennen eine Menge abzählbar wenn sie mit Hilfe
  der ganzen Zahlen \(\mathbb{N}\) indiziert werden kann. Das bedeutet,
  dass auch unendlich große Mengen als abzählbar gelten können.}

Ist \(\Omega\) \textbf{nicht abzählbar} handelt es sich dagegen um ein
\textbf{stetiges Wahrscheinlichkeitsmodell}. Ein Beispiel hierfür wäre
das Fallenlassen von Steinen und die Messung der Falldauer. Die
einzelnen Ereignisse wären dann die Falldauer und es würde gelten, dass
\(\Omega=\mathbb{R^+}\) und \(\mathbb{R^+}\) ist nicht abzählbar.

Welches Modell für den konkreten Anwendungsfall vorzuziehen ist, muss
auf Basis von theoretischen Überlegungen entschieden werden.

\section{Diskrete
Wahrscheinlichkeitsmodelle}\label{diskrete-wahrscheinlichkeitsmodelle}

Wenn wir die Wahrscheinlichkeit für das Eintreten eines Ereignisses
\(A\) erfahren möchten können wir im Falle eines diskreten Ergebnisraums
einfach die Eintrittswahrscheinlichkeiten für alle Ergebnisse, die zu
\(A\) gehören, aufsummieren:

\[ \mathbb{P}(A)=\sum_{\omega\in A} \mathbb{P}(\{\omega\})\]

\begin{quote}
\textbf{Beispiel:} Beim Werfen eines sechseitigen Würfels ist die
Wahrscheinlichkeit für das Ereignst ``Es wird eine gerade Zahl
gewürfelt'':
\(\mathbb{P}(2)+\mathbb{P}(4)+\mathbb{P}(6)=\frac{1}{6}+\frac{1}{6}+\frac{1}{6}=\frac{1}{2}\).
\end{quote}

Von Interesse ist häufig aus den Wahrscheinlichkeiten für zwei
Ereignisse, \(A\) und \(B\), die Wahrscheinlichkeit für \(A\cap B\),
also die Wahrscheinlichkeit, dass beide Ereignisse auftreten, zu
berechnen. Leider ist das nur im Spezialfall der \textbf{stochastischen
Unabhängigkeit} möglich. Stochastische Unabhängigkeit kann immer dann
sinnvollerweise angenommen werden, wenn zwischen den beteiligten
Ereignissen kein kausaler Zusammenhang besteht. In diesem Fall gilt
dann:

\[\mathbb{P}(A\cap B) = \mathbb{P}(A)\cdot\mathbb{P}(B)\]

\begin{quote}
\textbf{Beispiel für stochastische Unabhängigkeit}: Es ist plausibel
anzunehmen, dass es keinen kausalen Zusammenhang zwischen zwei
aufeinanderfolgenden Münzwürfen gibt. Entsprechend sind die Ereignisse
\(A\): ``Zahl im ersten Wurf'' und \(B\): ``Kopf im zweiten Wurf''
stochastisch unabhängig und
\(\mathbb{P}(A\cap B)=\mathbb{P}(A)\cdot \mathbb{P}(B)=\frac{1}{4}\).
\end{quote}

\begin{quote}
\textbf{Beispiel für stochastische Abhängigkeit}: Ein anderer Fall liegt
vor, wenn wir die Ereignisse \(C\): ``Die Summe beider Würfe ist 6'' und
\(D\): ``Der erste Wurf zeigt eine 2.'' betrachten. Hier ist
offensichtlich, dass ein kausaler Zusammenhang zwischen den beiden
Würfen und den Ereignissen besteht. Es gilt:
\(\mathbb{P}(C\cap D)=\mathbb{P}(\{2, 4\})=\frac{1}{36}\). Würden wir
die Wahrscheinlichkeiten einfach multiplizieren erhielten wir allerdings
\(\mathbb{P}(C)\cdot \mathbb{P}(D)=\frac{5}{36}\cdot\frac{1}{6}=\frac{5}{216}\),
wobei \(\mathbb{P}(C)=\frac{5}{36}\).
\end{quote}

Ein weiteres wichtiges Konzept ist das der \textbf{bedingten
Wahrscheinlichkeit}: die bedingten Wahrscheinlichkeit von \(A\) gegeben
\(B\), \(\mathbb{P}(A|B)\), bezeichnet die Wahrscheindlichkeit für
\(A\), wenn wir wissen, dass \(B\) bereits eingetreten ist.

Es gilt dabei:\footnote{An der Formel wird noch einmal deutlich, dass
  wenn \(A\) und \(B\) stochastisch unabhängig sind wir nichts von \(B\)
  über \(A\) und umgekehrt lernen können, also gilt:
  \(\mathbb{P}(A|B)=\mathbb{P}(A)\) und
  \(\mathbb{P}(B|A)=\mathbb{P}(B)\).}

\[\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\]

\begin{quote}
\textbf{Beispiel:} Sei \(A\): ``Der Würfel zeigt eine 6'' und \(B\):
``Der Würfelwurf zeigt eine gerade Zahl''. Wenn wir bereits wissen, dass
\(B\) eingetreten ist, ist \(\mathbb{P}(A)\) nicht mehr \(\frac{1}{6}\),
weil wir ja wissen, dass 1, 3 und 5 nicht auftreten können. Vielmehr
gilt \(\mathbb{P}(A|B)=\frac{1/6}{1/2}=\frac{1}{3}\).
\end{quote}

\subsection{Bayes Theorem und Gesetz der total
Wahrscheinlichkeiten}\label{bayes-theorem-und-gesetz-der-total-wahrscheinlichkeiten}

Ganz wichtig: es gilt \emph{nicht notwendigerweise}
\(\mathbb{P}(A|B)=\mathbb{P}(B|A)\). Vielmehr gilt nach dem \textbf{Satz
von Bayes}:

\[\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}=\frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}\]

Ein in Beweisen sehr häufig verwendeter Zusammenhang ist das
\textbf{Gesetz der totalen Wahrscheinlichkeit}: seien \(A_1,...,A_k\)
Ergeignisse, die sich nicht überschneiden und gemeinsam den kompletten
Ereignisraum \(\Omega\) abdecken, dann gilt:

\[\mathbb{P}(B)=\sum_{i=1}^k\mathbb{P}(B|A_k)\mathbb{P}(A_k)\]

Auch wenn das erst einmal sperrig aussieht, ist der Zusammenhang sehr
praktisch und wird häufig in Beweisen in der Stochastik verwendet.

\subsection{Diskrete Zufallsvariablen}\label{diskrete-zufallsvariablen}

Bei Zufallsvariablen (ZV) handelt es sich um besondere
\emph{Funktionen}. Die Definitionsmenge einer Zufallsvariable ist immer
der zurgundeliegende Ergebnisraum \(\Omega\), die Zielmenge ist i.d.R.
\(\mathbb{R}\), sodass gilt:

\[X:\Omega\rightarrow\mathbb{R}, \omega \mapsto X(\omega)\]

Im Kontext von ZV sprechen wir häufig nicht von dem zugrundeliegenden
Ergebnisraum \(\Omega\), sondern - inhaltlich äquivalent - vom
\emph{Wertebereich von X}, bezeichnet als \(W_X\).

In der Regel bezeichnen wir Zufallsvariablen (ZV) mit Großbuchstaben und
die konkrete Realisation einer ZV mit einem Kleinbuchstaben, sodass
\(\mathbb{P}(X=x)\) die Wahrscheinlichkeit angibt, dass die ZV \(X\) den
konkreten Wert \(x\) annimmt. Bei \(x\) sprechen wir von einer
\emph{Realisierung} der ZV \(X\). Wir nehmen für die weitere Notation
an, dass \(W_X=\{x_1, x_2,...,x_K\}\) und bezeichnen das einzelne
Element mit \(x_k\) mit \(1\leq k\leq K\).

Dies bedeutet streng genommen, dass die ZV selbst nicht als zufällig
definiert wird. Zufällig ist nur der Input \(\omega\) der entsprechenden
Funktion \(X: \Omega\rightarrow X(\omega)\), also z.B. ein Würfelwurf.
Der funktionale Zusammenhang zwischen Funktionswert \(X(\omega)\) und
dem Input \(\omega\) ist hingegen eindeutig.

Das bedeutet streng genommen, dass die ZV nicht \emph{selbst} zufällig
ist, sondern ihr Input \(\omega\). Das impliziert, dass wenn ein
Zufallsexperiment zweimal das gleiche Ergebnis \(\omega\) hat, ist auch
der Wert \(X(\omega)\) der gleiche.

Das mag im Moment ein wenig nach `Pfennigfuchserei' aussehen, die
Unterscheidung zwischen dem nicht-zufälligem funtionalen Zusammenhangs,
aber einem zufälligen Input bei ZV ist wichtig, um den Sinn in vielen
fortgeschrittenen Beiträgen im Bereich der Ökonometrie zu sehen.

Den unterschiedlichen Realisierungen von einer ZV haben jeweils
Wahrscheinlichkeiten, die von den Wahrscheinlichkeiten der
zugrundeliegenden Ergebnisse des modellierten Zufallsexperiments
abhängen.

Produkte und Summen von ZV sind selbst wieder Zufallsvariables. Man
addiert bzw. multipliziert ZV indem man ihre Werte addiert bzw.
mutlipliziert.

Im Falle von diskreten ZV können wir eine Liste erstellen, die für alle
möglichen Werte \(x_k\in W_X\) die jeweilige Wahrscheinlichkeit
\(\mathbb{P}(X=x_k)\) angibt.\footnote{Aus den \emph{Kolmogorow Axiomen}
  oben ergibt sich, dass die Summe all dieser Wahrscheinlichkeiten 1
  ergeben muss: \(\sum_{k\geq 1}\mathbb{P}(X=x_k)=1\).} Diese Liste
nennen wir \textbf{Wahrscheinlichkeitsverteilung} (\emph{Probability
Mass Function}, PMF) von \(X\) und sie werden häufig visuell
dargestellen. Um diese Liste zu erstellen verwenden wir die zu \(X\)
gehörende \textbf{Wahrscheinlichkeitsfunktion}, (\(p(x_k)\)),die uns für
jedes Ergebnis die zugehörige Wahrscheinlichkeit gibt:\footnote{Zu jeder
  Wahrscheinlichkeitsverteilung gibt es eine eindeutige
  Wahrscheinlichkeitsfunktion und jede Wahrscheinlichkeitsfunktion
  definiert umgekehrt eine eindeutig bestimmte diskrete
  Wahrscheinlichkeitsverteilung.}

\[p(x_k)=\mathbb{P}(X=x_k)\]

Wenn wir eine ZV analysieren tun wir dies in der Regel durch eine
Analyse ihrer Wahrscheinlichkeitsverteilung. Zur genaueren Beschreibung
einer ZV wird entsprechend häufig einfach die
Wahrscheinlichkeitsfunktion angegeben.

Im folgenden wollen wir einige häufig auftretende
Wahrscheinlichkeitsverteilungen kurz besprechen. Am Ende des Abschnitts
findet sich dann ein tabellarischer Überblick. Doch vorher wollen wir
uns noch mit den wichtigsten \textbf{Kennzahlen einer Verteilung}
vertraut machen. Denn wie Sie sich vorstellen können sind
Wahrscheinlichkeitsverteilungen als Listen, die alle möglichen
Realisierungen einer ZV enthalten ziemlich umständlich zu handhaben.
Daher beschreiben wir Wahrscheinlichkeitsverteilungen nicht indem wir
eine Liste beschreiben, sondern indem wir bestimmte Kennzahlen zu ihrer
Beschreibung verwenden. Die wichtigsten Kennzahlen einer ZV \(X\) sind
der \textbf{Erwartungswert} \(\mathbb{E}(x)\) als \emph{Lageparameter}
und die \textbf{Standardabweichung} \(\sigma(X)\) als
\emph{Streuungsmaß}.

Der Erwartungswert ist definitert als die nach ihrer Wahrscheinlichkeit
gewichtete Summe aller Elemente im Wertebereich von \(X\) und gibt damit
die mittlere Lage der Wahrscheinlichkeitsverteilung an. Wenn \(W_X\) der
Wertebereich von \(X\) ist, dann gilt:

\[\mathbb{E}(x)=\mu_X=\sum_{x_k\in W_X}p(x_k)x_k\]

\begin{quote}
Beispiel: Der Erwartungswert einer ZV \(X\), die das Werfen eines fairen
Würfels beschreibt ist:
\(\mathbb{E}(X)=\sum_{k=1}^6k\cdot\frac{1}{6}=3.5\).
\end{quote}

Wie wir \protect\hyperlink{stat-re}{später} sehen werden, wird der
Erwartungswert in der empirischen Praxis häufig über den Mittelwert
einer Stichprobe identifiziert.

Ein gängiges Maß für die Streuung einer Verteilung \(X\) ist die Varianz
\(Var(X)\) oder ihre Quadratwurzel, die Standardabweichung,
\(\sigma(X)=\sqrt{Var(X)}\). Letztere wird häufiger verwendet, weil sie
die gleiche Einheit hat wie \(X\):

\[Var(X)=\sum_{x_k\in W_X}\left[x_k-\mathbb{E}(X)\right]^2 p(x_k)\]

\begin{quote}
Beispiel: Die Standardabweichung einer ZV \(X\), die das Werfen eines
fairen Würfels beschreibt ist:
\(\sigma_X=\sqrt{\sum_{k}^6\left[x_k-\mathbb{E}(X)\right]^2 p(x_k)}=\sqrt{5.83}\approx 2.414\).
\end{quote}

Im folgenden wollen wir uns einige der am häufigsten verwendeten ZV und
ihre Verteilungen genauer ansehen. Am Ende der Beschreibung jeder
Funktion folgt ein Beispiel für eine Anwendung. Wenn Ihnen die
theoretischen Ausführungen am Anfang etwas kryptisch erscheinen,
empfiehlt es sich vielleicht erst einmal das Anwendungsbeispiel
anzusehen.

\subsection{Beispiel: die
Binomial-Verteilung}\label{beispiel-die-binomial-verteilung}

Die vielleicht bekannteste diskrete Wahrscheinlichkeitsverteilung ist
die Binomialverteilung \(\mathcal{B}(n,p)\). Mit ihr modelliert man
Zufallsexperimente, die aus einer Reihe von Aktionen bestehen, die
entweder zum `Erfolg' oder `Misserfolg' führen.

Die Binomialverteilung ist eine Verteilung mit zwei \textbf{Parametern}.
Parameter sind Werte, welche die Struktur der Verteilung bestimmen. In
der Statistik sind wir häufig daran interessiert, die Paramter einer
Verteilung zu bestimmen. Im Falle der Binomialverteilung gibt es die
folgenden zwei Parameter: \(p\) gibt die Erfolgswahrscheinlichkeit einer
einzelnen Aktion an (und es muss daher gelten \(p\in[0,1]\)) und \(n\)
gibt die Anzahl der Aktionen an. Daher auch die Kurzschreibweise
\(\mathcal{B}(n,p)\).

\begin{quote}
\textbf{Beispiel:} Wenn wir eine faire Münze zehn Mal werfen, können wir
das mit einer Binomialverteilung mit \(p=0.5\) und \(n=10\) modellieren.
\end{quote}

Die \emph{Wahrscheinlichkeitsfunktion} \(p(x)\) der Binomialverteilung
ist die folgende, wobei \(x\) die Anzahl der Erfolge darstellt:

\[\mathbb{P}(X=x)=p(x)=\binom{n}{x}p^x(1-p)^{n-x}\] Dies ergibt sich aus
den grundlegenden Wahrscheinlichkeitsgesetzen: \(\binom{n}{x}\) ist der
\href{https://de.wikipedia.org/wiki/Binomialkoeffizient}{Binomialkoeffizient}
und gibt uns die Anzahl der Möglichkeiten wie man bei \(n\) Versuchen
\(x\) Erfolge erziehlen kann. Dies multiplizieren wir mit der
Wahrscheinlichkeit \(x\)-mal einen Erfolg zu erziehlen und \(n-x\)-mal
einen Misserfolg zu erziehlen.

Wenn die ZV \(X\) einer Binomialverteilung mit bestimmten Parametern
\(p\) und \(n\) folgt, dann schreiben wir \(P \propto \mathcal{B}(n,p)\)
und es gilt, dass \(\mathbb{E}(X)=np\) und
\(\sigma(X)=\sqrt{np(1-p)}\).\footnote{Die Herleitung finden Sie im
  Statistikbuch Ihres Vertrauens oder auf
  \href{https://de.wikipedia.org/wiki/Binomialverteilung\#Erwartungswert}{Wikipedia}.}

Im folgenden sehen wir eine Darstellung der
Wahrscheinlichkeitsverteilung der Binomialverteilung für verschiedene
Parameterwerte:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-2-1} \end{center}

R stellt uns einige nützliche Funktionen bereit, mit denen wir typische
Rechenaufgaben einfach lösen können:

Möchten wir die Wahrscheinlichkeit berechnen, genau \(x\) Erfolge zu
beobachten, also \(\mathbb{P}(X=x)\) geht das mit der Funktion
\texttt{dbinom()}. Die notwendigen Argumente sind \texttt{x} für den
interessierenden x-Wert, \texttt{size} für den Parameter \(n\) und
\texttt{prob} für den Parameter \(p\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =} \DecValTok{10}\NormalTok{, }\DataTypeTok{size =} \DecValTok{50}\NormalTok{, }\DataTypeTok{prob =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.09851841
\end{verbatim}

Das bedeutet, wenn \(X \propto B(50, 0.25)\), dann:
\(\mathbb{P}(X=10)=0.09852\). Die folgende Abbildung illustriert dies:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-4-1} \end{center}

Natürlich können wir an die Funktion auch einen atomaren Vektor als
erstes Argument übergeben:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dbinom}\NormalTok{(}\DataTypeTok{x =} \DecValTok{5}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DataTypeTok{size =} \DecValTok{50}\NormalTok{, }\DataTypeTok{prob =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.004937859 0.012344647 0.025864974 0.046341412 0.072086641 0.098518410
\end{verbatim}

Häufig sind wir auch an der \textbf{kumulierten
Wahrscheinlichkeitsfunktion} interessiert. Während uns die
Wahrscheinlichkeitsfunktion die Wahrscheinlichkeit für genau \(x\)
Erfolge angibt, also \(\mathbb{P}(X=x)\), gibt uns die \emph{kumulierte}
Wahrscheinlichkeitsfunktion die Wahrscheinlichkeit für \(x\) oder
weniger Erfolge, also \(\mathbb{P}(X\leq x)\).

Die entsprechenden Werte für die kumulierten Wahrscheinlichkeitsfunktion
erhalten wir mit der Funktion \texttt{pbinom()}, welche quasi die
gleichen Argumente benötigt wie \texttt{dbinom()}. Nur gibt es anstatt
des Parameters \texttt{x} jetzt einen Parameter \texttt{q}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pbinom}\NormalTok{(}\DataTypeTok{q =} \DecValTok{10}\NormalTok{, }\DataTypeTok{size =} \DecValTok{50}\NormalTok{, }\DataTypeTok{prob =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2622023
\end{verbatim}

Die Wahrscheinlichkeit 5 oder weniger Erfolge bei 5 Versuchen und einer
Erfolgswahrscheinlichkeit von 25\% zu erzielen beträgt also 25.2\%:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-7-1} \end{center}

Schlussendlich haben wir die Funktion \texttt{qbinom()}, welche als
ersten Input eine Wahrscheinlichkeit \texttt{p} akzeptiert und dann den
kleinsten Wert \(x\) findet, für den gilt, dass
\(\mathbb{P}(X=x)\geq p\).

Wenn wir also wissen möchten wie viele Erfolge mit einer
Wahrscheinlichkeit von 50\% mindestens zu erwarten sind, dann schreiben
wir:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qbinom}\NormalTok{(}\DataTypeTok{p =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{50}\NormalTok{, }\DataTypeTok{prob =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12
\end{verbatim}

Es gilt also: \(\mathbb{P}(X=12)\geq p\).

Wir können dies grafisch verdeutlichen:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-9-1} \end{center}

Möchten wir schließlich eine bestimmte Menge an \textbf{Realisierungen}
aus einer Binomialverteilung ziehen geht das mit \texttt{rbinom()},
welches drei Argumente verlangt: \texttt{n} für die Anzahl der zu
ziehenden Realisierungen, sowie \texttt{size} und \texttt{prob} als da
Paramter \(n\) und \(p\) der Binomialverteilung:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample_binom <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(}\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{prob =} \FloatTok{0.4}\NormalTok{)}
\NormalTok{sample_binom}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 6 6 5 3
\end{verbatim}

\begin{quote}
\textbf{Anwendungsbeispiel Binomialverteilung:} Unser Zufallsexperiment
besteht aus dem zehnmaligen Werfen einer fairen Münze. Unter `Erfolg'
verstehen wir das Werfen von `Zahl'. Nehmen wir an, wir führen das
Zufallsexperiment 100 Mal durch, werfen also insgesamt 10 Mal die Münze
und schreiben jeweils auf, wie häufig wir dabei einen Erfolg verbuchen
konnten. Wenn wir unsere Ergebnisse aufmalen, indem wir auf der x-Achse
die Anzahl der Erfolge, und auf der y-Achse die Anzahl der Experimente
mit genau dieser Anzahl an Erfolgen aufmalen erhalten wir ein Histogram,
das ungefähr so aussieht:
\end{quote}

\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-11-1.pdf}
\textgreater{} Aus der Logik der Konstruktion des Zufallsexperiments und
der Inspektion unserer Daten können wir schließen, dass die
Binomialverteilung eine sinnvolle Beschreibung des Zufallsexperiments
und der daraus entstandenen Stichprobe von 100 Münzwurfergebnissen ist.
Da wir eine faire M+nze geworfen haben macht es Sinn für die
Binomialverteilung \(p=0.5\) anzunehmen, und da wir in jedem einzelnen
Experiment die Münze 10 Mal geworfen haben für \(n=10\). Wenn wir die
mit \(=10\) und \(p=0.5\) parametrisierte theoretische
Binomialverteilung nehmen und ihre theoretische Verteilungsfunktion über
die Aufzeichnungen unserer Ergebnisse legen, können wir uns in dieser
Vermutung bestärkt führen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{munzwurfe), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x)) }\OperatorTok{+}
\StringTok{  }\CommentTok{#geom_histogram(bins = wurzanzahl) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data=}\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{table}\NormalTok{(munzwurfe)), }
             \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{munzwurfe, }\DataTypeTok{y=}\NormalTok{Freq)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(munzwurfe), }\DecValTok{1}\NormalTok{), }
                               \DataTypeTok{y=}\KeywordTok{dbinom}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(munzwurfe), }\DecValTok{1}\NormalTok{), }\DataTypeTok{prob =}\NormalTok{ p_zahl, }
                                        \DataTypeTok{size =}\NormalTok{ wurfe_pro_experiment)}\OperatorTok{*}\NormalTok{wurzanzahl), }
             \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y)}
\NormalTok{             ) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(munzwurfe), }\DecValTok{1}\NormalTok{), }
                               \DataTypeTok{y=}\KeywordTok{dbinom}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{max}\NormalTok{(munzwurfe), }\DecValTok{1}\NormalTok{), }\DataTypeTok{prob =}\NormalTok{ p_zahl, }
                                        \DataTypeTok{size =}\NormalTok{ wurfe_pro_experiment)}\OperatorTok{*}\NormalTok{wurzanzahl), }
             \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{color=}\StringTok{"Theoretische Verteilung"}\NormalTok{), }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{1}
\NormalTok{             ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{expand =} \KeywordTok{expand_scale}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))) }\OperatorTok{+}
\StringTok{  }\CommentTok{#scale_color_manual(values=c("blue", "red"), name=c("Theoretische Verteilung", "Empirische Verteilung")) +}
\StringTok{  }\KeywordTok{theme_icae}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-12-1.pdf}

\subsection{Beispiel: die
Poisson-Verteilung}\label{beispiel-die-poisson-verteilung}

Bei der Poisson-Verteilung handelt es sich um die Standardverteilung für
unbeschränkte Zähldaten, also diskrete Daten, die kein natürliches
Maximum haben.

Bei der Poisson-Verteilung handelt es sich um eine
\textbf{ein-parametrische} Funktion, deren einziger Parameter
\(\lambda>0\) ist. \(\lambda\) wird häufig als die mittlere
Ereignishäufigkeit interpretiert und ist \textbf{zugleich Erwartungswert
als auch Varianz} der Verteilung:
\(\mathbb{E}(P_\lambda)=Var(P_\lambda)=\lambda\).

Ihre Definitionsmenge ist \(\mathbb{N}\), also alle natürlichen Zahlen -
daher ist sie im Gegensatz zur Binomialverteilung geeignet, wenn die
Definitionsmenge der Verteilung keine natürliche Grenze hat.

Die \textbf{Wahrscheinlichkeitsfunktion} der Poisson-Verteilung hat die
folgende Form:

\[P_\lambda(x)=\frac{\lambda^x}{x!}e^{-\lambda}\] Die folgende Abbildung
zeigt wie sich die Wahrscheinlichkeitsfunktion für unterschiedliche
Werte von \(\lambda\) manifestiert:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-13-1} \end{center}

Wir können die Verteilung mit sehr ähnlichen Funktionen wie bei der
Binomialverteilung analysieren. Nur die Parameter müssen entsprechend
angepasst werden, da es bei der Poisson-Verteilung jetzt nur noch einen
Paramter (\texttt{lambda}) gibt.

Möchten wir die Wahrscheinlichkeit bereichnen, genau \(x\) Erfolge zu
beobachten, also \(\mathbb{P}(X=x)\) geht das mit der Funktion
\texttt{dpois()}. Das einzige notwendige Argument ist \texttt{lambda}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dpois}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{lambda =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1562935
\end{verbatim}

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-15-1} \end{center}

Informationen über die CDF erhalten wir über die Funktion
\texttt{ppois()}, die zwei Argumente, \texttt{q} und \texttt{lambda},
annimmt.

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-16-1} \end{center}

Mit der Funktion \texttt{qpois()} finden wir für eine Wahrscheinlichkeit
\texttt{p} den kleinsten Wert \(x\), für den gilt, dass
\(\mathbb{P}(X=x)\geq p\).

Wenn wir also wissen möchten wie viele Erfolge mit einer
Wahrscheinlichkeit von 50\% mindestens zu erwarten sind, dann schreiben
wir:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qpois}\NormalTok{(}\DataTypeTok{p =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{lambda =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

Es gilt also: \(\mathbb{P}(X=4)\geq 0.5\).

Wir können dies grafisch verdeutlichen:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-18-1} \end{center}

Möchten wir schließlich eine bestimmte Menge an \textbf{Realisierungen}
der ZV aus einer Poisson-Verteilung ziehen geht das mit
\texttt{rpois()}, welches zwei notwendige Argumente annimmt: \texttt{n}
für die Anzahl der Realisierungen und \texttt{lambda} für den Parameter
\(\lambda\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pois_sample <-}\StringTok{ }\KeywordTok{rpois}\NormalTok{(}\DataTypeTok{n =} \DecValTok{5}\NormalTok{, }\DataTypeTok{lambda =} \DecValTok{4}\NormalTok{)}
\NormalTok{pois_sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3 8 4 4 3
\end{verbatim}

\subsection{Hinweise zu diskreten
Wahrscheinlichkeitsverteilungen}\label{hinweise-zu-diskreten-wahrscheinlichkeitsverteilungen}

Wie Sie vielleicht bereits bemerkt haben sind die R Befehle für
verschiedene Verteilungen alle gleich aufgebaut. Wenn \texttt{*} für die
Abkürzung einer bestimmten Verteilung steht, können wir mit der Funktion
\texttt{d*()} die Werte der Wahrscheinlichkeitsverteilung, mit
\texttt{p*()} die Werte der kumulierten Wahrscheinlichkeitsverteilung
und mit \texttt{q*()} die der Quantilsfunktion berechnen Mit
\texttt{r*()} werden Realisierungen von Zufallszahlen realisiert. Für
das Beispiel der Binomialverteilung, welcher die Abkürzung
\texttt{binom} zugewiesen wurde, heißen die Funktionen entsprechend
\texttt{dbinom()}, \texttt{pbinom()}, \texttt{qbinom()} und
\texttt{rbinom()}.

Die folgende Tabelle gibt einen Überblick über gängige Abkürzungen und
die Parameter der oben besprochenen diskreten Verteilungen.

\begin{longtable}[]{@{}lll@{}}
\toprule
Verteilung & Abkürzung & Parameter\tabularnewline
\midrule
\endhead
Binomialverteilung & \texttt{binom} & \texttt{size},
\texttt{prob}\tabularnewline
Poisson-Verteilung & \texttt{pois} & \texttt{lambda}\tabularnewline
\bottomrule
\end{longtable}

\section{Stetige
Wahrscheinlichkeitsmodelle}\label{stetige-wahrscheinlichkeitsmodelle}

\subsection{Stetige ZV}\label{stetige-zv}

In vorangegangen Abschnitt haben wir uns mit diskreten
Wahrscheinlichkeitsmodellen beschäftigt. Die diesen Modellen
zugrundeliegenden ZV hatten einen abzählbaren Wertebereich. Häufig
interessieren wir uns aber für ZV mit einem nicht abzählbaren
Wertebereich, z.B. \(\mathbb{R}\) oder \([0,1]\).\footnote{Die
  Intervallschreibweise \([0,1]\) ist potenziell verwirrent. Es gilt:
  \([a,b]=\{x\in\mathbb{R} | a\leq x \leq b\}\) (geschlossenes
  Intervall), \((a,b)=\{x\in\mathbb{R} | a < x < b\}\) (offenes
  Intervall), \((a,b)=\{x\in\mathbb{R} | a < x \leq b\}\)(linksoffenes
  Intervall) und
  \((a,b)=\{x\in\mathbb{R} | a \leq x < b\}\)(rechtsoffenes Intervall).}

Bei stetigen Wahrscheinlichkeitsmodellen liegen zwischen zwei Punkten
unendlich viele Punkte. Das hat bedeutende Implikationen für die Angabe
von Wahrscheinlichkeiten. Im Gegensatz zu diskreten
Wahrscheinlichkeitsmodellen hat demnach jeder einzelne Punkt im
Wertebereich der ZV die Wahrscheinlichkeit 0:

\[\mathbb{P}(X=x_k)=0 \quad \forall x_k \in W_X\] wobei \(W_X\) für den
Wertebereich von ZV \(X\) steht

Als Lösung werden Wahrscheinlichkeiten bei stetigen ZV nicht als
Punktwahrscheinlichkeiten, sondern als Intervallwahrscheinlichkeiten
angeben. Aus \(\mathbb{P}(X=x)\) im diskreten Fall wird im stetigen Fall
also:

\[\mathbb{P}(a<X\leq b), \quad a<b\]

Bei dieser Funktion sprechen wir von einer \emph{kumulative
Verteilungsfunktion} \(F(x)=\mathbb{P}(X\leq x)\), wobei immer gilt:

\[\mathbb{P}(a<X\leq b) = F(b)-F(a)\]

Wann immer wir im diskreten Fall eine Wahrscheinlichkeitsfunktion
verwendet haben um eine ZV zu beschreiben, verwenden wir im stetigen
Fall die \textbf{Dichtefunktion} (\emph{probability densitity function}
- PDF) einer ZV. Hierbei handelt es sich um eine integrierbare und
nicht-negative Funktion \(f(x)\geq 0 \forall x\in \mathbb{R}\) mit
\(\int_{-\infty}^{\infty}f(x)dx=1\) für die gilt:

\[\mathbb{P}([a,b])=\int_a^bf(x)dx\]

Dementsprechend können wir den Ausdruck für die kumulative
Verteilungsfunktion von oben ergänzen:

\[\mathbb{P}(a<X\leq b) = F(b)-F(a)=\int_a^bf(x)dx\]

Man sieht hier, dass die Dichtefunktion einer ZV die Ableitung ihrer
kumulative Verteilungsfunktion ist. Wie oben beschrieben können wir die
Werte an einzlnen Punkten nicht als \emph{absolute} Wahrscheinlichkeiten
interpretieren, da die Wahrscheinlichkeit für einzelne Punkte immer
gleich 0 ist. Wir können aber die Werte der PDF an zwei oder mehr
Punkten vergleichen um die \emph{relative} Wahrscheinlichkeit der
einzelnen Punkte zu bekommen.

Wie bei den diskreten ZV beschreiben wir eine ZV mit Hilfe von
bestimmten Kennzahlen, wie dem \textbf{Erwartungswert}, der
\textbf{Varianz} und den \textbf{Quantilen}. Diese sind quasi äquivalent
zum diskreten Fall definiert, nur eben über Integrale (wir vergleichen
alle folgenden Definitionen mit ihrem diskreten Pendant am Ende des
Abschnitts). Für den Erwartungswert der ZV \(X\) gilt somit:

\[\mathbb{E}(X)=\int_{-\infty}^{\infty}xf(x)dx\]

Für die Varianz und die Standardabweichung entsprechend:

\[Var(X)= \mathbb{E}(X-\mathbb{E}\left(X)\right)^2=\int_{-\infty}^{\infty}(x-\mathbb{E}(X))^2f(x)dx\]

\[\sigma_X=\sqrt{Var(X)}\]

Und, schlussendlich, gilt für das \(\alpha\)-Quantil \(q(\alpha)\):

\[\mathbb{P}(X\leq q(\alpha))=\alpha\]

Im folgenden werden das \(0.25\) und \(0.5\)-Quantil visuell
dargestellt:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-20-1} \end{center}

Abschließend wollen wir nun noch einmal die Definitionen der Kennzahlen
und charakteristischer Verteilungen für den stetigen und diskreten Fall
vergleichen:

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.17\columnwidth}\raggedright\strut
Bezeichnung\strut
\end{minipage} & \begin{minipage}[b]{0.38\columnwidth}\raggedright\strut
Diskreter Fall\strut
\end{minipage} & \begin{minipage}[b]{0.37\columnwidth}\raggedright\strut
Stetiger Fall\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
Erwartungswert\strut
\end{minipage} & \begin{minipage}[t]{0.38\columnwidth}\raggedright\strut
\(\mathbb{E}(x)=\sum_{x\in W_X}\mathbb{P}(X=x)x\)\strut
\end{minipage} & \begin{minipage}[t]{0.37\columnwidth}\raggedright\strut
\(\mathbb{E}(X)=\int_{-\infty}^{\infty}xf(x)dx\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
Varianz\strut
\end{minipage} & \begin{minipage}[t]{0.38\columnwidth}\raggedright\strut
\(Var(X)=\sum_{x\in W_X}\left[x-\mathbb{E}(X)\right]^2 \mathbb{P}(X=x)x\)\strut
\end{minipage} & \begin{minipage}[t]{0.37\columnwidth}\raggedright\strut
\(Var(X)= \mathbb{E}(X-\mathbb{E}\left(X)\right)^2\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
Standard-abweichung\strut
\end{minipage} & \begin{minipage}[t]{0.38\columnwidth}\raggedright\strut
\(\sqrt{Var(X)}\)\strut
\end{minipage} & \begin{minipage}[t]{0.37\columnwidth}\raggedright\strut
\(\sqrt{Var(X)}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
\(\alpha\)-Quantil\strut
\end{minipage} & \begin{minipage}[t]{0.38\columnwidth}\raggedright\strut
\(\mathbb{P}(X\leq q(\alpha))=\alpha\)\strut
\end{minipage} & \begin{minipage}[t]{0.37\columnwidth}\raggedright\strut
\(\mathbb{P}(X\leq q(\alpha))=\alpha\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
Dichtefunktion (PDF)\strut
\end{minipage} & \begin{minipage}[t]{0.38\columnwidth}\raggedright\strut
NA\strut
\end{minipage} & \begin{minipage}[t]{0.37\columnwidth}\raggedright\strut
\(\mathbb{P}([a,b])=\int_a^bf(x)dx\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
Wahrsch's-funktion (PMF)\strut
\end{minipage} & \begin{minipage}[t]{0.38\columnwidth}\raggedright\strut
\(p(x_k)=\mathbb{P}(X=x_k)\)\strut
\end{minipage} & \begin{minipage}[t]{0.37\columnwidth}\raggedright\strut
NA\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright\strut
Kumulierte Verteilungsfunktion (CDF)\strut
\end{minipage} & \begin{minipage}[t]{0.38\columnwidth}\raggedright\strut
\(\mathbb{P}(X\leq x)\)\strut
\end{minipage} & \begin{minipage}[t]{0.37\columnwidth}\raggedright\strut
\(F(x)=\mathbb{P}(X\leq x)\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Analog zum diskreten Fall wollen wir uns nun die am häufigsten
vorkommenden stetigen Verteilungen noch einmal genauer anschauen.

\subsection{Beispiel: die
Uniformverteilung}\label{beispiel-die-uniformverteilung}

Die Uniformverteilung kann auch einem beliebigen Intervall \([a,b]\) mit
\(a<b\) definiert werden und ist dadurch gekennzeichnet, dass die Dichte
über \([a,b]\) vollkommen konstant ist. Ihre einzigen Parameter sind die
Grenzen des Intervalls, \(a\) und \(b\).

Da bei stetigen Verteilungen die Dichte für aller Werte außerhalb des
Wertebereichs per definitionem gleich Null ist, haben wir folgenden
Ausdruck für die Dichte der Uniformverteilung:

\[f(x)=
\begin{cases} 
      \frac{1}{b-a} & a\leq x \leq b \\
      0 & \text{sonst} \left(x\notin W_X\right)
   \end{cases}
   \] Auch der Erwartungswert ist dann intuitiv definiert, er liegt
nämlich genau in der Mitte des Intervalls \([a,b]\). Er ist definiert
als \(\mathbb{E}(X)=\frac{a+b}{2}\) und ihre Varianz mit
\(Var(X)=\frac{(b-a)^2}{12}\) gegeben.

Ihre Dichtefunktion für \([a,b]=[2,4]\) ist im folgenden dargestellt:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-21-1} \end{center}

Die Abkürung in R für die Uniformverteilung ist \texttt{unif}.
Endsprechend berechnen wir Werte für die Dichte mit \texttt{dunif()},
welches lediglich die Argumente \texttt{a} und \texttt{b} für die
Grenzen des Intervalls benötigt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dunif}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25
\end{verbatim}

Wie wir sehen erhalten wir hier immer den gleichen Wert
\(\frac{1}{b-a}\), was die zentrale Eigenschaft der Uniformverteilung
ist. Hier wird auch deutlich, dass dieser Wert die \emph{relative}
Wahrscheinlichkeit angibt, da die absolute Wahrscheinlichkeit für jeden
einzelnen Wert wie oben beschrieben bei stetigen ZV 0 ist.

Die CDF berechnen wir entsprechend mit \texttt{punif()}. Wenn
\(X\propto U(0,4)\) erhalten wir \(\mathbb{P}(X\leq3)\) entprechend mit:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{punif}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2
\end{verbatim}

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-24-1} \end{center}

Auch ansonsten können wir die Syntax der diskreten Verteilungen mehr
oder weniger übernehmen: \texttt{qunif()} akzeptiert die gleichen
Parameter wie \texttt{punif()} und gibt uns Werte der inversen CDF.
\texttt{runif()} kann verwendet werden um Realisierungen einer uniform
verteilten ZV zu generieren:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uniform_sample <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{4}\NormalTok{)}
\NormalTok{uniform_sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5209862 1.4563675 1.1529571 0.6825809 0.6886870
\end{verbatim}

\subsection{Beispiel: die
Normalverteilung}\label{beispiel-die-normalverteilung}

Die wahrscheinlich bekannteste stetige Verteilung ist die
Normalverteilung. Das liegt nicht nur daran, dass viele natürliche
Phänomene als die Realisierung einer normalverteilten ZV modelliert
werden können, sondern auch weil es sich mit der Normalverteilung in der
Regel sehr einfach rechnen ist. Sie ist also häufig auch einfach eine
bequeme Annahme.

Bei der Normalverteilung handelt es sich um eine
\textbf{zwei-parametrige} Verteilung über den Wertebereich
\(W_X=\mathbb{R}\). Die beiden Parameter sind \(\mu\) und \(\sigma^2\),
welche unmittelbar als Erwartungswert (\(\mathbb{E}(X)=\mu\)) und
Varianz (\(Var(X)=\sigma^2\)) gelten. Wir schreiben
\(X\propto \mathscr{N}(\mu, \sigma^2)\) wenn für die PDF von \(X\) gilt:

\[f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \]

Unter der \textbf{Standard-Normalverteilung} verstehen wir eine
Normalverteilung mit den Paramtern \(\mu=0\) und \(\sigma=1\).\footnote{Viele
  Tabellen mit bestimmten Kennzahlen der Normalverteilung beziehen sich
  auf die Standard-Normalverteilung. Wenn man diese Werte verwenden
  will, muss man die tatsächlich verwendete Stichprobe ggf. erst
  \href{https://de.wikipedia.org/wiki/Standardisierung_(Statistik)}{z-transformieren}.
  Unter letzterem versteht man die \emph{Normalisierung} einer ZV sodass
  sie den Erwartungswert 0 und die Varianz 1 besitzt. Dies geht i.d.R.
  für jede ZV \(X\) recht einfach über die Formel
  \(Z=\frac{X-\mu}{\sigma}\), wobei \(Z\) die standartisierte ZV,
  \(\mu\) den Erwartungswert und \(\sigma\) die Standardabweichung von
  \(X\) bezeichnet} Sie verfügt über die deutlich vereinfachte PDF:

\[f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\]

Die CDF der Normalverteilung ist analytisch nicht einfach darzustellen,
die Werte können in R aber leicht über die Funktion \texttt{pnorm}
(s.u.) abgerufen werden.

Im folgenden sind die PDF und CDF für exemplarische
Parameterkombinationen dargestellt:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-26-1} \end{center}

Die Abkürzung in R ist \texttt{norm}. Alle Funktionen nehmen die
Paramter \(\mu\) und \(\sigma\) (nicht \(\sigma^2\)) über \texttt{mean}
und \texttt{sd} als notwendige Argumente. Ansonsten ist die Verwendung
äquivalent zu den vorherigen Beispielen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dnorm}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{mean =} \DecValTok{1}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{2}\NormalTok{) }\CommentTok{# relative Wahrscheinlichkeiten über PDF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1933341 0.1979188
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pnorm}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{mean =} \DecValTok{1}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{2}\NormalTok{) }\CommentTok{# Werte der CDF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4012937 0.4502618
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qnorm}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{mean =} \DecValTok{1}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{2}\NormalTok{) }\CommentTok{# Werte der I-CDF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.00000 2.34898
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{norm_sample <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{1}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{2}\NormalTok{) }\CommentTok{# 5 Realisierungen der ZV}
\NormalTok{norm_sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.9099446 -0.5698089 -2.3358839  0.2395470  2.8379932
\end{verbatim}

\begin{quote}
\textbf{Beispiel zum Zusammenhang} \texttt{dnorm()} und \texttt{qnorm()}
\end{quote}

\subsection{Beispiel: die
Exponentialverteilung}\label{beispiel-die-exponentialverteilung}

Sehr häufig wird uns auch die Exponentialverteilung begegnen. Außerhalb
der Ökonomik wird sie v.a. zur Modellierung von Zerfallsprozessen oder
Wartezeiten verwendet, in der Ökonomik spielt sie in der
Wachstumstheorie eine zentrale Rolle. Es handelt sich bei der
Exponentialverteilung um eine \textbf{ein-parametrige} Verteilung mit
Parameter \(\lambda \in \mathbb{R}^+\) und mit dem Wertebereich
\(W_X=[0, \infty ]\).

Die PDF der Exponentialverteilung ist:

\[f(x)=\begin{cases}
0 & x < 0\\
\lambda e^{-\lambda x} & x \geq 0
\end{cases}\]

wobei \(e\) die
\href{https://de.wikipedia.org/wiki/Eulersche_Zahl}{Eulersche Zahl} ist.
Die CDF ist entsprechend:

\[F(x)=\begin{cases}
0 & x < 0\\
1-e^{-\lambda x} & x \geq 0
\end{cases}\]

Beide Verteilungen sind im folgenden dargestellt:

\begin{center}\includegraphics{ChapA-Wahrscheinlichkeitstheorie_files/figure-latex/unnamed-chunk-28-1} \end{center}

Der Erwartungswert und die Varianz sind für die Exponentialverteilung
äquivalent und hängen ausschließlich von \(\lambda\) ab:
\(\mathbb{E}(X)=\sigma_X=\frac{1}{\lambda}\).

Die Abkürzung in R ist \texttt{exp}. Alle Funktionen nehmen den Paramter
\(\lambda\) über das Argument \texttt{rate} an:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dexp}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{rate =} \DecValTok{1}\NormalTok{) }\CommentTok{# relative Wahrscheinlichkeiten über PDF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6065307 0.4723666
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pexp}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{rate =} \DecValTok{1}\NormalTok{) }\CommentTok{# Werte der CDF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3934693 0.5276334
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qexp}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{rate =} \DecValTok{1}\NormalTok{) }\CommentTok{# Werte der I-CDF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6931472 1.3862944
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp_sample <-}\StringTok{ }\KeywordTok{rexp}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{rate =} \DecValTok{1}\NormalTok{) }\CommentTok{# 5 Realisierungen der ZV}
\NormalTok{exp_sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8232605 0.4757590 3.4635949 1.2740277 1.0814852
\end{verbatim}

Es gibt übrigens einen
\href{https://www.exponentialverteilung.de/vers/beweise/uebergang_poissonverteilung.html}{wichtigen
Zusammenhang} zwischen der stetigen Exponential- und der diskreten
Poisson-Verteilung.

\section{Zusammenfassung
Wahrscheinlichkeitsmodelle}\label{zusammenfassung-wahrscheinlichkeitsmodelle}

Die folgende Tabelle fasst noch einmal alle Wahscheinlichkeitsmodelle
zusammen, die wir bislang betrachtet haben:

\begin{longtable}[]{@{}llll@{}}
\toprule
Verteilung & Art & Abkürzung & Parameter\tabularnewline
\midrule
\endhead
Binomialverteilung & Diskret & \texttt{binom} & \texttt{size},
\texttt{prob}\tabularnewline
Poisson-Verteilung & Diskret & \texttt{pois} &
\texttt{lambda}\tabularnewline
Uniform-Verteilung & Kontinuierlich & \texttt{punif} & \texttt{min},
\texttt{max}\tabularnewline
Normalverteilung & Kontinuierlich & \texttt{norm} & \texttt{mean},
\texttt{sd}\tabularnewline
Exponential-Verteilung & Kontinuierlich & \texttt{exp} &
\texttt{rate}\tabularnewline
\bottomrule
\end{longtable}

In der statistischen Praxis sind das die Modelle, die wir verwenden, die
DGP (\emph{data generating processes}) zu beschreiben - also die
Prozesse, welche die Daten, die wir in unserer Forschung verwenden,
generiert haben.

Deswegen sprechen Statistiker*innen auch häufig von
\emph{Populationsmodellen}. Am besten stellt man es sich mit Hilfe der
\texttt{r*()} Funktionen vor: man nimmt an, dass es einen DGP gibt, und
unsere Daten der Output der \texttt{r*()}-Funktion zum Ziehen von
Realisierungen sind. Mit dem Begriff des Populationsmodells macht man
dabei deutlich, dass unsere Stichprobe nur eine Stichprobe darstellt -
und nicht die gesamte Population aller möglichen Realisierungen des DGP.

Nun wird auch deutlich, warum Kenntnisse in der
Wahrscheinlichkeitsrechnung so wichtig sind: wenn wir statistisch mit
Daten arbeiten, dann versuchen wir in der Regel über die Daten
Rückschlüsse auf den DGP zu schließen. Dafür müssen wir zunächst einmal
eine grobe Struktur für den DGP annehmen, und dafür brauchen wir
Kenntnisse in der Wahrscheinlichkeitsrechnung und für den entsprechenden
Anwendungsfall konkrete Vorannahmen. Dann können wir, gegeben unsere
Daten, unsere Beschreibung des DGP verfeinern.

Im Großteil dieses Kurses bedeutet das, dass wir für den DGP ein
bestimmtes Wahrscheinlichkeitsmodell annehmen und dann auf Basis unserer
Daten die Parameter für dieses Modell schätzen wollen. Dieses Vorgehen
nennen wir \emph{parametrisch}, weil wir hier vor allem Parameter
schätzen wollen.\footnote{Die Alternative, \emph{nicht-parametrische}
  Verfahren, nehmen kein konkretes Wahrscheinlichkeitsmodell an, sondern
  wählen das Modell auch auf Basis der Daten.}

\hypertarget{desk-stat}{\chapter{Wiederholung: Deskriptive
Statistik}\label{desk-stat}}

Bevor wir uns im \protect\hyperlink{stat-rep}{nächsten Anhang} mit dem
Schluss von den Daten auf die Parameter des zugrundeliegenden
Wahrscheinlichkeitsmodells beschäftigen, wollen wir uns im Folgenden
noch mit Methoden der deskriptiven Statistik beschäftigen: denn zum
einen setzt dieser Rückschluss der Daten auf das Populationsmodell
voraus, dass wir uns überhaupt mit den Daten auseinandergesetzt haben,
zum anderen sollte die Wahl des zugrundeliegenden Populationsmodell und
der Art der Schätzung auf Basis der Daten erfolgen - und auch dafür
benötigen wir Methoden der deskriptiven Statistik.

Die Methoden der deskriptiven Statistik helfen uns die Daten, die wir
erhoben haben möglichst gut zu \emph{beschreiben}. Die
\emph{deskriptive} Statistik grenzt sich von der \emph{induktiven}
Statistik davon ab, dass wir keine Aussagen über unseren Datensatz
hinaus treffen wollen: wenn unser Datensatz also z.B. aus 1000
Schüler\emph{innen besteht treffen wir mit den Methoden der deskriptiven
Statistik nur Aussagen über genau diese 1000 Schüler}innen. Mit Methoden
der \emph{induktiven} Statistik würden wir versuchen Aussagen über
Schüler*innen im Allgemeinen, zumindest über mehr als diese 1000
Schüler*innen zu treffen. Das ist genau der am Ende des vorherigen
Anhangs angesprochene Schluss von den Daten auf den \emph{data
generating process} (DGP).

In diesem Abschnitt beschäftigen wir uns zunächst nur mit der
deskriptiven Statistik. Das ist konsistent mit dem praktischen Vorgehen:
bevor wir irgendwelche Methoden der induktiven Statistik anwenden müssen
wir immer zunächst unsere Daten mit Hilfe deskriptiver Statistik besser
verstehen.

Der Code in diesem Kapitel verwendet die folgenden Pakete:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(ggpubr)}
\KeywordTok{library}\NormalTok{(latex2exp)}
\KeywordTok{library}\NormalTok{(icaeDesign)}
\KeywordTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

Für die direkte Anwendung in R verwenden wir einen Datensatz zu
ökonomischen Journalen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{journal_daten <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"data/tidy/journaldaten.csv"}\NormalTok{))}
\KeywordTok{head}\NormalTok{(journal_daten)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Kuerzel                                               Titel
## 1:    APEL                   Asian-Pacific Economic Literature
## 2:  SAJoEH           South African Journal of Economic History
## 3:      CE                             Computational Economics
## 4:  MEPiTE MOCT-MOST Economic Policy in Transitional Economics
## 5:    JoSE                          Journal of Socio-Economics
## 6:   LabEc                                    Labour Economics
##                    Verlag Society Preis Seitenanzahl Buchstaben_pS
## 1:              Blackwell      no   123          440          3822
## 2: So Afr ec history assn      no    20          309          1782
## 3:                 Kluwer      no   443          567          2924
## 4:                 Kluwer      no   276          520          3234
## 5:               Elsevier      no   295          791          3024
## 6:               Elsevier      no   344          609          2967
##    Zitationen Gruendung Abonnenten           Bereich
## 1:         21      1986         14           General
## 2:         22      1986         59  Economic History
## 3:         22      1987         17       Specialized
## 4:         22      1991          2      Area Studies
## 5:         24      1972         96 Interdisciplinary
## 6:         24      1994         15             Labor
\end{verbatim}

Dieser Datensatz enthält Informationen über Preise, Seiten, Zitationen
und Abonennten von 180 Journalen aus der Ökonomik im Jahr
2004.\footnote{Bei den hier verwendeten Daten handelt es sich um eine
  Übersetzung des Datensatzes \texttt{Journals} aus dem Paket
  \texttt{AER} \citep{AER}.}

\section{Kennzahlen zur Lage und Streuung der
Daten}\label{kennzahlen-zur-lage-und-streuung-der-daten}

Die am häufigsten verwendeten Kennzahlen der deskriptiven Statistik sind
das \textbf{arithmetische Mittel}, die \textbf{Standardabweichung} und
die \textbf{Quantile}. Für die folgenden Illustrationen nehmen wir an,
dass wir es mit einem Datensatz mit \(N\) kontinuiertlichen
Beobachtungen \(x_1, x_2, ..., x_n\) zu tun haben.

Das \textbf{arithmetische Mittel} ist ein klassisches Lagemaß und
definiert als:

\[\bar{x}=\frac{1}{N}\sum_{i=1}^Nx_i\] In R wird das arithmetische
Mittel mit der Funktion \texttt{mean()} berechnet:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{avg_preis <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(journal_daten[[}\StringTok{"Preis"}\NormalTok{]])}
\NormalTok{avg_preis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 417.7222
\end{verbatim}

Der durchschnittliche Preis der Journale ist also 417.7222222.

Die \textbf{Standardabweichung} ist dagegen ein Maß für die Streuung der
Daten und wird als die Quadratwurzel der \emph{Varianz}
definiert:\footnote{Man beachte den im Vergleich zur Varianzformel für
  theoretische Modelle modifizierten Nenner \(N-1\)!}

\[s_x=\sqrt{Var(x)}=\sqrt{\frac{1}{N-1}\sum_{i=1}^N\left(x_i-\bar{x}\right)^2}\]

Wir verwenden in R die Funktionen \texttt{var()} und \texttt{sd()} um
Varianz und Standardabweichung zu berechnen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preis_var <-}\StringTok{ }\KeywordTok{var}\NormalTok{(journal_daten[[}\StringTok{"Preis"}\NormalTok{]])}
\NormalTok{preis_sd <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(journal_daten[[}\StringTok{"Preis"}\NormalTok{]])}
\KeywordTok{cat}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}
  \StringTok{"Varianz: "}\NormalTok{, preis_var, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
  \StringTok{"Standardabweichung: "}\NormalTok{, preis_sd}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Varianz: 148868.335816263
## Standardabweichung: 385.834596448094
\end{verbatim}

Das \(\alpha\)-\textbf{Quantil} eines Datensatzes ist der Wert, bei dem
\(\alpha\cdot 100\%\) der Datenwerte kleiner und
\((1-\alpha)\cdot 100\%\) der Datenwerte kleiner sind. In R können wir
Quantile einfach mit der Funktion \texttt{quantile()} berechnen. Diese
Funktion akzeptiert als erstes Argument einen Vektor von Daten und als
zweites Argument ein oder mehrere Werte für \(\alpha\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(journal_daten[[}\StringTok{"Preis"}\NormalTok{]], }\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 50% 
## 282
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(journal_daten[[}\StringTok{"Preis"}\NormalTok{]], }\KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    25%    50%    75% 
## 134.50 282.00 540.75
\end{verbatim}

Diese Werte können folgendermaßen interpretiert werden: 25\% der
Journale kosten weniger als 134.5 Dollar, 50\% der Journale kosten
weniger als 282 Dollar und 75\% kosten weniger als 540.75 Dollar.

Dabei wird das \(0.5\)-Quantil auch \textbf{Median} genannt. Wie beim
Mittelwert handelt es sich hier um einen Lageparameter, der allerdings
robuster gegenüber Extremwerten ist, da es sich nur auf die Reihung der
Datenpunkte bezieht, nicht auf ihren numerischen Wert.\footnote{Wenn das
  teuerste Journal sich im Preis verdoppelt erhöht dies den Mittelwert
  beträchtlich, ändert den Median aber nicht.}

Wie im Kapitel \protect\hyperlink{basics}{Erste Schritte in R} für
\texttt{mean()} und \texttt{sd()} erklärt, akzeptieren auch die
Funktionen \texttt{mean()}, \texttt{var()}, \texttt{sd()} und
\texttt{quantile()} das optionale Argument \texttt{na.rm}, mit dem
fehlende Werte vor der Berechnung eliminiert werden können:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_daten <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\KeywordTok{quantile}\NormalTok{(test_daten, }\FloatTok{0.75}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in quantile.default(test_daten, 0.75): missing values and NaN's not allowed if 'na.rm' is FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(test_daten, }\FloatTok{0.75}\NormalTok{, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  75% 
## 7.75
\end{verbatim}

Ein häufig verwendetes Steuungsmaß, das im Gegensatz zu
Standardabweichung und Varianz robust gegen Ausreißer ist, ist die
\textbf{Quartilsdifferenz}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quantil_}\DecValTok{25}\NormalTok{ <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(journal_daten[[}\StringTok{"Preis"}\NormalTok{]], }\FloatTok{0.25}\NormalTok{, }\DataTypeTok{names =}\NormalTok{ F)}
\NormalTok{quantil_}\DecValTok{75}\NormalTok{ <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(journal_daten[[}\StringTok{"Preis"}\NormalTok{]], }\FloatTok{0.75}\NormalTok{, }\DataTypeTok{names =}\NormalTok{ F)}
\NormalTok{quant_differenz <-}\StringTok{ }\NormalTok{quantil_}\DecValTok{75} \OperatorTok{-}\StringTok{ }\NormalTok{quantil_}\DecValTok{25}
\NormalTok{quant_differenz}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 406.25
\end{verbatim}

Das optionale Argument \texttt{names=FALSE} unterdrückt die Benennung
der Ergebnisse. Wenn wir das nicht machen würde, würde
\texttt{quant\_differenz} verwirrenderweise den Namen \texttt{75\%}
tragen.

\section{Korrelationsmaße}\label{korrelationsmae}

Wie im Beispiel der Journale in diesem Kapitel erheben wir für einzelne
Untersuchungsobjekte in der Regel mehr als eine Ausprägung. Im
vorliegenden Falle haben wir das einzelne Journal z.B. Informationen
unter anderem über Preis, Dicke und Zitationen. Häufig möchten wir
wissen wie diese verschiedene Ausprägungen miteinender in Beziehung
stehen. Zum Beispiel möchten wir wissen, ob dickere Journale tendenziell
teurer sind. Neben der wichtigen grafischen Inspektion der Daten, zu der
es ein eigenes Kapitel geben wird, gibt es dafür wichtige quantitative
Maße, die häufig in den Bereich der Korrelationsmaße fallen.

Das einfachste Korrelationsmaß ist die empirische \textbf{Ko-Varianz},
die zwei stetige Ausprägungen \(x\) und \(y\) folgendermaßen definiert
ist:

\[s_{xy}=\frac{1}{N-1}\sum_{n=1}^N\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right) \]

Wenn wir die empirische Kovarianz für den Bereich \([-1, 1]\) normieren
erhalten wir die \textbf{empirische Korrelation} dieser Ausprägungen
Handelt es sich bei den beiden Ausprägung um stetige Ausprägungen nennen
wir das resultierende Maß den
\textbf{Pearson-Korrelationskoeffizienten}:

\[\rho_{x,y}=\frac{s_{xy}}{s_xs_y}, \quad \rho\in[-1,1]\]

wobei \(s_{xy}\) die Kovarianz der Ausprägungen \(x\) und \(y\) und
\(s_x\) und \(s_y\) deren Standardabweichung bezeichnet.

Der so definierte Korrelationskoeffizient informiert uns über die
Richtung und die Stärke des \textbf{linearen Zusammenhangs} zwischen
\(x\) und \(y\). Wenn \(\rho_{x,y}>0\) liegt ein positiver linearer
Zusammenhang vor, d.h. größere Werte von \(x_i\) treten in der Tendenz
mit größeren Werten von \(y_i\) auf. Hierbei gilt, dass
\(\rho_{x,y}=1 \leftrightarrow y_i = a + b x_i\) für \(a\in \mathbb{R}\)
und \(b>0\) Umgekehrt gilt, dass wenn \(\rho_{x,y}<0\) ein negativer
linearer Zusammenhang vorliegt und
\(\rho_{x,y}=-1 \leftrightarrow y_i = a + b x_i\) für
\(a\in \mathbb{R}\) und \(b<0\). Bei \(\rho_{x,y}=0\) liegt \textbf{kein
linearer} Zusammenhang zwischen den Ausprägungen vor.

Wie wir unten sehen werden, enthält \(\rho\) keine Informationen über
nicht-lineare Zusammenhänge zwischen \(x\) und \(y\). Vorsicht bei der
Interpretation ist also angebracht.

In unserem Datensatz haben wir z.B. Informationen über die Seitenzahl
(Spalte \texttt{Seiten}) und den Preis von Journalen (Spalte
\texttt{Preis}). Wir könnten uns nun fragen, ob dickere Journale
tendenziell teurer sind. Dazu können wir, wenn wir uns nur für den
linearen Zusammenhang interessieren, den
Pearson-Korrelationskoeffizienten mit der Funktion \texttt{cor()}
berechnen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(journal_daten[[}\StringTok{"Preis"}\NormalTok{]], journal_daten[[}\StringTok{"Seitenanzahl"}\NormalTok{]], }
    \DataTypeTok{method =} \StringTok{"pearson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4937243
\end{verbatim}

Wir sehen also, dass es tatsächlich einen mittleren positiven linearen
Zusammenhang zwischen Preis und Seitenzahl zu geben scheint.

Über das Argument \texttt{method} der Funktion \texttt{cor()} können
auch andere Korrelationsmaße berechnet werden: der
\href{https://de.wikipedia.org/wiki/Rangkorrelationskoeffizient\#Spearman'scher_Rangkorrelationskoeffizient}{Spearman-Korrelationskoeffizient}
(\texttt{method=\textquotesingle{}spearman\textquotesingle{}}) oder der
\href{https://de.wikipedia.org/wiki/Rangkorrelationskoeffizient\#Kendall'sches_Tau}{Kendall-Korrelationskoeffizient}
(\texttt{method=\textquotesingle{}kendall\textquotesingle{}}) sind
beides Maße, die nur die Ränge der Ausprägungen und nicht deren
numerische Werte berücksichtigen. Dies macht sie immun gegen Ausreißer
und wir müssen keine Annahme über die Art der Korrelation machen wie
beim Pearson-Korrelationskoeffizient, der nur lineare Zusammenhänge
quantifiziert. Gleichzeitig gehen uns natürlich auch viele Informationen
verloren. Das richtige Maß ist wie immer kontextabhängig und muss
entsprechend theoretisch begründet werden.

Darüber hinaus erlaubt die Funktion \texttt{cor()} über das Argument
\texttt{use} noch den Umgang mit fehlenden Werten genauer zu
spezifizieren. Wenn Sie an der (nicht-standartisierten) Ko-Varianz
interessiert sind, können Sie diese über die Funktion \texttt{cov()}
berechnen, die analog zu \texttt{cor()} funktioniert.

In jedem Fall ist bei der Interpretation von Korrelationen Vorsicht
angebracht: da der Korrelationskoeffizient nur die Stärke des
\emph{linearen} Zusammenhangs misst, können dem gleichen
Korrelationskoeffizienten sehr unterschiedliche nicht-lineare
Zusammenhänge zugrunde liegen:

\begin{center}\includegraphics{ChapA-DeskriptiveStatistik_files/figure-latex/unnamed-chunk-11-1} \end{center}

Daher ist es immer wichtig die Daten auch visuell zu inspizieren.
Datenvisualisierung ist aber so wichtig, dass sie in einem eigenen
Kapitel behandelt wird.

\section{Hinweise zur quantitativen und visuellen
Datenbeschreibung}\label{hinweise-zur-quantitativen-und-visuellen-datenbeschreibung}

Wie das Beispiel der Korrelationsmaße gerade demonstriert hat, ist bei
der Verwendung von quantitativen Maßen zur Beschreibung von Datensätzen
immer große Vorsicht geboten. Sie sollten daher \emph{immer} gemeinsam
mit grafischen Darstellungsformen, wie Streudiagrammen oder Histogrammen
verwendet werden.

Eine schöne Illustration ist \href{}{Anscombe's Quartett}
\citep{Anscombe}. Dabei handelt es sich um vier Datensätze, die alle
(fast exakt) gleiche deskriptive Statistiken aufweisen, jedoch
offensichtlich sehr unterschiedlich sind. Diese offensichtlichen
Unterschiede werden aber nur durch grafische Inspektion deutlich.

Der Datensatz ist in jeder R Installation vorhanden:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"anscombe"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(anscombe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x1 x2 x3 x4   y1   y2    y3   y4
## 1 10 10 10  8 8.04 9.14  7.46 6.58
## 2  8  8  8  8 6.95 8.14  6.77 5.76
## 3 13 13 13  8 7.58 8.74 12.74 7.71
## 4  9  9  9  8 8.81 8.77  7.11 8.84
## 5 11 11 11  8 8.33 9.26  7.81 8.47
## 6 14 14 14  8 9.96 8.10  8.84 7.04
\end{verbatim}

Die folgende Tabelle gibt die Werte der quantitativen Kennzahlen an:

\begin{longtable}[]{@{}ll@{}}
\toprule
Kennzahl & Wert\tabularnewline
\midrule
\endhead
Mittelwert von \(x\) & \texttt{9}\tabularnewline
Mittelwert von \(y\) & \texttt{7.5}\tabularnewline
Varianz von \(x\) & \texttt{11}\tabularnewline
Varianz von \(y\) & \texttt{4.13}\tabularnewline
Korrelation zw. \(x\) und \(y\) & \texttt{0.82}\tabularnewline
\bottomrule
\end{longtable}

Die grafische Inspektion zeigt, wie unterschiedlich die Datensätze
tatsächlich sind:

\includegraphics{ChapA-DeskriptiveStatistik_files/figure-latex/unnamed-chunk-13-1.pdf}

Interessanterweise ist bis heute nicht bekannt wie \citet{Anscombe}
seinen Datensatz erstellt hat. Für neuere Sammlungen von Datensätzen,
die das gleiche Phänomen illustrieren siehe z.B. \citet{AnscombeNew1}
oder \citet{AnscombeNew2} . Eine sehr schöne Illustration der Idee
findet sich auch auf
\href{https://www.autodeskresearch.com/publications/samestats}{dieser
Homepage}, die vom Autor von \citet{AnscombeNew2} gestaltet wurde.

\section{Zusamenfassung}\label{zusamenfassung}

In der folgenden Tabelle wollen wir noch einmal die hier besprochenen
Funktionen für den Themenbereich `Deskriptive Statistik' zusammenfassen:

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.13\columnwidth}\raggedright\strut
Maßzahl\strut
\end{minipage} & \begin{minipage}[b]{0.33\columnwidth}\raggedright\strut
Funktion\strut
\end{minipage} & \begin{minipage}[b]{0.46\columnwidth}\raggedright\strut
Beschreibung\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Mittelwert\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{mean()}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Wichtiges Lagemaß; arithmetisches Mittel der Daten\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Varianz\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{var()}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Maß für die Streuung; Einheit oft schwer interpretiertbar\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Standardabweichung\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{sd()}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Üblichstes Maß für die Streuung\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
\(\alpha\)-Quantil\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{quantile()}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
\(\alpha\cdot 100\%\) der Werte sind kleiner \(\alpha\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Median\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{quantile(0.5)}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Robustes Lagemaß; die Hälfte der Daten sind größer/kleiner\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Ko-Varianz (num. Daten)\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{cov(method\ =\ \textquotesingle{}pearson\textquotesingle{})}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Nicht-normierter linearer Zusammenhang\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Ko-Varianz (Ränge)\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{cov(method\ =\ \textquotesingle{}kendall\textquotesingle{})}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Ko-Varianz der Ränge nach der Kendall-Methode\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Ko-Varianz (Ränge)\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{cov(method\ =\ \textquotesingle{}spearman\textquotesingle{})}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Ko-Varianz der Ränge nach der Spearman-Methode\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Pearson Korrelationskoeffizient\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{cor(method\ =\ \textquotesingle{}pearson\textquotesingle{})}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
In \([-1, 1]\) normierter linearer Zusammenhang\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Spearman-Korrelationskoeffizient\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{cor(method\ =\ \textquotesingle{}kendall\textquotesingle{})}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Korrelation der Ränge nach der Kendall-Methode\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
Kendall-Korrelationskoeffizient\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright\strut
\texttt{cor(method\ =\ \textquotesingle{}spearman\textquotesingle{})}\strut
\end{minipage} & \begin{minipage}[t]{0.46\columnwidth}\raggedright\strut
Korrelation der Ränge nach der Spearman-Methode\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{stat-rep}{\chapter{Wiederholung: Drei Verfahren der
schließenden Statistik}\label{stat-rep}}

In diesem Kapitel werden wir drei zentrale Verfahren der schließenden
Statistik wiederholen. Dabei schließen wir unmittelbar an die beiden
vorangegangenen Kapitel zur
\protect\hyperlink{stat-stoch}{Wahrscheinlichkeitstheorie} und
\protect\hyperlink{desk-stat}{deskriptiven Statistik} an: mit Hilfe der
Wahrscheinlichkeitstheorie beschreiben wir mögliche Prozesse, die unsere
Daten generiert haben könnten (DGP - \emph{data generating processes}).
Mit Hilfe der deskriptiven Statistik beschreiben wir unsere Daten und
wählen auf dieser Basis Kandidaten für den DGP und sinnvolle
Schätzverfahren aus. In der \emph{schließenden Statistik} geht es nun
genau um diese Schätzverfahren, die es uns erlauben von unseren Daten
Rückschlüsse auf die DGP zu ziehen. Eine andere Art dies auszudrücken
ist: mit Hilfe der schließenden Statistik wollen wir durch Analyse
unserer Stichprobe auf die Gesamtpopulation, aus der die Stichprobe
gezogen wurde schließen - und dabei möglichst die Unsicherheit, die
diesem Schließprozess inhärent ist genau quantifizieren.

Natürlich ist wie immer Vorsicht geboten: wie bei der deskriptiven
Statistik suggerieren viele der quantitativen Methoden der schließenden
Statistik eine Genauigkeit und Exaktheit, die in der Wirklichkeit an der
Korrektheit vieler Annahmen hängt. Man darf daher nicht den Fehler
machen, die `genauen' Ergebnisse der schließenden Statistik
unhinterfragt zu glauben. Gleichzeitig darf man sie auch nicht
verteufeln, denn viele Annahmen kann man mit ein wenig formalem Geschick
und theoretischen Kenntnissen auch sinnvoll hinsichtlich ihrer
Angemessenheit überprüfen.

Dafür ist es wichtig, die Grundlagen der schließenden Statistik gut
verstanden zu haben. In diesem Kapitel wiederholen wir diese Grundlagen
grob und kombinieren die Wiederholung mit einer Einführung in die
entsprechenden Befehle in R.

Wie oben bereits angekündigt gehen wir in der Regel gehen wir davon aus,
dass die von uns beobachteten Daten das Resultat eines gewissen
Zufallsprozesses ist, den wir mit Hilfe der Wahrscheinlichkeitstheorie
mathematisch beschreiben können. Da wir den DGP aber nicht direkt
beobachten können, müssen wir auf Basis von empirischen Hinweisen und
theoretischem Wissen entscheiden, welches Wahrscheinlichkeitsmodell wir
unserer Analyse zugrunde legen. Sobald wir das getan haben, versuchen
wir die Parameter, die für das von uns ausgewählte
wahrscheinlichkeitstheoretische Modell relevant sind, so zu wählen, dass
sie die Daten möglichst gut erklären können. Man nennt derlei Ansätze in
der Statistik \textbf{parametrische Verfahren}, weil man mit den Daten
die Parameter eines Modells bestimmen will, das man vorher selbst
ausgewählt hat. Alternativ gibt es auch \textbf{nicht-parametrische
Verfahren}: hier wird auch das Modell auf Basis der Daten bestimmt. Hier
beschäftigen wir uns jedoch nur mit den parametrischen Verfahren.

In diesem Kontext sind drei Vorgehen in der statistischen Analyse
besonders gängig:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Punktschätzung:}
\item
  \textbf{Statistische Tests:}
\item
  \textbf{Konfidenzintervalle}
\end{enumerate}

Wir wollen die verschiedenen Vorgehensweisen auf anhand eines Beispiels
durchspielen: Nehnem wir an wir haben einen Datensatz und wir nehmen an,
dass diese Daten von einer \emph{Binominalverteilung} stammen.\footnote{Wenn
  Sie nicht mehr wissen, was eine Binominalverteilung ist finden Sie im
  \protect\hyperlink{stat-stoch}{Anhang zur Wahrscheinlichkeitstheorie}
  eine Erläuternug.} Wir wissen, dass die Binominalverteilung durch zwei
Parameter spezifiziert wird: \(n\) als die Anzahl der Versuche und \(p\)
als die Erfolgswahrscheinlichkeit für den einzelnen Versuch. Wir sind
nun daran interessiert auf Basis von unseren Daten Aussagen über den
Paramter \(p\) der zugrundeliegenden Binominalverteilung zu
treffen.\footnote{Die Annahme, dass die Daten \emph{überhaupt} von einer
  Binominalverteilung stammen wird hier nicht in Frage gestellt! Das ist
  genau die Vor-Annahme, die wir bei parametrischen Verfahren treffen
  müssen.}

Wenn wir einen konkreten Wert für \(p\) herausbekommen wollen müssen wir
ein Verfahren der \emph{Punktschätzung} wählen.\\
Wenn wir wissen wollen ob ein bestimmter Wert für \(p\) gegeben der
Daten plausibel ist, dann sollten wir mit \emph{statistischen Tests}
(oder `Hypothesentests') arbeiten. Wenn wir schließlich ein Intervall
für \(p\) spezifizieren wollen, das mit den Beobachtungen kompatibel
ist, dann suchen wir nach einem \emph{Konfidenzintervall} für \(p\).

Im folgenden werden die drei Verfahren in größerem Detail besprochen.
Der Code in diesem Kapitel verwendet dabei die folgenden Pakete:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(ggpubr)}
\KeywordTok{library}\NormalTok{(latex2exp)}
\KeywordTok{library}\NormalTok{(icaeDesign)}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

\section{Punktschätzung}\label{punktschatzung}

Bei der Punktschätzung geht es darum auf Basis der Daten konkrete Werte
für die Parameter der den Daten zugrundeliegenden Verteilung zu
schätzen. In der Regel bezeichnet man den Parameter, den man schätzen
möchte, mit dem Symbol \(\theta\). Der Grund ist Faulheit und bessere
Lesbarkeit: man kann dann nämlich die selbe Notation verwenden, egal
welche zugrundeliegende Verteilung man vorher ausgewählt hat.

Im vorliegenden Fall wollen wir also einen konkreten Wert für \(\theta\)
auf Basis der Daten schätzen. Dabei ist ganz wichtig zu beachten, dass
wir den wahren Wert von \(\theta\) in der Regel nicht kennen und auch
nie genau kennen lernen werden.

Um zwischen dem wahren Wert von \(\theta\) und dem Schätzer für
\(\theta\) in unserer Notation unterscheiden zu können, verwenden wir
das \(\hat{\cdot}\)-Symbol. Entsprechend bezeichnet \(\hat{\theta}\)
einen \textbf{Schätzer} für \(\theta\).

Ein Schätzer ist dabei eine Funktion, die als Input unsere Daten nimmt,
und als Output einen Wert ausgibt, der eine möglichst gute Schätzung für
\(\theta\) darstellt. Entsprechend können wir für eine Stichprobe vom
Umfang \(n\) schreiben:

\[\hat{\theta}: \mathbb{R}^n \rightarrow \mathbb{R}, \quad \hat{\theta}=\hat{\theta}(x_1,...,x_n)\]

Damit ist auch klar, dass es sich bei einem Schätzer um eine
Zufallsvariable (ZV) handelt: Funktionen von ZV sind selbst ZV und
unsere Daten \(x_1,...,x_n\) interpretieren wir ja als Realisierungen
von ZV \(X_1,...,X_n\). Der unbekannt wahre Wert \(\theta\) ist dagegen
keine ZV.

\begin{quote}
\textbf{Hinweis: Schätzer vs.~geschätzter Wert} Die Unterscheidung
zwischen einem Schätzer (\emph{estimator}) und einem geschätzten Wert
(\emph{estimate}) ist in der Statistik zentral: der Schätzer beschreibt
die Prozedur einen geschätzten Wert zu bekommen. Er nimmt in der Regel
die Form einer Formel oder eines Algorithmus an. Der \emph{geschätzte
Wert} ist für einen konkreten Anwendungsfall der Wert, den der Schätzer
liefert.
\end{quote}

Die Konstruktion von Schätzern ist keine einfache Aufgabe. Wir lernen im
Laufe der Veranstaltungen verschiedene Methoden, wie die
\emph{Momentenmethode} und die \emph{Maximum-Likelihood Methode} genauer
kennen.

\section{Hypothesentests}\label{hypothesentests}

Wir verwenden statistische Tests um Fragen der folgenden Art zu
beantworten: gegeben der Daten die wir sehen und der Annahmen, die wir
treffen, ist ein bestimmter Wert für Parameter \(\theta\) plausibel?

\begin{quote}
\textbf{Beispiel:} Das klassische Beispiel ist die Frage, ob eine Münze
manipuliert wurde oder nicht. Wenn wir beim Ereignis `Zahl' von Erfolg
sprechen, dann können wir \(n\) Münzwürfe als Binomialverteilung mit
\(B(n,p)\) modellieren. Bei einer nicht manipulierten Münze wäre
\(p=0.5\): die Wahrscheinlichkeit, dass wir das Ereignis `Zahl' erleben
liegt beim einzelnen Wurf bei 50\%. Nennen wir das unsere Ausgangs-,
oder \emph{Nullhypothese}. Zur Überprüfung dieser Hypothese werfen wir
die Münze nun 100 mal. Nehmen wir nun an, dass wir das Ereignis `Zahl'
in 60 von 100 Würfen beobachten. Bedeutet das, dass unsere Nullhypothese
von \(p=0.5\) plausibel ist? Um diese Frage zu beantworten fragen wir
uns, wie wahrscheinlich es bei \(p=0.5\) wäre, tatsächlich 60 mal Zahl
zu beobachten. Diese Wahrscheinlichkeit können wir berechnen, aus
Tabellen auslesen oder von R bestimmen lassen (die genaue Verwendung der
Funktion \texttt{binom.test()} wird unten genauer besprochen):
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b_test_object <-}\StringTok{ }\KeywordTok{binom.test}\NormalTok{(}\DataTypeTok{x =} \DecValTok{60}\NormalTok{, }\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }\DataTypeTok{p =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{b_test_object[[}\StringTok{"p.value"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05688793
\end{verbatim}

\begin{quote}
Die Wahrscheinlichkeit liegt also bei 5.7 \%. Dies ist der so genannte
p-Wert. In der Regel lehnt man eine Hypothese ab, wenn \(p<0.1\) oder
\(p<0.05\). Im vorliegenden Falle ist unsere Hypothese einer fairen
Münze aber kompatibel mit der Beobachtung von 60 mal Zahl.
\end{quote}

Wir wollen nun das Vorgehen aus dem Beispiel generalisieren und das
standardmäßige Vorgehen bei einem statistischen Test
zusammenfassen:\footnote{Wir beschränken uns hier auf so genannte
  \emph{parametrische} Tests. Das bedeutet, dass wir zunächst ein
  bestimmtes Modell für den Datengenerierungsprozess annehmen. Im
  Beispiel war dieses Modell die Binomialverteilung. Es gibt auch Tests,
  die ohne eine solche Annahme auskommen. Sie werden
  \emph{nicht-parametrisch} genannt und später in dem Kurs besprochen.}

\textbf{1. Schritt: Aufstellen eines wahrscheinlichkeitstheoretischen
Modells} Zunächst müssen wir eine Annahme über den Prozess treffen,
welcher der Generierung unserer Daten zugrunde liegt. Im Beispiel oben
haben wir eine Binomialverteilung \(\mathcal{B}(n,p)\) angenommen. Diese
Entscheidung muss auf Basis von theoretischen und empirischen
Überlegungen getroffen werden. Für diskrete Daten macht es z.B. keinen
Sinn eine stetige Verteilung anzunehmen und umgekehrt.

\textbf{2. Schritt: Formulierung der Nullhypothese} Die Hypothese, die
wir mit unseren Daten testen wollen wird \textbf{Nullhypothese} genannt.
Wir wollen also immer fragen, ob \(H_0\) gegeben der Daten plausibel
ist. Die Formulierung von \(H_0\) wird also durch unser
Erkenntnisinteresse bestimmt. In der Regel formulieren wir eine
Hypothese, die wir verwerfen wollen als \(H_0\).\footnote{An machen
  Stellen der sozial- und wirtschaftswissenschaftlichen Literatur wird
  anstelle von ``verwerfen'' auch das Wort ``falsifizieren'' benutzt um
  die Zurückweisung der Null-Hypothese zu umschreiben. Diese Wortwahl
  ist allerdings irreführend, da hier nicht Aussagen aus einer Theorie
  widerlegt werden, die einen gewissen Zusammenhang behaupten. Im
  Gegensatz wird die Hypothese zurückgewiesen, dass der vermutete
  Zusammenhang eben nicht besteht - die zu Grunde gelegte Theorie wird
  also durch die Zurückweisung der Null-Hypothese im Normalfall nicht
  widerlegt sondern vielmehr bestätigt.} Wenn wir also die Hypothese
bezüglich eines Parameters \(\theta\) testen wollen, dass
\(\beta\neq 0\), dann formulieren wir \(H_0: \theta = 0\). Anders
formuliert: wir möchten andere mit den Daten überzeugen, dass \(H_0\)
falsch ist.

Aus der Nullhypothese und unserem Erkenntnisinteresse ergibt sich die
\textbf{Alternativhypothese} \(H_1\). Sie umfasst alle interessierenden
Ereignsse, die \(H_0\) widersprechen. Je nach dem wie wir \(H_1\)
formulieren unterscheiden wir folgende Arten von Hypothesentests:

\(H_0: \theta=0\) und \(H_1: \theta\neq 0\): hier sprechen wir von einem
\textbf{zwei-seitigen Test}, denn wir machen keine Aussage darüber ob
die Alternative zu \(H_0\) entweder in \(\theta>0\) oder \(\theta<0\)
liegt. Gemeinsam decken \(H_0\) und \(H_1\) hier alle möglichen
Ereignisse ab.

\(H_0: \theta=0\) und \(H_1: \theta> 0\): Hier sprechen wir von einem
\textbf{einseitigen Test nach oben}. Wir fragen uns hier nur ob
\(\theta\) größer ist als 0. Der Fall, dass \(\theta<0\), wird nicht
beachtet. Natürlich können wir den einseitigen Test auch andersherum
formulieren als \(H_0: \theta=0\) und \(H_1: \theta< 0\). Dann sprechen
wir von einem \textbf{einseitigen Test nach unten}.

\begin{quote}
\textbf{Beispiel:} Wenn wir unser Münzbeispiel von oben betrachten
können wir die drei verschiedenen Testarten folgendermaßen
konkretisieren: beim \emph{zweiseitigen Test} wäre \(H_0: p=0.5\) und
\(H_1: p\neq 0.5\) und wir würden ganz allgemein fragen ob die Münze
manipuliert ist. Beim \textbf{einseitigen Test nach oben} würden wir
\(H_0: p=0.5\) und \(H_1: p>0.5\) testen und damit fragen ob die Münze
\emph{zugunsten von Zahl} manipuliert wurde. Wir lassen dabei die
Möglichkeit, dass die Münze zugunsten von Kopf manipuliert wurde völlig
außen vor. Beim \textbf{einseitigen Test nach unten} wäre es genau
umgekehrt: \(H_0: p=0.5\) und \(H_1: p<0.5\). KONKRETE BERECHNUNGEN FÜR
DEN P WERT
\end{quote}

\textbf{3. Schritt: Berechnung einer Teststatistik} Wir überlegen nun
welche Verteilung unserer Daten wir erwarten würden \emph{wenn die
Nullhypothese korrekt wäre}. Wenn wir im ersten Schritt also eine
Binomialverteilung mit \(n=100\) angenommen haben und \(H_0: p=0.5\),
dann würden wir vermuten, dass unsere Daten gemäß \(B(n, 0.5)\) verteilt
sind.\footnote{In der Praxis wird die Berechnung der Teststatistik durch
  eine R Funktion in einem der nächsten Schritte übernommen, aber es
  macht Sinn, sich das grundsätzliche Vorgehen dennoch in dieser Sequenz
  bewusst zu machen.} Diese theoretische Verteilung können wir dann mit
den tatsächlichen Daten vergleichen und fragen, wie wahrscheinlich es
ist diese Daten tatsächlich so beobachten zu können wenn \(H_0\) wahr
wäre:

\begin{center}\includegraphics{ChapA-SchliesendeStatistik_files/figure-latex/unnamed-chunk-3-1} \end{center}

\textbf{4. Schritt: Festlegung des Signifikanzniveaus}: Wir müssen nun
festlegen welches Risiko wir bereit sind einzugehen, unsere
Nullhypothese \(H_0\) zu verwerfen, obwohl sie eigentlich richtig ist.
Die maximale Wahrscheinlichkeit für dieses unglückliche Ereigns
bezeichnen wir mit \(\alpha\) uns sie bestimmt unser Signifikanzniveau.
Typischweise nimmt man als Standardwert \(\alpha=0.05\), d.h. wir
konstruieren unsere Test so, dass die Wahrscheinlichkeit, dass wir
\(H_0\) fälschlicherweise verwerfen maximal \(\alpha=0.05\) beträgt. Mit
anderen Worten, wir legen hier die Wahrscheinlichkeit für einen
\textbf{Fehler 1. Art} explizit fest.\footnote{Wir sprechen von einem
  \emph{Fehler 1. Art} wenn wir auf Basis eines Tests \(H_0\) verwerfen
  obwohl sie eigentlich richtig ist. Von einem \emph{Fehler 2. Art}
  sprechen wir, wenn wir \(H_0\) nicht verwerfen, obwohl \(H_0\)
  eigentlich falsch ist.}

Aus dem gewählten Signifikanzniveau ergibt sich dann der
\textbf{Verwerfungsbereich} für unsere Nullhypothese. Wenn unsere
beobachteten Daten im Verwerfungsbereich liegen wollen wir \(H_0\) als
verworfen betrachten.\footnote{Ganz im Sinne von Popper können mit Hilfe
  von statistischen Tests alle Hypothesen immer nur verworfen werden.
  Verifizieren können wir nichts!} Es ergibt sich logisch aus dem vorher
gesagten, dass ein höheres \(\alpha\) mit einem größeren
Verwerfungsbereich einhergeht.

Der Verwerfungsbereich für das oben darstellte Beispiel mit
\(H_0: \theta=0\) und \(H_1: \theta\neq 0\) ergibt sich für
\(\alpha=0.05\) also folgendermaßen:

\begin{center}\includegraphics{ChapA-SchliesendeStatistik_files/figure-latex/unnamed-chunk-4-1} \end{center}

\textbf{5. Schritt: Die Entscheidung} Wenn sich die beobachtbaren Daten
im Verwerfungsbereich befinden wollen wir \(H_0\) verwerfen und die
Nullhypothese entsprechend als verworfen ansehen. Falls nicht kann die
Nullhypothese nicht verworfen werden - was aber nicht bedeutet, dass sie
\emph{verifiziert} wurde. Letzteres ist mit statistischen Tests nicht
möglich.

In \texttt{R} werden die gerade besprochenen Tests in der Regel in einer
Funktion zusammengefasst. Die Wahl der Funktion wird dabei von der im
ersten Schritt angenommenen Verteilung bestimmt. Im Falle der
Binomialverteilung verwenden wir die Funktion \texttt{binom.test()},
welche eine Liste mit relevanten Informationen über den Test erstellt.
Es macht Sinn, dieser Liste einen Namen zuzuweisen und dann die
relevanten Informationen explizit abzurufen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b_test_object <-}\StringTok{ }\KeywordTok{binom.test}\NormalTok{(}\DataTypeTok{x =} \DecValTok{60}\NormalTok{, }\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }\DataTypeTok{p =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\KeywordTok{typeof}\NormalTok{(b_test_object)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

Bevor wir uns mit dem Ergebnis befassen wollen wir uns die notwendigen
Argumente von \texttt{binom.test()} genauer anschauen (eine gute
Erläuterung liefert wie immer \texttt{help(binom.test)}).

Über das Argument \texttt{x} informieren wir R über die tatsächlich
beobachtete Anzahl von Erfolgen (in unserem Fall hier 60). Das Argument
\texttt{n} spezifiziert die Anzahl der Beobachtungen. Mit \texttt{p}
geben wir den unter \(H_0\) angenommenen Wert für die
Erfolgswahrscheinlichkeit an. Mit dem Argument \texttt{alternative}
informieren wir R schließlich darüber ob wir einen zweiseitigen
(\texttt{alternative\ =\ "two.sided"}), einen einseitigen Test nach oben
(\texttt{alternative\ =\ "greater"}) oder einen einseitigen Test nach
unten (\texttt{alternative\ =\ "less"}) durchführen wollen.

Wenn wir einen Überblick über die Ergebnisse bekommen wollen können wir
das Objekt direkt aufrufen. Die Liste wurde innerhalb der Funktion
\texttt{binom.test} so modifiziert, dass uns die Zusammenfassung visuell
ansprechend aufbereitet angezeigt wird:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b_test_object}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Exact binomial test
## 
## data:  60 and 100
## number of successes = 60, number of trials = 100, p-value =
## 0.05689
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.4972092 0.6967052
## sample estimates:
## probability of success 
##                    0.6
\end{verbatim}

Die Überschrift macht deutlich was für ein Test durchgeführt wurde und
die ersten beiden Zeilen fassen noch einmal die Daten zusammen. In der
zweiten Zeile findet sich zudem der \textbf{p-Wert}. Der p-Wert gibt die
Wahrscheinlichkeit an, mit der die beobacheten Daten unter \(H_0\)
tatsächlich beobachtet werden können. Wir können den p-Wert aus der
theoretischen Verteilung von oben auf der y-Achse ablesen, wenn wir den
beobachteten Wert auf der x-Achse suchen:

\begin{center}\includegraphics{ChapA-SchliesendeStatistik_files/figure-latex/unnamed-chunk-7-1} \end{center}

Die nächste Zeile formuliert dann die Alternativhypothese aus (und hängt
entsprechend vom Argument \texttt{alternative} ab). Die Zeilen danach
geben das 95\%-Intervall an (mehr dazu im nächsten Abschnitt) und den
Punktschätzer für den zu testenden Parameter (siehe vorheriger
Abschnitt).

Wenn wir wissen wollen welche Informationen die so erstellte Liste sonst
noch für uns bereit hält, bzw. wie wir diese Informationen direkt
ausgeben lassen können, sollten wir uns die Struktur der Liste genauer
ansehen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(b_test_object)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 9
##  $ statistic  : Named num 60
##   ..- attr(*, "names")= chr "number of successes"
##  $ parameter  : Named num 100
##   ..- attr(*, "names")= chr "number of trials"
##  $ p.value    : num 0.0569
##  $ conf.int   : num [1:2] 0.497 0.697
##   ..- attr(*, "conf.level")= num 0.95
##  $ estimate   : Named num 0.6
##   ..- attr(*, "names")= chr "probability of success"
##  $ null.value : Named num 0.5
##   ..- attr(*, "names")= chr "probability of success"
##  $ alternative: chr "two.sided"
##  $ method     : chr "Exact binomial test"
##  $ data.name  : chr "60 and 100"
##  - attr(*, "class")= chr "htest"
\end{verbatim}

Wir sehen hier, dass wir viele der Werte wie bei Listen üblich direkt
anwählen können, z.B. den p-Wert:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b_test_object[[}\StringTok{"p.value"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05688793
\end{verbatim}

Oder das den Punktschätzer für \(p\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b_test_object[[}\StringTok{"estimate"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## probability of success 
##                    0.6
\end{verbatim}

Wenn wir eine andere Verteilung annehmen, verwenden wir auch eine andere
Testfunktion, das Prinzip ist aber sehr ähnlich. Wollen wir z.B. für
einen beobachtbaren Datensatz die Hypothese testen, ob der Datensatz aus
einer Normalverteilung mit dem Erwartungswert \(\mu=0.5\) stammen
könnte, würden wir die Funktion \texttt{t.test()} verwenden.

Zum Abschluss dieses Abschnitts wollen wir kurz auf die \emph{Macht von
statistischen Tests} (engl: \emph{Power}) und auf die \emph{Wahl
zwischen einseitigen und zweiseitigen Tests} eingehen.

\textbf{Die Macht eines Tests und Fehler 1. und 2. Art}:

Wir sprechen von einem \emph{Fehler 1. Art} wenn wir auf Basis eines
Tests \(H_0\) verwerfen obwohl sie eigentlich richtig ist. Von einem
\emph{Fehler 2. Art} sprechen wir, wenn wir \(H_0\) nicht verwerfen,
obwohl \(H_0\) eigentlich falsch ist.

In der Wissenschaft hat es sich ergeben, dass man vor allem auf den
Fehler 1. Art schaut. Denn man möchte auf gar keinen Fall eine
Nullhypothese verwerfen, obwohl sie eigentlich richtig ist. In der
Praxis würde dies bedeuten, eine Aussage zu vorschnell zu treffen.
Deswegen wählt man in den empirischen Studien das Signifikanzniveau so,
dass die Wahrscheinlichkeit für einen Fehler 1. Art sehr klein ist, in
der Regel 5\%.

Leider geht damit eine vergleichsweise hohe Wahrscheinlichkeit für einen
\emph{Fehler 2. Art} einher, denn die beiden Fehler sind untrennbar
miteinender verbunden: reduzieren wir bei gleichbleibender
Stichprobengröße die Wahrscheinlichkeit für einen Fehler 1. Art, erhöhen
wir damit die Wahrscheinlichkeit für einen Fehler 2. Art und umgekehrt.

Dennoch ist auch ein Fehler 2. Art relevant. Die Wahrscheinlichkeit für
einen solchen Fehler ist invers mit der \textbf{Macht} (engl:
\emph{power}) eines Tests verbunden, die definiert ist als:

\[\text{Macht}=1-\mathbb{P}(\text{Fehler 2. Art})\]

Eine vertiefte Diskussion von Macht und dem Trade-Off zwischen Fehlern
1. und 2. Art findet zu einem späteren Zeitpunkt in der Vorlesung statt.

\textbf{Die Wahl zwischen einseitigen und zweiseitigen Tests}:

Wir haben oben am Beispiel der potenziell manipulierten Münze
folgendermaßen zwischen einseitigen und zweiseitigen Tests
unterschieden: Beim zwei-seitigen Test testen wir \(H_0: p=0.5\) gegen
\(H_1: p\neq 0.5\). Wir überprüfen also ob die Münze entweder zugunsten
oder zulasten von Zahl manipuliert wurde.

Beim einseitigen Test testen wir nur gegen eine Alternative:
\(H_0: p=0.5\) bleibt gleich allerdings ist die Alternativhypothese nun
entweder \(H_1: p<0.5\) oder \(H_1: p>0.5\). Im ersten Fall überprüfen
wir also nur ob die Münze zugunsten von Zahl manipuliert wurde, im
zweiten Fall nur ob die Münze zugunsten von Kopf manipuliert wurde.

Man mag sich nun fragen wo der Vorteil von einseitigen Tests liegt,
erscheint der zweiseitige Test doch allgemeiner. Letzteres ist zwar
richtig, allerdings ist die Macht des zweiseitigen Tests im Vergleich
zum einseitigen Tests deutlich geringer. Das bedeutet, dass wenn möglich
immer der einseitige Test verwendet werden soll. Die Beurteilung ob ein
einseitiger oder zweiseitiger Test angemessen ist, muss auf Basis von
Vorwissen getroffen werden, und häufig spielen theoretische Überlegungen
oder Kontextwissen eine wichtige Rolle.

\section{Berechnung von
Konfidenzintervallen}\label{berechnung-von-konfidenzintervallen}

Konfidenzintervalle für einen Parameter geben eine Antwort auf die
Frage: \emph{``Welche Werte für den interessierenden Parameter sind mit
unseren Daten kompatibel?''} Wie bei Hypothesentests müssen wir zur
Berechnung von Konfidenzintervallen ein Signifikanzniveau \(\alpha\)
festlegen. Das liegt daran, dass zwischen Konfidenzintervallen und
Hypothesentests eine enge Verbindung besteht: ein Konfidenzintervall
\(I_{\alpha}\) besteht aus allen Parameterwerten, die bei einem
zweiseitigen Hypothesentest zum Signifikanzniveau \(\alpha\) als
Nullhypothese nicht verworfen werden können.

Wir haben oben auch schon gesehen, dass das Konfidenzintervall ganz
leicht aus den typischen Test-Funktionen in R ausgelesen werden kann.
Für das Beispiel der Binomialverteilung schreiben wir daher nur:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b_test_object <-}\StringTok{ }\KeywordTok{binom.test}\NormalTok{(}\DataTypeTok{x =} \DecValTok{60}\NormalTok{, }\DataTypeTok{n =} \DecValTok{100}\NormalTok{, }\DataTypeTok{p =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\NormalTok{b_test_object[[}\StringTok{"conf.int"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4972092 0.6967052
## attr(,"conf.level")
## [1] 0.95
\end{verbatim}

Die Interpretation dieses Intervals ist dabei die folgende: wenn der
zugrundeliegende Datengenerierungsprozess sehr häufig wiederholt werden
würde, dann würde 95\% der jeweils berechneten 95\%-Konfidenzintervalle
diesen wahren Wert enthalten. Wir können \textbf{auf gar keinen Fall}
behaupten, dass ein bestimmtes Konfidenzintervall den wahren
Parameterwert mit einer Wahrscheinlichkeit von \(95\)\% enthält. Eine
solche Aussage macht auch keinen Sinn: der wahre Wert ist - wie eingangs
beschrieben - keine Zufallsvariable.\footnote{Diese Interpretation ist
  etwas sperrig und das hängt mit dem
  \href{https://de.wikipedia.org/wiki/Frequentistischer_Wahrscheinlichkeitsbegriff}{frequentistischen
  Wahrscheinlichkeitsbegriff} zusammen, den wir hier verwenden. Einen
  philosophisch attraktiveren Weg stellt der
  \href{https://de.wikipedia.org/wiki/Bayesscher_Wahrscheinlichkeitsbegriff}{bayessche
  Wahrscheinlichkeitsbegriff}, auf dem die die Bayesianische Statistik
  aufbaut. Letztere werden wir hier allerdings nicht behandeln können}

\chapter{Referenzen}\label{refs}

\bibliography{book.bib,packages.bib}


\end{document}
