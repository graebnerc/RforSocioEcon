<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Grundlagen der Wahrscheinlichkeitstheorie | R für die sozio-ökonomische Forschung</title>
  <meta name="description" content="Einführung in R für die sozioökonomische Forschung; Version 0.9.2" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Grundlagen der Wahrscheinlichkeitstheorie | R für die sozio-ökonomische Forschung" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Einführung in R für die sozioökonomische Forschung; Version 0.9.2" />
  <meta name="github-repo" content="graebnerc/RforSocioEcon" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Grundlagen der Wahrscheinlichkeitstheorie | R für die sozio-ökonomische Forschung" />
  
  <meta name="twitter:description" content="Einführung in R für die sozioökonomische Forschung; Version 0.9.2" />
  

<meta name="author" content="Dr. Claudius Gräbner" />


<meta name="date" content="2020-12-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="formalia.html"/>
<link rel="next" href="desk-stat.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R für die sozioökonomische Forschung</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Willkommen</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#danksagung"><i class="fa fa-check"></i>Danksagung</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lizenz"><i class="fa fa-check"></i>Lizenz</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#änderungshistorie"><i class="fa fa-check"></i>Änderungshistorie</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="precons.html"><a href="precons.html"><i class="fa fa-check"></i><b>1</b> Vorbemerkungen</a>
<ul>
<li class="chapter" data-level="1.1" data-path="precons.html"><a href="precons.html#warum-r"><i class="fa fa-check"></i><b>1.1</b> Warum R?</a></li>
<li class="chapter" data-level="1.2" data-path="precons.html"><a href="precons.html#besonderheiten-von-r"><i class="fa fa-check"></i><b>1.2</b> Besonderheiten von R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="einrichtung.html"><a href="einrichtung.html"><i class="fa fa-check"></i><b>2</b> Einrichtung</a>
<ul>
<li class="chapter" data-level="2.1" data-path="einrichtung.html"><a href="einrichtung.html#installation-von-r-und-r-studio"><i class="fa fa-check"></i><b>2.1</b> Installation von R und R-Studio</a></li>
<li class="chapter" data-level="2.2" data-path="einrichtung.html"><a href="einrichtung.html#die-r-studio-oberfläche"><i class="fa fa-check"></i><b>2.2</b> Die R Studio Oberfläche</a></li>
<li class="chapter" data-level="2.3" data-path="einrichtung.html"><a href="einrichtung.html#einrichtung-eines-r-projekts"><i class="fa fa-check"></i><b>2.3</b> Einrichtung eines R Projekts</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="einrichtung.html"><a href="einrichtung.html#arbeitsverzeichnisse-und-pfade"><i class="fa fa-check"></i><b>2.3.1</b> Arbeitsverzeichnisse und Pfade</a></li>
<li class="chapter" data-level="2.3.2" data-path="einrichtung.html"><a href="einrichtung.html#schritt-1-projektordner-anlegen"><i class="fa fa-check"></i><b>2.3.2</b> Schritt 1: Projektordner anlegen</a></li>
<li class="chapter" data-level="2.3.3" data-path="einrichtung.html"><a href="einrichtung.html#schritt-2-ein-r-studio-projekt-im-projektordner-erstellen"><i class="fa fa-check"></i><b>2.3.3</b> Schritt 2: Ein R-Studio Projekt im Projektordner erstellen</a></li>
<li class="chapter" data-level="2.3.4" data-path="einrichtung.html"><a href="einrichtung.html#unterordner"><i class="fa fa-check"></i><b>2.3.4</b> Schritt 3: Relevante Unterordner erstellen</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="einrichtung.html"><a href="einrichtung.html#optional-schritt-4-und-das-here-paket"><i class="fa fa-check"></i><b>2.4</b> Optional: Schritt 4 und das here-Paket</a></li>
<li class="chapter" data-level="2.5" data-path="einrichtung.html"><a href="einrichtung.html#abschließende-bemerkungen"><i class="fa fa-check"></i><b>2.5</b> Abschließende Bemerkungen</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>3</b> Erste Schritte in R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="basics.html"><a href="basics.html#es:befehle"><i class="fa fa-check"></i><b>3.1</b> Befehle in R an den Computer übermitteln</a></li>
<li class="chapter" data-level="3.2" data-path="basics.html"><a href="basics.html#es:objekte"><i class="fa fa-check"></i><b>3.2</b> Objekte, Funktionen und Zuweisungen</a></li>
<li class="chapter" data-level="3.3" data-path="basics.html"><a href="basics.html#es:objektarten"><i class="fa fa-check"></i><b>3.3</b> Grundlegende Objeke in R</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="basics.html"><a href="basics.html#funktionen"><i class="fa fa-check"></i><b>3.3.1</b> Funktionen</a></li>
<li class="chapter" data-level="3.3.2" data-path="basics.html"><a href="basics.html#basics-types-vectors"><i class="fa fa-check"></i><b>3.3.2</b> Vektoren</a></li>
<li class="chapter" data-level="3.3.3" data-path="basics.html"><a href="basics.html#basics-logic"><i class="fa fa-check"></i><b>3.3.3</b> Logische Werte (logical)</a></li>
<li class="chapter" data-level="3.3.4" data-path="basics.html"><a href="basics.html#wörter-character"><i class="fa fa-check"></i><b>3.3.4</b> Wörter (character)</a></li>
<li class="chapter" data-level="3.3.5" data-path="basics.html"><a href="basics.html#fehlende-werte-und-null"><i class="fa fa-check"></i><b>3.3.5</b> Fehlende Werte und NULL</a></li>
<li class="chapter" data-level="3.3.6" data-path="basics.html"><a href="basics.html#indizierung-und-ersetzung"><i class="fa fa-check"></i><b>3.3.6</b> Indizierung und Ersetzung</a></li>
<li class="chapter" data-level="3.3.7" data-path="basics.html"><a href="basics.html#nützliche-funktionen-für-atomare-vektoren"><i class="fa fa-check"></i><b>3.3.7</b> Nützliche Funktionen für atomare Vektoren</a></li>
<li class="chapter" data-level="3.3.8" data-path="basics.html"><a href="basics.html#listen"><i class="fa fa-check"></i><b>3.3.8</b> Listen</a></li>
<li class="chapter" data-level="3.3.9" data-path="basics.html"><a href="basics.html#introfactors"><i class="fa fa-check"></i><b>3.3.9</b> Faktoren</a></li>
<li class="chapter" data-level="3.3.10" data-path="basics.html"><a href="basics.html#intro-matrix"><i class="fa fa-check"></i><b>3.3.10</b> Matrizen</a></li>
<li class="chapter" data-level="3.3.11" data-path="basics.html"><a href="basics.html#data-frames"><i class="fa fa-check"></i><b>3.3.11</b> Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="basics.html"><a href="basics.html#es:pakete"><i class="fa fa-check"></i><b>3.4</b> Pakete</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Datenkunde und Datenaufbereitung</a>
<ul>
<li class="chapter" data-level="" data-path="data.html"><a href="data.html#verwendete-pakete"><i class="fa fa-check"></i>Verwendete Pakete</a></li>
<li class="chapter" data-level="4.1" data-path="data.html"><a href="data.html#data-arten"><i class="fa fa-check"></i><b>4.1</b> Arten von Daten</a></li>
<li class="chapter" data-level="4.2" data-path="data.html"><a href="data.html#data-get"><i class="fa fa-check"></i><b>4.2</b> Datenakquise</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="data.html"><a href="data.html#exkurs-1-ländercodes-übersetzen"><i class="fa fa-check"></i><b>4.2.1</b> Exkurs 1: Ländercodes übersetzen</a></li>
<li class="chapter" data-level="4.2.2" data-path="data.html"><a href="data.html#data-download-R"><i class="fa fa-check"></i><b>4.2.2</b> Exkurs 2: Daten direkt mit R herunterladen</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="data.html"><a href="data.html#data-read-write"><i class="fa fa-check"></i><b>4.3</b> Daten einlesen und schreiben</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="data.html"><a href="data.html#einlesen-von-datensätzen"><i class="fa fa-check"></i><b>4.3.1</b> Einlesen von Datensätzen</a></li>
<li class="chapter" data-level="4.3.2" data-path="data.html"><a href="data.html#speichern-von-daten"><i class="fa fa-check"></i><b>4.3.2</b> Speichern von Daten</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="data.html"><a href="data.html#data-wrangling"><i class="fa fa-check"></i><b>4.4</b> Verarbeitung von Daten (‘data wrangling’)</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="data.html"><a href="data.html#das-konzept-von-tidy-data"><i class="fa fa-check"></i><b>4.4.1</b> Das Konzept von ‘tidy data’</a></li>
<li class="chapter" data-level="4.4.2" data-path="data.html"><a href="data.html#data-long-wide"><i class="fa fa-check"></i><b>4.4.2</b> Von langen und breiten Datensätzen</a></li>
<li class="chapter" data-level="4.4.3" data-path="data.html"><a href="data.html#data-merge"><i class="fa fa-check"></i><b>4.4.3</b> Zusammenführen von Daten</a></li>
<li class="chapter" data-level="4.4.4" data-path="data.html"><a href="data.html#date-select"><i class="fa fa-check"></i><b>4.4.4</b> Datensätze filtern und selektieren</a></li>
<li class="chapter" data-level="4.4.5" data-path="data.html"><a href="data.html#data-summary"><i class="fa fa-check"></i><b>4.4.5</b> Datensätze zusammenfassen</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="data.html"><a href="data.html#data-manycols"><i class="fa fa-check"></i><b>4.5</b> Gleichzeigite Bearbeitung mehrerer Spalten</a></li>
<li class="chapter" data-level="4.6" data-path="data.html"><a href="data.html#data-role"><i class="fa fa-check"></i><b>4.6</b> Abschließende Bemerkungen zum Umgang mit Daten innerhalb eines Forschungsprojekts</a></li>
<li class="chapter" data-level="4.7" data-path="data.html"><a href="data.html#data-packages"><i class="fa fa-check"></i><b>4.7</b> Anmerkungen zu Paketen</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="vis.html"><a href="vis.html"><i class="fa fa-check"></i><b>5</b> Visualisierung von Daten</a>
<ul>
<li class="chapter" data-level="" data-path="data.html"><a href="data.html#verwendete-pakete"><i class="fa fa-check"></i>Verwendete Pakete</a></li>
<li class="chapter" data-level="" data-path="vis.html"><a href="vis.html#einleitung"><i class="fa fa-check"></i>Einleitung</a></li>
<li class="chapter" data-level="5.1" data-path="vis.html"><a href="vis.html#vis-theorie"><i class="fa fa-check"></i><b>5.1</b> Optional: Theoretische Grundlagen</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="vis.html"><a href="vis.html#vis-base-ggplot2"><i class="fa fa-check"></i><b>5.1.1</b> <code>ggplot2</code> vs. <code>base plot</code></a></li>
<li class="chapter" data-level="5.1.2" data-path="vis.html"><a href="vis.html#grammar"><i class="fa fa-check"></i><b>5.1.2</b> Einleitung zu Wickham’s <em>Grammar of Graphics</em></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="vis.html"><a href="vis.html#vis-elemente"><i class="fa fa-check"></i><b>5.2</b> Grundlegende Elemente von <code>ggplot2</code>-Grafiken</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="vis.html"><a href="vis.html#elemente-eines-ggplot"><i class="fa fa-check"></i><b>5.2.1</b> Elemente eines <code>ggplot</code></a></li>
<li class="chapter" data-level="5.2.2" data-path="vis.html"><a href="vis.html#beispiel-workflow"><i class="fa fa-check"></i><b>5.2.2</b> Beispiel Workflow</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="vis.html"><a href="vis.html#arten-von-datenvisualisierung"><i class="fa fa-check"></i><b>5.3</b> Arten von Datenvisualisierung</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="vis.html"><a href="vis.html#allgemeine-tipps-zum-grafikdesign"><i class="fa fa-check"></i><b>5.3.1</b> Allgemeine Tipps zum Grafikdesign</a></li>
<li class="chapter" data-level="5.3.2" data-path="vis.html"><a href="vis.html#streu--oder-blasendiagramm"><i class="fa fa-check"></i><b>5.3.2</b> Streu- oder Blasendiagramm</a></li>
<li class="chapter" data-level="5.3.3" data-path="vis.html"><a href="vis.html#linienchart"><i class="fa fa-check"></i><b>5.3.3</b> Linienchart</a></li>
<li class="chapter" data-level="5.3.4" data-path="vis.html"><a href="vis.html#histogramme-und-dichteplots"><i class="fa fa-check"></i><b>5.3.4</b> Histogramme und Dichteplots</a></li>
<li class="chapter" data-level="5.3.5" data-path="vis.html"><a href="vis.html#balkendiagramme"><i class="fa fa-check"></i><b>5.3.5</b> Balkendiagramme</a></li>
<li class="chapter" data-level="5.3.6" data-path="vis.html"><a href="vis.html#vis-pie"><i class="fa fa-check"></i><b>5.3.6</b> Kuchendiagramme</a></li>
<li class="chapter" data-level="5.3.7" data-path="vis.html"><a href="vis.html#vis-kinds-summary"><i class="fa fa-check"></i><b>5.3.7</b> Zusammenfassung</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="vis.html"><a href="vis.html#vis-adv"><i class="fa fa-check"></i><b>5.4</b> Beispiele aus der Praxis und fortgeschrittene Themen</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="vis.html"><a href="vis.html#regressionsgerade"><i class="fa fa-check"></i><b>5.4.1</b> Regressionsgerade</a></li>
<li class="chapter" data-level="5.4.2" data-path="vis.html"><a href="vis.html#vis-viele-plots"><i class="fa fa-check"></i><b>5.4.2</b> Mehrere Plots in einer Abbildung</a></li>
<li class="chapter" data-level="5.4.3" data-path="vis.html"><a href="vis.html#mehr-zu-den-skalen-ggplot2expansion-und-skalentransformation"><i class="fa fa-check"></i><b>5.4.3</b> Mehr zu den Skalen: <code>ggplot2::expansion()</code> und Skalentransformation</a></li>
<li class="chapter" data-level="5.4.4" data-path="vis.html"><a href="vis.html#mehr-zur-farbauswahl"><i class="fa fa-check"></i><b>5.4.4</b> Mehr zur Farbauswahl</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="vis.html"><a href="vis.html#vis-fehler"><i class="fa fa-check"></i><b>5.5</b> Typische Fehler in der Datenvisualisierung vermeiden</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="vis.html"><a href="vis.html#clutterplots-und-ihre-tranformation-zum-beschrifteten-streudiagramm"><i class="fa fa-check"></i><b>5.5.1</b> Clutterplots und ihre Tranformation zum beschrifteten Streudiagramm</a></li>
<li class="chapter" data-level="5.5.2" data-path="vis.html"><a href="vis.html#ein-unbalancierter-plot"><i class="fa fa-check"></i><b>5.5.2</b> Ein ‘unbalancierter’ Plot</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="vis.html"><a href="vis.html#vis-lies"><i class="fa fa-check"></i><b>5.6</b> Lügen mit grafischer Statistik</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="vis.html"><a href="vis.html#klassiker-1-kontraintuitiver-nullpunkt"><i class="fa fa-check"></i><b>5.6.1</b> Klassiker 1: Kontraintuitiver ‘Nullpunkt’</a></li>
<li class="chapter" data-level="5.6.2" data-path="vis.html"><a href="vis.html#klassiker-2-geschickt-gewählter-zeitraum-und-clever-gewählte-achsenabschnitte"><i class="fa fa-check"></i><b>5.6.2</b> Klassiker 2: Geschickt gewählter Zeitraum und clever gewählte Achsenabschnitte</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="vis.html"><a href="vis.html#vis-links"><i class="fa fa-check"></i><b>5.7</b> Links und weiterführende Literatur</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="formalia.html"><a href="formalia.html"><i class="fa fa-check"></i><b>6</b> Formale Methoden der Sozioökonomie</a>
<ul>
<li class="chapter" data-level="" data-path="data.html"><a href="data.html#verwendete-pakete"><i class="fa fa-check"></i>Verwendete Pakete</a></li>
<li class="chapter" data-level="6.1" data-path="formalia.html"><a href="formalia.html#formalia-wachstum"><i class="fa fa-check"></i><b>6.1</b> Änderungsraten und die Rolle des Logarithmus</a></li>
<li class="chapter" data-level="6.2" data-path="formalia.html"><a href="formalia.html#formalia-diff"><i class="fa fa-check"></i><b>6.2</b> Grundlagen der Differentialrechnung</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="formalia.html"><a href="formalia.html#einleitung-differential--und-integralrechnung"><i class="fa fa-check"></i><b>6.2.1</b> Einleitung: Differential- und Integralrechnung</a></li>
<li class="chapter" data-level="6.2.2" data-path="formalia.html"><a href="formalia.html#wiederholung-ableitungsregeln"><i class="fa fa-check"></i><b>6.2.2</b> Wiederholung: Ableitungsregeln</a></li>
<li class="chapter" data-level="6.2.3" data-path="formalia.html"><a href="formalia.html#ableitungen-in-r"><i class="fa fa-check"></i><b>6.2.3</b> Ableitungen in R</a></li>
<li class="chapter" data-level="6.2.4" data-path="formalia.html"><a href="formalia.html#maximierung-die-analytische-perspektive"><i class="fa fa-check"></i><b>6.2.4</b> Maximierung: die analytische Perspektive</a></li>
<li class="chapter" data-level="6.2.5" data-path="formalia.html"><a href="formalia.html#maximierung-die-algorithmische-perspektive"><i class="fa fa-check"></i><b>6.2.5</b> Maximierung: die algorithmische Perspektive</a></li>
<li class="chapter" data-level="6.2.6" data-path="formalia.html"><a href="formalia.html#subsec:keynes-expl"><i class="fa fa-check"></i><b>6.2.6</b> Anwendungsbeispiel</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="formalia.html"><a href="formalia.html#formalia-linalg"><i class="fa fa-check"></i><b>6.3</b> Lineare Algebra</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="formalia.html"><a href="formalia.html#linalg-expls"><i class="fa fa-check"></i><b>6.3.1</b> Einführungsbeispiele</a></li>
<li class="chapter" data-level="6.3.2" data-path="formalia.html"><a href="formalia.html#einführung-von-matrizen"><i class="fa fa-check"></i><b>6.3.2</b> Einführung von Matrizen</a></li>
<li class="chapter" data-level="6.3.3" data-path="formalia.html"><a href="formalia.html#grundregeln-der-matrizenalgebra"><i class="fa fa-check"></i><b>6.3.3</b> Grundregeln der Matrizenalgebra</a></li>
<li class="chapter" data-level="6.3.4" data-path="formalia.html"><a href="formalia.html#anwendungsbeispiel-1-das-einfache-keynesianische-modell"><i class="fa fa-check"></i><b>6.3.4</b> Anwendungsbeispiel 1: Das einfache Keynesianische Modell</a></li>
<li class="chapter" data-level="6.3.5" data-path="formalia.html"><a href="formalia.html#anwendungsbeispiel-2-ols-regression"><i class="fa fa-check"></i><b>6.3.5</b> Anwendungsbeispiel 2: OLS-Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="formalia.html"><a href="formalia.html#ols-deriv"><i class="fa fa-check"></i><b>6.3.6</b> Optional: Herleitung des OLS-Schätzers</a></li>
<li class="chapter" data-level="6.3.7" data-path="formalia.html"><a href="formalia.html#weiterführende-literatur"><i class="fa fa-check"></i><b>6.3.7</b> Weiterführende Literatur</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="formalia.html"><a href="formalia.html#formalia-dist"><i class="fa fa-check"></i><b>6.4</b> Analyse von Verteilungen</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="formalia.html"><a href="formalia.html#vert-begriff"><i class="fa fa-check"></i><b>6.4.1</b> Theoretische und empirische Verteilungen</a></li>
<li class="chapter" data-level="6.4.2" data-path="formalia.html"><a href="formalia.html#vert-kennzahlen"><i class="fa fa-check"></i><b>6.4.2</b> Kennzahlen zur Beschreibung empirischer Verteilungen</a></li>
<li class="chapter" data-level="6.4.3" data-path="formalia.html"><a href="formalia.html#vert-grafik"><i class="fa fa-check"></i><b>6.4.3</b> Grafische Komplemente zu klassischen Kennzahlen</a></li>
<li class="chapter" data-level="6.4.4" data-path="formalia.html"><a href="formalia.html#vert-bemerkungen"><i class="fa fa-check"></i><b>6.4.4</b> Abschließende Bemerkungen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stat-stoch.html"><a href="stat-stoch.html"><i class="fa fa-check"></i><b>7</b> Grundlagen der Wahrscheinlichkeitstheorie</a>
<ul>
<li class="chapter" data-level="" data-path="data.html"><a href="data.html#verwendete-pakete"><i class="fa fa-check"></i>Verwendete Pakete</a></li>
<li class="chapter" data-level="7.1" data-path="stat-stoch.html"><a href="stat-stoch.html#einleitung-wahrscheinlichkeitstheorie-und-statistik"><i class="fa fa-check"></i><b>7.1</b> Einleitung: Wahrscheinlichkeitstheorie und Statistik</a></li>
<li class="chapter" data-level="7.2" data-path="stat-stoch.html"><a href="stat-stoch.html#grundbegriffe-der-wahrscheinlichkeitstheorie"><i class="fa fa-check"></i><b>7.2</b> Grundbegriffe der Wahrscheinlichkeitstheorie</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="stat-stoch.html"><a href="stat-stoch.html#wahrscheinlichkeitstheoretische-modelle"><i class="fa fa-check"></i><b>7.2.1</b> Wahrscheinlichkeitstheoretische Modelle</a></li>
<li class="chapter" data-level="7.2.2" data-path="stat-stoch.html"><a href="stat-stoch.html#stochastische-unabhängigkeit"><i class="fa fa-check"></i><b>7.2.2</b> Stochastische Unabhängigkeit</a></li>
<li class="chapter" data-level="7.2.3" data-path="stat-stoch.html"><a href="stat-stoch.html#bedingte-wahrscheinlichkeiten"><i class="fa fa-check"></i><b>7.2.3</b> Bedingte Wahrscheinlichkeiten</a></li>
<li class="chapter" data-level="7.2.4" data-path="stat-stoch.html"><a href="stat-stoch.html#der-satz-von-bayes"><i class="fa fa-check"></i><b>7.2.4</b> Der Satz von Bayes</a></li>
<li class="chapter" data-level="7.2.5" data-path="stat-stoch.html"><a href="stat-stoch.html#das-gesetz-der-total-wahrscheinlichkeiten"><i class="fa fa-check"></i><b>7.2.5</b> Das Gesetz der total Wahrscheinlichkeiten</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="stat-stoch.html"><a href="stat-stoch.html#diskrete-wahrscheinlichkeitsmodelle"><i class="fa fa-check"></i><b>7.3</b> Diskrete Wahrscheinlichkeitsmodelle</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="stat-stoch.html"><a href="stat-stoch.html#diskrete-zufallsvariablen"><i class="fa fa-check"></i><b>7.3.1</b> Diskrete Zufallsvariablen</a></li>
<li class="chapter" data-level="7.3.2" data-path="stat-stoch.html"><a href="stat-stoch.html#beispiel-die-binomial-verteilung"><i class="fa fa-check"></i><b>7.3.2</b> Beispiel: die Binomial-Verteilung</a></li>
<li class="chapter" data-level="7.3.3" data-path="stat-stoch.html"><a href="stat-stoch.html#beispiel-die-poisson-verteilung"><i class="fa fa-check"></i><b>7.3.3</b> Beispiel: die Poisson-Verteilung</a></li>
<li class="chapter" data-level="7.3.4" data-path="stat-stoch.html"><a href="stat-stoch.html#hinweise-zu-diskreten-wahrscheinlichkeitsverteilungen"><i class="fa fa-check"></i><b>7.3.4</b> Hinweise zu diskreten Wahrscheinlichkeitsverteilungen</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="stat-stoch.html"><a href="stat-stoch.html#stetige-wahrscheinlichkeitsmodelle"><i class="fa fa-check"></i><b>7.4</b> Stetige Wahrscheinlichkeitsmodelle</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="stat-stoch.html"><a href="stat-stoch.html#stetige-zv"><i class="fa fa-check"></i><b>7.4.1</b> Stetige ZV</a></li>
<li class="chapter" data-level="7.4.2" data-path="stat-stoch.html"><a href="stat-stoch.html#beispiel-die-uniformverteilung"><i class="fa fa-check"></i><b>7.4.2</b> Beispiel: die Uniformverteilung</a></li>
<li class="chapter" data-level="7.4.3" data-path="stat-stoch.html"><a href="stat-stoch.html#beispiel-die-normalverteilung"><i class="fa fa-check"></i><b>7.4.3</b> Beispiel: die Normalverteilung</a></li>
<li class="chapter" data-level="7.4.4" data-path="stat-stoch.html"><a href="stat-stoch.html#beispiel-die-exponentialverteilung"><i class="fa fa-check"></i><b>7.4.4</b> Beispiel: die Exponentialverteilung</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="stat-stoch.html"><a href="stat-stoch.html#zusammenfassung-wahrscheinlichkeitsmodelle-für-einzelne-zv"><i class="fa fa-check"></i><b>7.5</b> Zusammenfassung Wahrscheinlichkeitsmodelle für einzelne ZV</a></li>
<li class="chapter" data-level="7.6" data-path="stat-stoch.html"><a href="stat-stoch.html#analyse-mehrerer-zufallsvariablen-gemeinsame-und-marginale-verteilungen"><i class="fa fa-check"></i><b>7.6</b> Analyse mehrerer Zufallsvariablen: gemeinsame und marginale Verteilungen</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="stat-stoch.html"><a href="stat-stoch.html#gemeinsame-verteilungen-für-diskrete-zv"><i class="fa fa-check"></i><b>7.6.1</b> Gemeinsame Verteilungen für diskrete ZV</a></li>
<li class="chapter" data-level="7.6.2" data-path="stat-stoch.html"><a href="stat-stoch.html#gemeinsame-verteilungen-für-stetige-zv"><i class="fa fa-check"></i><b>7.6.2</b> Gemeinsame Verteilungen für stetige ZV</a></li>
<li class="chapter" data-level="7.6.3" data-path="stat-stoch.html"><a href="stat-stoch.html#gemeinsame-kumulative-verteilungen"><i class="fa fa-check"></i><b>7.6.3</b> Gemeinsame kumulative Verteilungen</a></li>
<li class="chapter" data-level="7.6.4" data-path="stat-stoch.html"><a href="stat-stoch.html#marginale-verteilungen"><i class="fa fa-check"></i><b>7.6.4</b> Marginale Verteilungen</a></li>
<li class="chapter" data-level="7.6.5" data-path="stat-stoch.html"><a href="stat-stoch.html#bedingte-verteilungen-und-bedinge-momente"><i class="fa fa-check"></i><b>7.6.5</b> Bedingte Verteilungen und bedinge Momente</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="desk-stat.html"><a href="desk-stat.html"><i class="fa fa-check"></i><b>8</b> Wiederholung: Deskriptive Statistik</a>
<ul>
<li class="chapter" data-level="" data-path="desk-stat.html"><a href="desk-stat.html#verwendete-pakete-und-datensätze"><i class="fa fa-check"></i>Verwendete Pakete und Datensätze</a></li>
<li class="chapter" data-level="8.1" data-path="desk-stat.html"><a href="desk-stat.html#kennzahlen-zur-lage-und-streuung-der-daten"><i class="fa fa-check"></i><b>8.1</b> Kennzahlen zur Lage und Streuung der Daten</a></li>
<li class="chapter" data-level="8.2" data-path="desk-stat.html"><a href="desk-stat.html#korrelationsmaße"><i class="fa fa-check"></i><b>8.2</b> Korrelationsmaße</a></li>
<li class="chapter" data-level="8.3" data-path="desk-stat.html"><a href="desk-stat.html#descVis"><i class="fa fa-check"></i><b>8.3</b> Hinweise zur quantitativen und visuellen Datenbeschreibung</a></li>
<li class="chapter" data-level="8.4" data-path="desk-stat.html"><a href="desk-stat.html#zusamenfassung"><i class="fa fa-check"></i><b>8.4</b> Zusamenfassung</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="stat-rep.html"><a href="stat-rep.html"><i class="fa fa-check"></i><b>9</b> Wiederholung: Drei Verfahren der schließenden Statistik</a>
<ul>
<li class="chapter" data-level="" data-path="data.html"><a href="data.html#verwendete-pakete"><i class="fa fa-check"></i>Verwendete Pakete</a></li>
<li class="chapter" data-level="9.1" data-path="stat-rep.html"><a href="stat-rep.html#punktschätzung"><i class="fa fa-check"></i><b>9.1</b> Punktschätzung</a></li>
<li class="chapter" data-level="9.2" data-path="stat-rep.html"><a href="stat-rep.html#hypothesentests"><i class="fa fa-check"></i><b>9.2</b> Hypothesentests</a></li>
<li class="chapter" data-level="9.3" data-path="stat-rep.html"><a href="stat-rep.html#berechnung-von-konfidenzintervallen"><i class="fa fa-check"></i><b>9.3</b> Berechnung von Konfidenzintervallen</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="linmodel.html"><a href="linmodel.html"><i class="fa fa-check"></i><b>10</b> Lineare statistische Modelle in R</a>
<ul>
<li class="chapter" data-level="10.1" data-path="linmodel.html"><a href="linmodel.html#einleitung-und-überblick"><i class="fa fa-check"></i><b>10.1</b> Einleitung und Überblick</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="linmodel.html"><a href="linmodel.html#einführung-in-die-lineare-regression"><i class="fa fa-check"></i><b>10.1.1</b> Einführung in die lineare Regression</a></li>
<li class="chapter" data-level="10.1.2" data-path="linmodel.html"><a href="linmodel.html#einführungsbeispiel"><i class="fa fa-check"></i><b>10.1.2</b> Einführungsbeispiel</a></li>
<li class="chapter" data-level="10.1.3" data-path="linmodel.html"><a href="linmodel.html#überblick-über-die-inhalte-des-kapitels"><i class="fa fa-check"></i><b>10.1.3</b> Überblick über die Inhalte des Kapitels</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="linmodel.html"><a href="linmodel.html#lin-grundlagen"><i class="fa fa-check"></i><b>10.2</b> Grundlagen der einfachen linearen Regression</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="linmodel.html"><a href="linmodel.html#grundlegende-begriffe"><i class="fa fa-check"></i><b>10.2.1</b> Grundlegende Begriffe</a></li>
<li class="chapter" data-level="10.2.2" data-path="linmodel.html"><a href="linmodel.html#schätzung-mit-der-kleinste-quadrate-methode"><i class="fa fa-check"></i><b>10.2.2</b> Schätzung mit der Kleinste-Quadrate-Methode</a></li>
<li class="chapter" data-level="10.2.3" data-path="linmodel.html"><a href="linmodel.html#ols-ass"><i class="fa fa-check"></i><b>10.2.3</b> Annahmen für den OLS Schätzer</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="linmodel.html"><a href="linmodel.html#lin-kennzahlen"><i class="fa fa-check"></i><b>10.3</b> Kennzahlen in der linearen Regression</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="linmodel.html"><a href="linmodel.html#erklärte-varianz-und-das-r2"><i class="fa fa-check"></i><b>10.3.1</b> Erklärte Varianz und das <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="10.3.2" data-path="linmodel.html"><a href="linmodel.html#linmodelHypothesentests"><i class="fa fa-check"></i><b>10.3.2</b> Hypothesentests und statistische Signifikanz</a></li>
<li class="chapter" data-level="10.3.3" data-path="linmodel.html"><a href="linmodel.html#konfidenzintervalle-für-die-schätzer"><i class="fa fa-check"></i><b>10.3.3</b> Konfidenzintervalle für die Schätzer</a></li>
<li class="chapter" data-level="10.3.4" data-path="linmodel.html"><a href="linmodel.html#zur-rolle-der-stichprobengröße"><i class="fa fa-check"></i><b>10.3.4</b> Zur Rolle der Stichprobengröße</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="linmodel.html"><a href="linmodel.html#lin-multi"><i class="fa fa-check"></i><b>10.4</b> Multiple lineare Regression</a></li>
<li class="chapter" data-level="10.5" data-path="linmodel.html"><a href="linmodel.html#stat-ablauf"><i class="fa fa-check"></i><b>10.5</b> Zum Ablauf einer Regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="advlin.html"><a href="advlin.html"><i class="fa fa-check"></i><b>11</b> Fortgeschrittene Themen der linearen Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="advlin.html"><a href="advlin.html#annahmen-und-eigenschaften-des-einfachen-ols-modells"><i class="fa fa-check"></i><b>11.1</b> Annahmen und Eigenschaften des einfachen OLS Modells</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="advlin.html"><a href="advlin.html#annahmen-im-matrixschreibweise"><i class="fa fa-check"></i><b>11.1.1</b> Annahmen im Matrixschreibweise</a></li>
<li class="chapter" data-level="11.1.2" data-path="advlin.html"><a href="advlin.html#erwartungstreue-effizienz-und-konsistenz"><i class="fa fa-check"></i><b>11.1.2</b> Erwartungstreue, Effizienz und Konsistenz</a></li>
<li class="chapter" data-level="11.1.3" data-path="advlin.html"><a href="advlin.html#abweichungen-von-den-ols-annahmen"><i class="fa fa-check"></i><b>11.1.3</b> Abweichungen von den OLS Annahmen</a></li>
<li class="chapter" data-level="11.1.4" data-path="advlin.html"><a href="advlin.html#monte-carlo-simulationen-in-r"><i class="fa fa-check"></i><b>11.1.4</b> Monte Carlo Simulationen in R</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="advlin.html"><a href="advlin.html#heteroskedastie"><i class="fa fa-check"></i><b>11.2</b> Heteroskedastie</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="advlin.html"><a href="advlin.html#liegt-heteroskedastie-vor-advlin-hetero-test"><i class="fa fa-check"></i><b>11.2.1</b> Liegt Heteroskedastie vor? {advlin-hetero-test}</a></li>
<li class="chapter" data-level="11.2.2" data-path="advlin.html"><a href="advlin.html#reaktionen-auf-heteroskedastie"><i class="fa fa-check"></i><b>11.2.2</b> Reaktionen auf Heteroskedastie</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="advlin.html"><a href="advlin.html#autokorrelation"><i class="fa fa-check"></i><b>11.3</b> Autokorrelation</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="advlin.html"><a href="advlin.html#folgen-von-autokorrelation"><i class="fa fa-check"></i><b>11.3.1</b> Folgen von Autokorrelation</a></li>
<li class="chapter" data-level="11.3.2" data-path="advlin.html"><a href="advlin.html#testen-auf-autokorrelation"><i class="fa fa-check"></i><b>11.3.2</b> Testen auf Autokorrelation</a></li>
<li class="chapter" data-level="11.3.3" data-path="advlin.html"><a href="advlin.html#reaktionen-auf-autokorrelation"><i class="fa fa-check"></i><b>11.3.3</b> Reaktionen auf Autokorrelation</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="advlin.html"><a href="advlin.html#multikollinearität"><i class="fa fa-check"></i><b>11.4</b> Multikollinearität</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="advlin.html"><a href="advlin.html#folgen-von-multikollinearität"><i class="fa fa-check"></i><b>11.4.1</b> Folgen von Multikollinearität</a></li>
<li class="chapter" data-level="11.4.2" data-path="advlin.html"><a href="advlin.html#testen-auf-multikollinearität"><i class="fa fa-check"></i><b>11.4.2</b> Testen auf Multikollinearität</a></li>
<li class="chapter" data-level="11.4.3" data-path="advlin.html"><a href="advlin.html#reaktionen-auf-multikollinearität"><i class="fa fa-check"></i><b>11.4.3</b> Reaktionen auf Multikollinearität</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="advlin.html"><a href="advlin.html#advlin-omitted-var"><i class="fa fa-check"></i><b>11.5</b> Vergessene Variablen</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="advlin.html"><a href="advlin.html#folgen-vergessener-variablen"><i class="fa fa-check"></i><b>11.5.1</b> Folgen vergessener Variablen</a></li>
<li class="chapter" data-level="11.5.2" data-path="advlin.html"><a href="advlin.html#testen-auf-vergessene-variablen"><i class="fa fa-check"></i><b>11.5.2</b> Testen auf vergessene Variablen</a></li>
<li class="chapter" data-level="11.5.3" data-path="advlin.html"><a href="advlin.html#reaktion-auf-vergessene-variablen"><i class="fa fa-check"></i><b>11.5.3</b> Reaktion auf vergessene Variablen</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="advlin.html"><a href="advlin.html#falsche-funktionale-form"><i class="fa fa-check"></i><b>11.6</b> Falsche funktionale Form</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="advlin.html"><a href="advlin.html#folgen-einer-falschen-funktionalen-form"><i class="fa fa-check"></i><b>11.6.1</b> Folgen einer falschen funktionalen Form</a></li>
<li class="chapter" data-level="11.6.2" data-path="advlin.html"><a href="advlin.html#testen-auf-die-richtige-funktionale-form"><i class="fa fa-check"></i><b>11.6.2</b> Testen auf die richtige funktionale Form</a></li>
<li class="chapter" data-level="11.6.3" data-path="advlin.html"><a href="advlin.html#wahl-der-funktionalen-form"><i class="fa fa-check"></i><b>11.6.3</b> Wahl der funktionalen Form</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="advlin.html"><a href="advlin.html#normalverteilung-der-fehlerterme"><i class="fa fa-check"></i><b>11.7</b> Normalverteilung der Fehlerterme</a></li>
<li class="chapter" data-level="11.8" data-path="advlin.html"><a href="advlin.html#weitere-fehlerquellen-systematische-messfehler-selbstselektion-und-simulatanität"><i class="fa fa-check"></i><b>11.8</b> Weitere Fehlerquellen: Systematische Messfehler, Selbstselektion und Simulatanität</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="advlin.html"><a href="advlin.html#messfehler"><i class="fa fa-check"></i><b>11.8.1</b> Messfehler</a></li>
<li class="chapter" data-level="11.8.2" data-path="advlin.html"><a href="advlin.html#selbstselektion"><i class="fa fa-check"></i><b>11.8.2</b> Selbstselektion</a></li>
<li class="chapter" data-level="11.8.3" data-path="advlin.html"><a href="advlin.html#simulatanität"><i class="fa fa-check"></i><b>11.8.3</b> Simulatanität</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="advlin.html"><a href="advlin.html#anhang-übersicht-über-die-testverfahren"><i class="fa fa-check"></i><b>11.9</b> Anhang: Übersicht über die Testverfahren</a></li>
<li class="chapter" data-level="11.10" data-path="advlin.html"><a href="advlin.html#advlin-proofs"><i class="fa fa-check"></i><b>11.10</b> Anhang: Relevante Theoreme und ihre mathematischen Beweise</a>
<ul>
<li class="chapter" data-level="11.10.1" data-path="advlin.html"><a href="advlin.html#theoreme"><i class="fa fa-check"></i><b>11.10.1</b> Theoreme</a></li>
<li class="chapter" data-level="11.10.2" data-path="advlin.html"><a href="advlin.html#beweise"><i class="fa fa-check"></i><b>11.10.2</b> Beweise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="nonlin.html"><a href="nonlin.html"><i class="fa fa-check"></i><b>12</b> Ausgewählte nichtlineare Schätzverfahren</a>
<ul>
<li class="chapter" data-level="12.1" data-path="nonlin.html"><a href="nonlin.html#logit"><i class="fa fa-check"></i><b>12.1</b> Binäre abhängige Variablen: Logit- und Probit-Modelle</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="nonlin.html"><a href="nonlin.html#warum-nicht-ols"><i class="fa fa-check"></i><b>12.1.1</b> Warum nicht OLS?</a></li>
<li class="chapter" data-level="12.1.2" data-path="nonlin.html"><a href="nonlin.html#logit-und-probit-theoretische-grundidee"><i class="fa fa-check"></i><b>12.1.2</b> Logit und Probit: theoretische Grundidee</a></li>
<li class="chapter" data-level="12.1.3" data-path="nonlin.html"><a href="nonlin.html#logit-und-probit-implementierung-in-r"><i class="fa fa-check"></i><b>12.1.3</b> Logit und Probit: Implementierung in R</a></li>
<li class="chapter" data-level="12.1.4" data-path="nonlin.html"><a href="nonlin.html#logit-und-probit-interpretation-der-ergebnisse"><i class="fa fa-check"></i><b>12.1.4</b> Logit und Probit: Interpretation der Ergebnisse</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="nonlin.html"><a href="nonlin.html#abschließende-anmerkungen"><i class="fa fa-check"></i><b>12.2</b> Abschließende Anmerkungen</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="outlook.html"><a href="outlook.html"><i class="fa fa-check"></i><b>13</b> Ausblick</a></li>
<li class="chapter" data-level="14" data-path="markdown.html"><a href="markdown.html"><i class="fa fa-check"></i><b>14</b> Eine kurze Einführung in R Markdown</a></li>
<li class="chapter" data-level="15" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>15</b> Eine kurze Einführung in die Versionskontrolle mit Git</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R für die sozio-ökonomische Forschung</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="stat-stoch" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Grundlagen der Wahrscheinlichkeitstheorie</h1>
<p>In diesem Kapitel werden einige grundlegende Konzepte der
Wahrscheinlichkeitstheorie eingeführt, bzw. wiederholt.
Die zentralen Themen auf die wir uns fokussieren werden sind dabei:</p>
<ul>
<li>Der Zusammenhang zwischen Wahrscheinlichkeitstheorie und Statistik</li>
<li>Grundbegriffe der Wahrscheinlichkeitstheorie und Statistik</li>
<li>Zufallsvariablen</li>
<li>Diskrete und stetige Verteilungen einzelner und mehrerer Zufallsvariablen</li>
</ul>
<p>Grundkonzepte der deskriptiven und schließenden Statistik (insb.
Parameterschätzung, Hypothesentests und die Berechnung von Konfidenzintervallen)
werden in den beiden folgenden Kapiteln zur deskriptiven und schließenden
Statistik (siehe Kapitel <a href="desk-stat.html#desk-stat">8</a> und Kapitel <a href="stat-rep.html#stat-rep">9</a> behandelt.</p>
<div id="verwendete-pakete" class="section level2 unnumbered">
<h2>Verwendete Pakete</h2>
<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb678-1"><a href="stat-stoch.html#cb678-1" aria-hidden="true"></a><span class="kw">library</span>(here)</span>
<span id="cb678-2"><a href="stat-stoch.html#cb678-2" aria-hidden="true"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb678-3"><a href="stat-stoch.html#cb678-3" aria-hidden="true"></a><span class="kw">library</span>(ggpubr)</span>
<span id="cb678-4"><a href="stat-stoch.html#cb678-4" aria-hidden="true"></a><span class="kw">library</span>(latex2exp)</span>
<span id="cb678-5"><a href="stat-stoch.html#cb678-5" aria-hidden="true"></a><span class="kw">library</span>(data.table)</span>
<span id="cb678-6"><a href="stat-stoch.html#cb678-6" aria-hidden="true"></a><span class="kw">library</span>(viridis)</span></code></pre></div>
</div>
<div id="einleitung-wahrscheinlichkeitstheorie-und-statistik" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Einleitung: Wahrscheinlichkeitstheorie und Statistik</h2>
<p>Statistik und Wahrscheinlichkeitstheorie sind untrennbar miteinander verbunden.
In der Wahrscheinlichkeitstheorie beschäftigt man sich mit Modellen von
Zufallsprozessen, also Prozessen, deren Ausgang nicht exakt vorhersehbar ist.
Häufig spricht man von <em>Zufallsexperimenten</em>.</p>
<p>Die Wahrscheinlichkeitstheorie entwickelt dabei Modelle, welche diese
Zufallsexperimente und deren mögliche Ausgänge beschreiben und dabei den
möglichen Ausgängen Wahrscheinlichkeiten zuordnen.
Diese Modelle werden <em>Wahrscheinlichkeitsmodelle</em> genannt.</p>
<p>In der Statistik versuchen wir anhand von beobachteten Daten herauszufinden,
welches Wahrscheinlichkeitsmodell gut geeignet ist, den die Daten generierenden
Prozess (<em>data generating process</em> - DGP) zu beschreiben.
Das ist der Grund warum man für Statistik auch immer Kenntnisse der
Wahrscheinlichkeitstheorie braucht.</p>
<blockquote>
<p>Kurz gesagt: in der Wahrscheinlichkeitstheorie wollen wir mit Hilfe von
Wahrscheinlichkeitsmodellen Daten vorhersagen, in der Statistik mit Hilfe
bekannter Daten Rückschlüsse auf die zugrundeliegenden Wahrscheinlichkeitsmodelle
ziehen.</p>
</blockquote>
</div>
<div id="grundbegriffe-der-wahrscheinlichkeitstheorie" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Grundbegriffe der Wahrscheinlichkeitstheorie</h2>
<div id="wahrscheinlichkeitstheoretische-modelle" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Wahrscheinlichkeitstheoretische Modelle</h3>
<p>Ein wahrscheinlichkeitstheoretisches Modell besteht <em>immer</em> aus den folgenden drei
Komponenten:</p>
<p><strong>Ergebnisraum</strong>: diese Menge <span class="math inline">\(\Omega\)</span> enthält alle möglichen Ergebnisse des
modellierten Zufallsexperiments.
Das einzelne Ergebnis bezeichnen wir mit <span class="math inline">\(\omega\)</span>.</p>
<blockquote>
<p><strong>Beispiel:</strong> Handelt es sich bei dem Zufallsexperiment um das Werfen eines
normalen sechseitigen Würfels, so gilt <span class="math inline">\(\Omega=\{1,2,3,4,5,6\}\)</span>. Wenn der Würfel
gefallen ist, bezeichnen wir die oben liegende Zahl als das Ergebnis <span class="math inline">\(\omega\)</span> des
Würfelwurfs, wobei hier gilt <span class="math inline">\(\omega_1=\)</span> “Der Würfel zeigt 1”, u.s.w.</p>
</blockquote>
<p><strong>Ereignisse:</strong> unter Ereignissen <span class="math inline">\(A, B, C,...\)</span> verstehen wir die Teilmengen
des Ergebnisraums. Ein Ereignis enthält ein oder mehrere Elemente des Ergebnisraums.
Enthält ein Ereignis genau ein Element, sprechen wir von einem <em>Elementarereignis</em>.</p>
<blockquote>
<p><strong>Beispiel:</strong> “Es wird eine gerade Zahl gewürfelt” ist ein mögliches Ereignis
im oben beschriebenen Zufallsexperiment. Das Ereignis - nennen wir es hier <span class="math inline">\(A\)</span> -
tritt ein, wenn ein Würfelwurf mit dem Ergebnis “2”, “4” oder “6” endet. Also:
<span class="math inline">\(A=\{\omega_2, \omega_4, \omega_6\}\)</span>.
Das Ereignis <span class="math inline">\(B\)</span> “Es wird eine 2 gewürfelt” tritt nur ein, wenn das Ergebnis des
Würfelwurfs eine 2 ist: <span class="math inline">\(B=\{\omega_2\}\)</span>.
Entsprechend nennen wir es ein <em>Elementarereignis</em>.</p>
</blockquote>
<p>Da es sich bei Ereignissen um Mengen handelt können wir die typischen
mengentheoretischen Konzepte wie ‘Vereinigung’, ‘Differenz’ oder ‘Komplement’ zu
ihrer Beschreibung verwenden. Diese sind in Tablle <a href="stat-stoch.html#tab:Mengentheorie">7.1</a> zusammengefasst:</p>
<table>
<caption><span id="tab:Mengentheorie">Table 7.1: </span> Mengentheoretische Konzepte und ihre Übersetzungen.</caption>
<thead>
<tr class="header">
<th>Konzept</th>
<th>Symbol</th>
<th>Übersetzung</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Schnittmenge</td>
<td><span class="math inline">\(A\cap B\)</span></td>
<td><span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span></td>
</tr>
<tr class="even">
<td>Vereinigung</td>
<td><span class="math inline">\(A\cup B\)</span></td>
<td><span class="math inline">\(A\)</span> und/oder <span class="math inline">\(B\)</span></td>
</tr>
<tr class="odd">
<td>Komplement</td>
<td><span class="math inline">\(A^c\)</span></td>
<td>Nicht <span class="math inline">\(A\)</span></td>
</tr>
<tr class="even">
<td>Differenz</td>
<td><span class="math inline">\(A \setminus B = A\cap B^c\)</span></td>
<td><span class="math inline">\(A\)</span> ohne <span class="math inline">\(B\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Wahrscheinlichkeiten</strong>: jedem <em>Ereignis</em> <span class="math inline">\(A\)</span> wird eine Wahrscheinlichkeit
<span class="math inline">\(\mathbb{P}(A)\)</span> zugeordnet. Wahrscheinlichkeiten können aber nicht beliebige
Zahlen sein. Vielmehr müssen sie im Einklang mit den drei
<em>Axiomen von Kolmogorow</em> stehen:</p>
<ol style="list-style-type: decimal">
<li><p>Für jedes Ereignis <span class="math inline">\(A\)</span> gilt: <span class="math inline">\(0\leq\mathbb{P}(A)\leq1\)</span></p></li>
<li><p>Das sichere Ereignis <span class="math inline">\(\Omega\)</span> umfasst den ganzen Ergebnisraum und es gilt
entsprechend <span class="math inline">\(\mathbb{P}(\Omega)=1\)</span>.</p></li>
<li><p>Es gilt: <span class="math inline">\(\mathbb{P}(A\cup B) = \mathbb{P}(A)+\mathbb{P}(B)\)</span> falls
<span class="math inline">\(A\cap B=\emptyset\)</span>, also wenn sich A und B gegenseitig ausschließen.</p></li>
</ol>
<p>Aus diesen Axiomen lassen sich eine ganze Menge Sätze heraus ableiten, auf die
wir im Folgenden nicht weiter eingehen wollen.
Die Grundidee ist aber, bestimmten Ereignissen von Anfang an bestimmte Wahrscheinlichkeiten
zuzuordnen, und die Wahrscheinlichkeiten für andere Ereignisse dann aus den eben
beschriebenen Regeln abzuleiten.</p>
<p>Je nach Art des Ergebnisraums <span class="math inline">\(\Omega\)</span> unterscheiden wir zwei grundsätzlich
verschiedene Arten von Wahrscheinlichkeitsmodellen:
ist <span class="math inline">\(\Omega\)</span> <strong>abzählbar</strong> handelt es sich um ein
<strong>diskretes Wahrscheinlichkeitsmodell</strong>.
Der Würfelwurf oder ein Münzwurf sind hierfür Beispiele: die Menge der
möglichen Ergebnisse ist hier klar abzählbar.<a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a></p>
<p>Ist <span class="math inline">\(\Omega\)</span> <strong>nicht abzählbar</strong> handelt es sich dagegen um ein
<strong>stetiges Wahrscheinlichkeitsmodell</strong>.
Ein Beispiel hierfür wäre das Fallenlassen von Steinen und die Messung der
Falldauer.
Die einzelnen Ereignisse wären dann die Falldauer und da wir die Zeiten
in immer kleineren Intervallen angeben können, es also allein zwischen
den Messungen “1 Sekunde” und “2 Sekunden” unendlich viele Zwischenschritte
gibt, würde gelten, dass <span class="math inline">\(\Omega=\mathbb{R^+}\)</span>.
Dabei handelt es sich um eine nicht abzählbare Menge.</p>
<p>Welches Modell für den konkreten Anwendungsfall vorzuziehen ist, muss auf Basis
von theoretischen Überlegungen entschieden werden.</p>
</div>
<div id="stochastische-unabhängigkeit" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Stochastische Unabhängigkeit</h3>
<p>Von Interesse ist häufig aus den Wahrscheinlichkeiten für zwei Ereignisse,
<span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span>, die Wahrscheinlichkeit für <span class="math inline">\(A\cap B\)</span>, also die Wahrscheinlichkeit,
dass beide Ereignisse auftreten, zu berechnen.
Leider ist das nur im Spezialfall der <strong>stochastischen Unabhängigkeit</strong> ohne
Probleme möglich.
Stochastische Unabhängigkeit kann immer dann sinnvollerweise angenommen werden,
wenn zwischen den beteiligten Ereignissen kein kausaler Zusammenhang besteht.
In diesem Fall gilt dann:</p>
<p><span class="math display">\[\mathbb{P}(A\cap B) = \mathbb{P}(A)\cdot\mathbb{P}(B)\]</span></p>
<blockquote>
<p><strong>Beispiel für stochastische Unabhängigkeit</strong>:
Es ist plausibel anzunehmen, dass es keinen kausalen Zusammenhang zwischen
zwei aufeinanderfolgenden Münzwürfen gibt. Entsprechend sind die Ereignisse
<span class="math inline">\(A\)</span>: “Zahl im ersten Wurf” und <span class="math inline">\(B\)</span>: “Kopf im zweiten Wurf” stochastisch
unabhängig und <span class="math inline">\(\mathbb{P}(A\cap B)=\mathbb{P}(A)\cdot \mathbb{P}(B)=\frac{1}{4}\)</span>.</p>
</blockquote>
<blockquote>
<p><strong>Beispiel für stochastische Abhängigkeit</strong>:
Ein anderer Fall liegt vor, wenn wir die Ereignisse <span class="math inline">\(C\)</span>:
“Die Summe beider Würfe ist 6” und <span class="math inline">\(D\)</span>: “Der erste Wurf zeigt eine 2.” betrachten.
Hier ist offensichtlich, dass ein kausaler Zusammenhang zwischen den beiden
Würfen und den Ereignissen besteht.
Es gilt:
<span class="math inline">\(\mathbb{P}(C\cap D)=\mathbb{P}(\{2, 4\})=\frac{1}{36}\)</span>.
Würden wir die Wahrscheinlichkeiten einfach multiplizieren erhielten wir
allerdings <span class="math inline">\(\mathbb{P}(C)\cdot \mathbb{P}(D)=\frac{5}{36}\cdot\frac{1}{6}=\frac{5}{216}\)</span>,
wobei <span class="math inline">\(\mathbb{P}(C)=\frac{5}{36}\)</span>.</p>
</blockquote>
</div>
<div id="bedingte-wahrscheinlichkeiten" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Bedingte Wahrscheinlichkeiten</h3>
<p>Ein weiteres wichtiges Konzept ist das der <strong>bedingten Wahrscheinlichkeit</strong>:
die bedingte Wahrscheinlichkeit von <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span>, <span class="math inline">\(\mathbb{P}(A|B)\)</span>,
bezeichnet die Wahrscheinlichkeit für <span class="math inline">\(A\)</span>, wenn wir wissen, dass <span class="math inline">\(B\)</span>
bereits eingetreten ist.</p>
<p>Es gilt dabei:<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a></p>
<p><span class="math display">\[\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\]</span></p>
<blockquote>
<p><strong>Beispiel:</strong> Sei <span class="math inline">\(A\)</span>: “Der Würfel zeigt eine 6” und <span class="math inline">\(B\)</span>:
“Der Würfelwurf zeigt eine gerade Zahl”. Wenn wir bereits wissen, dass <span class="math inline">\(B\)</span>
eingetreten ist, ist <span class="math inline">\(\mathbb{P}(A)\)</span> nicht mehr <span class="math inline">\(\frac{1}{6}\)</span>, weil wir ja
wissen, dass 1, 3 und 5 nicht auftreten können.
Vielmehr gilt <span class="math inline">\(\mathbb{P}(A|B)=\frac{1/6}{1/2}=\frac{1}{3}\)</span>.</p>
</blockquote>
</div>
<div id="der-satz-von-bayes" class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Der Satz von Bayes</h3>
<p>Wenn wir aus der bedingten Wahrscheinlichkeit für <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span> die bedingte
Wahrscheinlichkeit von <span class="math inline">\(B\)</span> gegeben <span class="math inline">\(A\)</span> berechnen wollen, also aus
<span class="math inline">\(\mathbb{P}(A|B)\)</span> den Ausdruck <span class="math inline">\(\mathbb{P}(B|A)\)</span> ableiten möchten, dann müssen
wir den <strong>Satz von Bayes</strong> anwenden - es gilt nämlich leider nicht
notwendigerweise <span class="math inline">\(\mathbb{P}(A|B)=\mathbb{P}(B|A)\)</span>.
Vielmehr gilt nach dem <em>Satz von Bayes</em>:</p>
<p><span class="math display">\[\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}=\frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}\]</span></p>
<blockquote>
<p><strong>Beispiel für die Anwendung von Bayes’ Theorem</strong>:
Nehmen wir an, Claudius fährt an 70% der Tage mit dem Fahrrad in die
Universität.
Immer wenn Claudius mit dem Fahrrad fährt, ist er zu 80% pünktlich.
Insgesamt ist er an 60% der Tage pünktlich.
Wenn er nun heute pünktlich gekommen ist, wie hoch ist dann die
Wahrscheinlichkeit, dass er mit dem Fahrrad gekommen ist?
Um diese Frage zu beantworten können wir den Satz von Bayes verwenden.
Sei <span class="math inline">\(A:\)</span> “Claudius kommt mit dem Fahrrad” und <span class="math inline">\(B:\)</span> “Claudius ist pünklich”.
Dann gilt in jedem Falle <span class="math inline">\(\mathbb{P}(A)=0.7\)</span>, <span class="math inline">\(\mathbb{P}(B)=0.6\)</span> sowie
<span class="math inline">\(\mathbb{P}(B|A)=0.8\)</span>.
Wir sind interessiert an <span class="math inline">\(\mathbb{P}(A|B)\)</span>, also der Wahrscheinlichkeit, dass Claudius mit dem
Fahrrad gefahren ist, wenn er pünktlich war.
Das können wir nun mit der oben beschriebene Formel herausfinden:</p>
</blockquote>
<p><span class="math display">\[\mathbb{P}(A|B)=\frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}=\frac{0.8\cdot 0.7}{0.6}\approx 0.93\]</span></p>
<blockquote>
<p>Die Wahrscheinlichkeit, dass Claudius mit dem Fahrrad gekommen ist beträgt
also ca. 93 Prozent!</p>
</blockquote>
<blockquote>
<p><strong>Herleitung von Bayes’ Theorem aus den Formeln für bedingte Wahrscheinlichkeiten</strong>
Wir können Bayes’ Theorem aus den oben beschriebenen Formeln für bedingte
Wahrscheinlichkeiten recht einfach herleiten.
Wir wissen, dass</p>
</blockquote>
<p><span class="math display">\[\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\]</span></p>
<blockquote>
<p>Für <span class="math inline">\(\mathbb{P}(A\cap B)\)</span> im Zähler können wir äquivalent <span class="math inline">\(\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}\cdot \mathbb{P}(A)\)</span> schreiben.
Daraus ergibt sich für die Formel oben:</p>
</blockquote>
<p><span class="math display">\[\mathbb{P}(A|B)=\frac{\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}\cdot \mathbb{P}(A)}{\mathbb{P}(B)}\]</span></p>
<blockquote>
<p>Gleichzeitig gilt aber auch:</p>
</blockquote>
<p><span class="math display">\[\mathbb{P}(B|A)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}\]</span></p>
<blockquote>
<p>Das können wir nun im Zähler ersetzen und erhalten so den Satz von Bayes:</p>
</blockquote>
<p><span class="math display">\[\mathbb{P}(A|B)=\frac{\mathbb{P}(B|A)\cdot \mathbb{P}(A)}{\mathbb{P}(B)}\]</span></p>
</div>
<div id="das-gesetz-der-total-wahrscheinlichkeiten" class="section level3" number="7.2.5">
<h3><span class="header-section-number">7.2.5</span> Das Gesetz der total Wahrscheinlichkeiten</h3>
<p>Wenn wir die Wahrscheinlichkeiten für mehrstufige Zufallsexperimente berechnen
wollen müssen wir oft Wahrscheinlichkeiten von verschiedenen Ebenen
“aggregieren”. Das machen wir mit dem <strong>Gesetz der totalen Wahrscheinlichkeit</strong>,
das in Beweisen im Bereich der Stochastik sehr häufig verwendet wird.
Formal besagt das <em>Gesetz der totalen Wahrscheinlichkeit</em> folgendes:
seien <span class="math inline">\(A_1,...,A_k\)</span> Ergeignisse, die sich nicht überschneiden und gemeinsam
den kompletten Ereignisraum <span class="math inline">\(\Omega\)</span> abdecken, dann gilt:</p>
<p><span class="math display">\[\mathbb{P}(B)=\sum_{i=1}^k\mathbb{P}(B|A_k)\mathbb{P}(A_k)\]</span></p>
<p>Das sieht natürlich erst einmal sperrig aus, wie so oft ist es aber eigentlich
ganz einfach. Das folgende Beispiel illustriert das.</p>
<blockquote>
<p><strong>Beispiel für die Anwendung vom Gesetz der totalen Wahrscheinlichkeit</strong>:
Wir haben eine Urne mit drei weißen und sieben schwarzen Kugeln.
Wir ziehen zweimal eine Kugel ohne sie dabei zurückzulegen - wir haben es also
mit einem zweistufigen Zufallsexperiment zu tun.
Wie hoch ist nun die Wahrscheinlichkeit, dass wir genau eine schwarze Kugel
gezogen haben?
Sei <span class="math inline">\(B:\)</span> “Eine schwarze Kugel wird gezogen” und
$A: $ “Eine weiße Kugel wird gezogen”.
Wir addieren nun die Wahrscheinlichkeiten für aller Ergebnisse, die in Kombination
zu unserem Gesamtereignis führen, also die Wahrscheinlichkeit erst eine weiße
und dann eine schwarze und die Wahrscheinlichkeit erst eine schwarze
und dann eine weiße Kugel zu ziehen:</p>
</blockquote>
<p><span class="math display">\[\mathbb{P}(B=1)=\frac{7}{9}\cdot\frac{3}{10} + \frac{3}{9}\cdot\frac{7}{10}=\frac{42}{90}\]</span></p>
<blockquote>
<p>wobei <span class="math inline">\(\mathbb{P}(B|A)=\frac{7}{9}\cdot\frac{3}{10}\)</span> und <span class="math inline">\(\mathbb{P}(A|B)=\frac{3}{9}\cdot\frac{7}{10}\)</span>. Die Wahrscheinlichkeit genau
eine schwarze Kugel zu ziehen liegt also bei ca. <span class="math inline">\(45.5\)</span> Prozent.</p>
</blockquote>
</div>
</div>
<div id="diskrete-wahrscheinlichkeitsmodelle" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Diskrete Wahrscheinlichkeitsmodelle</h2>
<p>Wenn wir die Wahrscheinlichkeit für das Eintreten eines Ereignisses <span class="math inline">\(A\)</span>
erfahren möchten können wir im Falle eines diskreten Ergebnisraums einfach
die Eintrittswahrscheinlichkeiten für alle Ergebnisse, die zu <span class="math inline">\(A\)</span> gehören,
aufsummieren:</p>
<p><span class="math display">\[ \mathbb{P}(A)=\sum_{\omega\in A} \mathbb{P}(\{\omega\})\]</span></p>
<blockquote>
<p><strong>Beispiel:</strong> Beim Werfen eines sechseitigen Würfels ist die Wahrscheinlichkeit
für das Ereignis “Es wird eine gerade Zahl gewürfelt”: <span class="math inline">\(\mathbb{P}(2)+\mathbb{P}(4)+\mathbb{P}(6)=\frac{1}{6}+\frac{1}{6}+\frac{1}{6}=\frac{1}{2}\)</span>.</p>
</blockquote>
<div id="diskrete-zufallsvariablen" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Diskrete Zufallsvariablen</h3>
<p>Bei Zufallsvariablen (ZV) handelt es sich um besondere <em>Funktionen</em>.
Die Definitionsmenge einer Zufallsvariable ist immer der zurgundeliegende
Ergebnisraum <span class="math inline">\(\Omega\)</span>, die Zielmenge ist i.d.R. <span class="math inline">\(\mathbb{R}\)</span>, sodass gilt:</p>
<p><span class="math display">\[X:\Omega\rightarrow\mathbb{R}, \omega \mapsto X(\omega)\]</span></p>
<p>Im Kontext von ZV sprechen wir häufig nicht von dem zugrundeliegenden
Ergebnisraum <span class="math inline">\(\Omega\)</span>, sondern - inhaltlich äquivalent - vom <em>Wertebereich von X</em>,
bezeichnet als <span class="math inline">\(W_X\)</span>.
Produkte und Summen von ZV sind selbst wieder Zufallsvariablen.
Man addiert bzw. multipliziert ZV indem man ihre Werte addiert bzw. mutlipliziert.</p>
<p>In der Regel bezeichnen wir Zufallsvariablen mit Großbuchstaben und die
konkrete Realisation einer ZV mit einem Kleinbuchstaben, sodass
<span class="math inline">\(\mathbb{P}(X=x)\)</span> die Wahrscheinlichkeit angibt, dass die ZV <span class="math inline">\(X\)</span> den konkreten
Wert <span class="math inline">\(x\)</span> annimmt. Bei <span class="math inline">\(x\)</span> sprechen wir von einer <em>Realisierung</em> der ZV <span class="math inline">\(X\)</span>.
Wir nehmen für die weitere Notation an, dass <span class="math inline">\(W_X=\{x_1, x_2,...,x_K\}\)</span> und
bezeichnen das einzelne Element mit <span class="math inline">\(x_k\)</span> mit <span class="math inline">\(1\leq k\leq K\)</span>.</p>
<p>Dies bedeutet streng genommen, dass die ZV selbst nicht als zufällig definiert wird.
Zufällig ist nur der Input <span class="math inline">\(\omega\)</span> der entsprechenden Funktion
<span class="math inline">\(X: \Omega\rightarrow X(\omega)\)</span>, also z.B. ein Würfelwurf.
Der funktionale Zusammenhang zwischen Funktionswert <span class="math inline">\(X(\omega)\)</span>
und dem Input <span class="math inline">\(\omega\)</span> ist hingegen eindeutig und deterministisch.</p>
<p>Das impliziert, dass wenn ein Zufallsexperiment zweimal das
gleiche Ergebnis <span class="math inline">\(\omega\)</span> hat, auch der Wert <span class="math inline">\(X(\omega)\)</span> der gleiche ist.</p>
<p>Das mag im Moment ein wenig nach ‘Pfennigfuchserei’ aussehen, die Unterscheidung
zwischen dem nicht-zufälligem funtionalen Zusammenhang, aber einem zufälligen
Input bei ZV ist wichtig, um den Sinn in vielen fortgeschrittenen Beiträgen im
Bereich der Ökonometrie zu sehen.</p>
<p>Im Falle von diskreten ZV können wir eine Liste erstellen, die für alle möglichen
Werte <span class="math inline">\(x_k\in W_X\)</span> die jeweilige Wahrscheinlichkeit <span class="math inline">\(\mathbb{P}(X=x_k)\)</span> angibt.<a href="#fn65" class="footnote-ref" id="fnref65"><sup>65</sup></a>
Diese Liste nennen wir
<strong>Wahrscheinlichkeitsverteilung</strong> von <span class="math inline">\(X\)</span>
und sie wird häufig visuell dargestellt.
Um diese Liste zu erstellen verwenden wir die zu <span class="math inline">\(X\)</span> gehörende
<strong>Wahrscheinlichkeitsfunktion</strong> (<em>Probability Mass Function</em>, PMF), <span class="math inline">\(p(x_k)\)</span>,
die uns für jedes Ergebnis die zugehörige Wahrscheinlichkeit gibt:<a href="#fn66" class="footnote-ref" id="fnref66"><sup>66</sup></a></p>
<p><span class="math display">\[p(x_k)=\mathbb{P}(X=x_k)\]</span></p>
<p>Ebenfalls häufig verwendet wird die <strong>kumulierte Wahrscheinlichkeitsfunktion</strong>
(<em>cumulative distribution function</em>, CDF):</p>
<p><span class="math display">\[F_X(a)=\mathbb{P}(X\leq x_k)\]</span></p>
<p>Die CDF einer diskrete ZV <span class="math inline">\(X\)</span> gibt also die Wahrscheinlichkeit an, dass <span class="math inline">\(X\)</span> sich
als ein Wert kleiner gleich einem Schwellenwert realisiert. Daher auch der
Name: sie kumuliert die Eintrittswahrscheinlichkeiten aller Events zwischen
<span class="math inline">\(-\infty\)</span> und <span class="math inline">\(a\)</span>.
Für eine solche Funktion gilt wie für die PMF, dass <span class="math inline">\(0\leq F_X(a)\leq1\)</span>.
Zudem handelt es sich bei der CDF um eine wachsende Funktion (<span class="math inline">\(F_X(a)\leq F(b) \leftrightarrow a\leq b\)</span>) und es gilt, dass <span class="math inline">\(\lim_{a\rightarrow \infty}F_X(a)=1\)</span>
sowie <span class="math inline">\(\lim_{a\rightarrow -\infty}F_X(a)=0\)</span>, d.h. für sehr große Werte von <span class="math inline">\(a\)</span>
geht <span class="math inline">\(F_X\)</span> gegen 1 und für sehr kleine Werte gegen 0.</p>
<p>Wenn wir eine ZV analysieren tun wir dies in der Regel durch eine Analyse ihrer
Wahrscheinlichkeitsverteilung.
Zur genaueren Beschreibung einer ZV wird entsprechend häufig einfach die
Wahrscheinlichkeitsfunktion angegeben.</p>
<p>Im Folgenden wollen wir einige häufig auftretende Wahrscheinlichkeitsverteilungen
kurz einführen
Am Ende des Abschnitts findet sich dann ein tabellarischer Überblick.
Doch vorher wollen wir uns noch mit den wichtigsten <strong>Kennzahlen einer Verteilung</strong>
vertraut machen.
Denn wie Sie sich vorstellen können sind Wahrscheinlichkeitsverteilungen als
Listen, die alle möglichen Realisierungen einer ZV enthalten ziemlich umständlich
zu handhaben.
Daher beschreiben wir Wahrscheinlichkeitsverteilungen nicht indem wir eine Liste
beschreiben, sondern indem wir bestimmte Kennzahlen zu ihrer Beschreibung
verwenden.
Die wichtigsten Kennzahlen einer ZV <span class="math inline">\(X\)</span> sind der <strong>Erwartungswert</strong>
<span class="math inline">\(\mathbb{E}(x)\)</span> als <em>Lageparameter</em> und die <strong>Standardabweichung</strong>
<span class="math inline">\(\sigma(X)\)</span> als <em>Streuungsmaß</em>.</p>
<p>Der Erwartungswert ist definitert als die nach ihrer Wahrscheinlichkeit
gewichtete Summe aller Elemente im Wertebereich von <span class="math inline">\(X\)</span> und gibt damit die
mittlere Lage der Wahrscheinlichkeitsverteilung an.
Wenn <span class="math inline">\(W_X\)</span> der Wertebereich von <span class="math inline">\(X\)</span> ist, dann gilt:</p>
<p><span class="math display">\[\mathbb{E}(x)=\mu_X=\sum_{x_k\in W_X}p(x_k)x_k\]</span></p>
<blockquote>
<p><strong>Beispiel:</strong> Der Erwartungswert einer ZV <span class="math inline">\(X\)</span>, die das Werfen eines fairen
Würfels beschreibt ist: <span class="math inline">\(\mathbb{E}(X)=\sum_{k=1}^6k\cdot\frac{1}{6}=3.5\)</span>.</p>
</blockquote>
<p>Wie wir im Kapitel <a href="stat-rep.html#stat-rep">9</a> sehen werden, wird der Erwartungswert in der empirischen
Praxis häufig über den Mittelwert einer Stichprobe identifiziert.</p>
<p>Ein gängiges Maß für die Streuung einer Verteilung <span class="math inline">\(X\)</span> ist die Varianz <span class="math inline">\(Var(X)\)</span>
oder ihre Quadratwurzel, die Standardabweichung, <span class="math inline">\(\sigma(X)=\sqrt{Var(X)}\)</span>.
Letztere wird häufiger verwendet, weil sie die gleiche Einheit hat wie <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[Var(X)=\sum_{x_k\in W_X}\left[x_k-\mathbb{E}(X)\right]^2 p(x_k)\]</span></p>
<blockquote>
<p><strong>Beispiel:</strong> Die Standardabweichung einer ZV <span class="math inline">\(X\)</span>, die das Werfen eines fairen
Würfels beschreibt ist: <span class="math inline">\(\sigma_X=\sqrt{\sum_{k}^6\left[x_k-\mathbb{E}(X)\right]^2 p(x_k)}=\sqrt{5.83}\approx 2.414\)</span>.</p>
</blockquote>
<p>Im Folgenden wollen wir uns einige der am häufigsten verwendeten ZV und ihre
Verteilungen genauer ansehen.
Am Ende der Beschreibung jeder Funktion folgt ein Beispiel für eine
Anwendung.
Wenn Ihnen die theoretischen Ausführungen am Anfang etwas kryptisch erscheinen,
empfiehlt es sich vielleicht erst einmal das Anwendungsbeispiel anzusehen.</p>
</div>
<div id="beispiel-die-binomial-verteilung" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Beispiel: die Binomial-Verteilung</h3>
<p>Die vielleicht bekannteste diskrete Wahrscheinlichkeitsverteilung ist die
Binomialverteilung <span class="math inline">\(\mathcal{B}(n,p)\)</span>.
Mit ihr modelliert man Zufallsexperimente, die aus einer Reihe von Aktionen
bestehen, die entweder zum ‘Erfolg’ oder ‘Misserfolg’ führen.</p>
<p>Die Binomialverteilung ist eine Verteilung mit zwei <strong>Parametern</strong>.
Parameter sind Werte, welche die Struktur der Verteilung bestimmen.
In der Statistik sind wir häufig daran interessiert, die Paramter einer
Verteilung zu bestimmen.
Im Falle der Binomialverteilung gibt es die folgenden zwei Parameter:
<span class="math inline">\(p\)</span> gibt die Erfolgswahrscheinlichkeit einer einzelnen Aktion an
(und es muss daher gelten <span class="math inline">\(p\in[0,1]\)</span>) und <span class="math inline">\(n\)</span> gibt
die Anzahl der Aktionen an.
Daher auch die Kurzschreibweise <span class="math inline">\(\mathcal{B}(n,p)\)</span>.</p>
<blockquote>
<p><strong>Beispiel:</strong> Wenn wir eine faire Münze zehn Mal werfen, können wir das
mit einer Binomialverteilung mit <span class="math inline">\(p=0.5\)</span> und <span class="math inline">\(n=10\)</span> modellieren.</p>
</blockquote>
<p>Die <em>Wahrscheinlichkeitsfunktion</em> <span class="math inline">\(p(x)\)</span> der Binomialverteilung ist die Folgende,
wobei <span class="math inline">\(x\)</span> die Anzahl der Erfolge darstellt:</p>
<p><span class="math display">\[\mathbb{P}(X=x)=p(x)=\binom{n}{x}p^x(1-p)^{n-x}\]</span>
Dies ergibt sich aus den grundlegenden Wahrscheinlichkeitsgesetzen:
<span class="math inline">\(\binom{n}{x}\)</span> ist der
<a href="https://de.wikipedia.org/wiki/Binomialkoeffizient">Binomialkoeffizient</a>
und gibt uns die Anzahl der Möglichkeiten wie man bei <span class="math inline">\(n\)</span> Versuchen <span class="math inline">\(x\)</span>
Erfolge erzielen kann.
Dies multiplizieren wir mit der Wahrscheinlichkeit <span class="math inline">\(x\)</span>-mal einen Erfolg zu
erzielen und <span class="math inline">\(n-x\)</span>-mal einen Misserfolg zu erzielen.</p>
<p>Wenn die ZV <span class="math inline">\(X\)</span> einer Binomialverteilung mit bestimmten Parametern <span class="math inline">\(p\)</span> und <span class="math inline">\(n\)</span>
folgt, dann schreiben wir <span class="math inline">\(P \propto \mathcal{B}(n,p)\)</span> und es gilt, dass
<span class="math inline">\(\mathbb{E}(X)=np\)</span> und <span class="math inline">\(\sigma(X)=\sqrt{np(1-p)}\)</span>.<a href="#fn67" class="footnote-ref" id="fnref67"><sup>67</sup></a></p>
<p>In Abbildung <a href="stat-stoch.html#fig:Binomialverteilung">7.1</a> sehen wir eine Darstellung der Wahrscheinlichkeitsverteilung der Binomialverteilung für verschiedene Parameterwerte.</p>
<div class="figure" style="text-align: center"><span id="fig:Binomialverteilung"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/Binomialverteilung-1.png" alt="Wahrscheinlichkeitsverteilung der Binomialverteilung für verschiedene Parameterwerte." width="672" />
<p class="caption">
Figure 7.1: Wahrscheinlichkeitsverteilung der Binomialverteilung für verschiedene Parameterwerte.
</p>
</div>
<p>R stellt uns einige nützliche Funktionen bereit, mit denen wir typische
Rechenaufgaben einfach lösen können:</p>
<p>Möchten wir die Wahrscheinlichkeit berechnen, genau <span class="math inline">\(x\)</span> Erfolge zu
beobachten, also <span class="math inline">\(\mathbb{P}(X=x)\)</span> geht das mit der Funktion <code>dbinom()</code>.
Die notwendigen Argumente sind <code>x</code> für den interessierenden x-Wert,
<code>size</code> für den Parameter <span class="math inline">\(n\)</span> und <code>prob</code> für den Parameter <span class="math inline">\(p\)</span>:</p>
<div class="sourceCode" id="cb679"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb679-1"><a href="stat-stoch.html#cb679-1" aria-hidden="true"></a><span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">10</span>, <span class="dt">size =</span> <span class="dv">50</span>, <span class="dt">prob =</span> <span class="fl">0.25</span>)</span></code></pre></div>
<pre><code>## [1] 0.09851841</code></pre>
<p>Das bedeutet, wenn <span class="math inline">\(X \propto B(50, 0.25)\)</span>, dann: <span class="math inline">\(\mathbb{P}(X=10)=0.09852\)</span>.
Dies ist in Abbildung <a href="stat-stoch.html#fig:bspbinverteilung">7.2</a> illustriert.</p>
<div class="figure" style="text-align: center"><span id="fig:bspbinverteilung"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/bspbinverteilung-1.png" alt="Beispiel einer Binomialverteilung mit $p$=0.25 und $n$=50." width="672" />
<p class="caption">
Figure 7.2: Beispiel einer Binomialverteilung mit <span class="math inline">\(p\)</span>=0.25 und <span class="math inline">\(n\)</span>=50.
</p>
</div>
<p>Natürlich können wir an die Funktion auch einen atomaren Vektor als erstes
Argument übergeben:</p>
<div class="sourceCode" id="cb681"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb681-1"><a href="stat-stoch.html#cb681-1" aria-hidden="true"></a><span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">5</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">size =</span> <span class="dv">50</span>, <span class="dt">prob =</span> <span class="fl">0.25</span>)</span></code></pre></div>
<pre><code>## [1] 0.004937859 0.012344647 0.025864974 0.046341412 0.072086641 0.098518410</code></pre>
<p>Häufig sind wir auch an der <strong>kumulierten Wahrscheinlichkeitsfunktion</strong>
interessiert.
Während uns die Wahrscheinlichkeitsfunktion die Wahrscheinlichkeit für genau
<span class="math inline">\(x\)</span> Erfolge angibt, also <span class="math inline">\(\mathbb{P}(X=x)\)</span>, gibt uns die <em>kumulierte</em>
Wahrscheinlichkeitsfunktion die Wahrscheinlichkeit für <span class="math inline">\(x\)</span> oder weniger Erfolge,
also <span class="math inline">\(\mathbb{P}(X\leq x)\)</span>.</p>
<p>Die entsprechenden Werte für die kumulierten Wahrscheinlichkeitsfunktion
erhalten wir mit der Funktion <code>pbinom()</code>, welche quasi die gleichen Argumente
benötigt wie <code>dbinom()</code>.
Nur gibt es anstatt des Parameters <code>x</code> jetzt einen Parameter <code>q</code>:</p>
<div class="sourceCode" id="cb683"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb683-1"><a href="stat-stoch.html#cb683-1" aria-hidden="true"></a><span class="kw">pbinom</span>(<span class="dt">q =</span> <span class="dv">10</span>, <span class="dt">size =</span> <span class="dv">50</span>, <span class="dt">prob =</span> <span class="fl">0.25</span>)</span></code></pre></div>
<pre><code>## [1] 0.2622023</code></pre>
<p>Die Wahrscheinlichkeit 10 oder weniger Erfolge bei 10 Versuchen und einer
Erfolgswahrscheinlichkeit von 25% zu erzielen beträgt also 26.2%. Dies ist auch
in Abbildung <a href="stat-stoch.html#fig:kumulativ">7.3</a> ersichtlich.</p>
<div class="figure" style="text-align: center"><span id="fig:kumulativ"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/kumulativ-1.png" alt="Kumulative Wahrscheinlichkeitsfunktion mit $p$=0.25 und $n$=50" width="672" />
<p class="caption">
Figure 7.3: Kumulative Wahrscheinlichkeitsfunktion mit <span class="math inline">\(p\)</span>=0.25 und <span class="math inline">\(n\)</span>=50
</p>
</div>
<p>Schlussendlich haben wir die Funktion <code>qbinom()</code>, welche als ersten Input eine
Wahrscheinlichkeit <code>p</code> akzeptiert und dann den kleinsten Wert <span class="math inline">\(x\)</span> findet,
für den gilt, dass <span class="math inline">\(\mathbb{P}(X=x)\geq p\)</span>.</p>
<p>Wenn wir also wissen möchten wie viele Erfolge mit einer Wahrscheinlichkeit von
50% mindestens zu erwarten sind, dann schreiben wir:</p>
<div class="sourceCode" id="cb685"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb685-1"><a href="stat-stoch.html#cb685-1" aria-hidden="true"></a><span class="kw">qbinom</span>(<span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">size =</span> <span class="dv">50</span>, <span class="dt">prob =</span> <span class="fl">0.25</span>)</span></code></pre></div>
<pre><code>## [1] 12</code></pre>
<p>Es gilt also: <span class="math inline">\(\mathbb{P}(X=12)\geq p\)</span>.</p>
<p>Abbildung <a href="stat-stoch.html#fig:qbinom">7.4</a> verdeutlicht dies grafisch.</p>
<div class="figure" style="text-align: center"><span id="fig:qbinom"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/qbinom-1.png" alt="Graph der invertierten kumulierten Wahrscheinlichkeitsfunktion der Binomialverteilung mit $p=0.25$ und $n=50$" width="672" />
<p class="caption">
Figure 7.4: Graph der invertierten kumulierten Wahrscheinlichkeitsfunktion der Binomialverteilung mit <span class="math inline">\(p=0.25\)</span> und <span class="math inline">\(n=50\)</span>
</p>
</div>
<p>Möchten wir schließlich eine bestimmte Menge an <strong>Realisierungen</strong> aus einer
Binomialverteilung ziehen geht das mit der Funktion <code>rbinom()</code>, welche auch wieder
drei Argumente verlangt: <code>n</code> für die Anzahl der zu ziehenden Realisierungen, sowie <code>size</code> und
<code>prob</code> als da Paramter <span class="math inline">\(n\)</span> und <span class="math inline">\(p\)</span> der Binomialverteilung:</p>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb687-1"><a href="stat-stoch.html#cb687-1" aria-hidden="true"></a>sample_binom &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="dv">5</span>, <span class="dt">size =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="fl">0.4</span>)</span>
<span id="cb687-2"><a href="stat-stoch.html#cb687-2" aria-hidden="true"></a>sample_binom</span></code></pre></div>
<pre><code>## [1] 4 5 2 2 4</code></pre>
<blockquote>
<p><strong>Anwendungsbeispiel Binomialverteilung:</strong> Unser Zufallsexperiment besteht
aus dem zehnmaligen Werfen einer fairen Münze. Unter ‘Erfolg’ verstehen wir
das Werfen von ‘Zahl’. Nehmen wir an, wir führen das Zufallsexperiment 10 Mal durch,
werfen also insgesamt 100 Mal die Münze und schreiben jeweils auf, wie häufig wir
dabei einen Erfolg verbuchen konnten. Wenn wir unsere Ergebnisse aufmalen, indem
wir auf der x-Achse die Anzahl der Erfolge, und auf der y-Achse die Anzahl der
Experimente mit genau dieser Anzahl an Erfolgen festhalten, erhalten wir ein
Histogram, das ungefähr so aussieht wie in Abbildung <a href="#fig:Munzwurf"><strong>??</strong></a>.</p>
</blockquote>
<p><img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/Munzwurf-1.png" width="672" /></p>
<blockquote>
<p>Aus der Logik der Konstruktion des Zufallsexperiments und der Inspektion
unserer Daten können wir schließen, dass die Binomialverteilung eine sinnvolle
Beschreibung des Zufallsexperiments und der daraus entstandenen Stichprobe von
100 Münzwurfergebnissen ist. Da wir eine faire Münze geworfen haben macht es
Sinn für die Binomialverteilung <span class="math inline">\(p=0.5\)</span> anzunehmen, und da wir in jedem einzelnen
Experiment die Münze 10 Mal geworfen haben für <span class="math inline">\(n=10\)</span>. Wenn wir die mit <span class="math inline">\(n=10\)</span> und
<span class="math inline">\(p=0.5\)</span> parametrisierte theoretische Binomialverteilung nehmen und ihre theoretische
Verteilungsfunktion über die Aufzeichnungen unserer Ergebnisse legen, können wir
uns in dieser Vermutung bestärkt fühlen, wie in Abbildung
<a href="stat-stoch.html#fig:Munzwurfverteilung">7.5</a> ersichtlich ist.</p>
</blockquote>
<div class="figure"><span id="fig:Munzwurfverteilung"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/Munzwurfverteilung-1.png" alt="Vergleich der empirischen Stichprobe und der parametrisierten theoretischen Binomialverteilungsfunktion" width="672" />
<p class="caption">
Figure 7.5: Vergleich der empirischen Stichprobe und der parametrisierten theoretischen Binomialverteilungsfunktion
</p>
</div>
</div>
<div id="beispiel-die-poisson-verteilung" class="section level3" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Beispiel: die Poisson-Verteilung</h3>
<p>Bei der Poisson-Verteilung handelt es sich um die Standardverteilung für
unbeschränkte Zähldaten, also diskrete Daten, die kein natürliches Maximum
haben.</p>
<p>Es handelt sich dabei zudem um eine <strong>ein-parametrische</strong>
Funktion, deren einziger Parameter <span class="math inline">\(\lambda&gt;0\)</span> ist.
<span class="math inline">\(\lambda\)</span> wird häufig als die mittlere Ereignishäufigkeit interpretiert und
ist <strong>zugleich Erwartungswert als auch Varianz</strong> der
Verteilung: <span class="math inline">\(\mathbb{E}(P_\lambda)=Var(P_\lambda)=\lambda\)</span>.</p>
<p>Ihre Definitionsmenge ist <span class="math inline">\(\mathbb{N}\)</span>, also alle natürlichen Zahlen -
daher ist sie im Gegensatz zur Binomialverteilung geeignet, wenn die
Definitionsmenge der Verteilung keine natürliche Grenze hat.</p>
<p>Die <strong>Wahrscheinlichkeitsfunktion</strong> der Poisson-Verteilung hat die folgende
Form:</p>
<p><span class="math display">\[p_\lambda(x)=\frac{\lambda^x}{x!}e^{-\lambda}\]</span>
Abbildung <a href="stat-stoch.html#fig:Poisson">7.6</a> zeigt wie sich die Wahrscheinlichkeitsfunktion für
unterschiedliche Werte von <span class="math inline">\(\lambda\)</span> manifestiert.</p>
<div class="figure" style="text-align: center"><span id="fig:Poisson"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/Poisson-1.png" alt="Poisson-Verteilung für verschiedene Parameter." width="672" />
<p class="caption">
Figure 7.6: Poisson-Verteilung für verschiedene Parameter.
</p>
</div>
<p>Wir können die Verteilung mit sehr ähnlichen Funktionen wie bei der
Binomialverteilung analysieren. Nur die Parameter müssen entsprechend angepasst
werden, da es bei der Poisson-Verteilung jetzt nur noch einen Paramter (<code>lambda</code>)
gibt.</p>
<p>Möchten wir die Wahrscheinlichkeit berechnen, genau <span class="math inline">\(x\)</span> Erfolge zu
beobachten, also <span class="math inline">\(\mathbb{P}(X=x)\)</span> geht das mit der Funktion <code>dpois()</code>.
Das einzige notwendige Argument ist <code>lambda</code>
(siehe zudem Abbildung <a href="stat-stoch.html#fig:PoissonBsp">7.7</a>):</p>
<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb689-1"><a href="stat-stoch.html#cb689-1" aria-hidden="true"></a><span class="kw">dpois</span>(<span class="dv">5</span>, <span class="dt">lambda =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## [1] 0.1562935</code></pre>
<div class="figure" style="text-align: center"><span id="fig:PoissonBsp"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/PoissonBsp-1.png" alt="Poisson-Verteilung mit ausgewählten Parameterwerten." width="672" />
<p class="caption">
Figure 7.7: Poisson-Verteilung mit ausgewählten Parameterwerten.
</p>
</div>
<p>Informationen über die CDF erhalten wir über die Funktion <code>ppois()</code>, die zwei
Argumente, <code>q</code> und <code>lambda</code>, annimmt. Grafisch dargestellt ist dies in Abbildung
<a href="stat-stoch.html#fig:PoissonCDF">7.8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:PoissonCDF"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/PoissonCDF-1.png" alt="Kumulierte Wahrscheinlichkeitsfunktion der Poisson-Verteilung mit $\lambda = 4$" width="672" />
<p class="caption">
Figure 7.8: Kumulierte Wahrscheinlichkeitsfunktion der Poisson-Verteilung mit <span class="math inline">\(\lambda = 4\)</span>
</p>
</div>
<p>Mit der Funktion <code>qpois()</code> finden wir für eine Wahrscheinlichkeit <code>p</code> den
kleinsten Wert <span class="math inline">\(x\)</span>, für den gilt, dass <span class="math inline">\(\mathbb{P}(X=x)\geq p\)</span>.</p>
<p>Wenn wir also wissen möchten wie viele Erfolge mit einer Wahrscheinlichkeit von
50% mindestens zu erwarten sind, dann schreiben wir:</p>
<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb691-1"><a href="stat-stoch.html#cb691-1" aria-hidden="true"></a><span class="kw">qpois</span>(<span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">lambda =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<p>Es gilt also: <span class="math inline">\(\mathbb{P}(X=4)\geq 0.5\)</span>.</p>
<p>Wir können dies erneut grafisch verdeutlichen, wie in Abbildung
<a href="stat-stoch.html#fig:PoissonCDFinv">7.9</a> dargestellt.</p>
<div class="figure" style="text-align: center"><span id="fig:PoissonCDFinv"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/PoissonCDFinv-1.png" alt="Invertierte CDF der Poisson-Verteilung mit $\lambda = 4$" width="672" />
<p class="caption">
Figure 7.9: Invertierte CDF der Poisson-Verteilung mit <span class="math inline">\(\lambda = 4\)</span>
</p>
</div>
<p>Möchten wir schließlich eine bestimmte Menge an <strong>Realisierungen</strong> der ZV aus einer
Poisson-Verteilung ziehen geht das mit <code>rpois()</code>, welches zwei notwendige
Argumente annimmt: <code>n</code> für die Anzahl der Realisierungen und <code>lambda</code> für den
Parameter <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb693"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb693-1"><a href="stat-stoch.html#cb693-1" aria-hidden="true"></a>pois_sample &lt;-<span class="st"> </span><span class="kw">rpois</span>(<span class="dt">n =</span> <span class="dv">5</span>, <span class="dt">lambda =</span> <span class="dv">4</span>)</span>
<span id="cb693-2"><a href="stat-stoch.html#cb693-2" aria-hidden="true"></a>pois_sample</span></code></pre></div>
<pre><code>## [1] 3 8 4 4 3</code></pre>
</div>
<div id="hinweise-zu-diskreten-wahrscheinlichkeitsverteilungen" class="section level3" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Hinweise zu diskreten Wahrscheinlichkeitsverteilungen</h3>
<p>Wie Sie vielleicht bereits bemerkt haben sind die R Befehle für
verschiedene Verteilungen alle gleich aufgebaut.
Wenn <code>*</code> für die Abkürzung einer bestimmten Verteilung steht, können wir mit
der Funktion <code>d*()</code> die Werte der Wahrscheinlichkeitsverteilung, mit
<code>p*()</code> die Werte der kumulierten Wahrscheinlichkeitsverteilung und
mit <code>q*()</code> die der Quantilsfunktion berechnen.
Mit <code>r*()</code> werden Realisierungen von Zufallszahlen generiert.
Für das Beispiel der Binomialverteilung, welcher die Abkürzung <code>binom</code>
zugewiesen wurde, heißen die Funktionen entsprechend <code>dbinom()</code>, <code>pbinom()</code>,
<code>qbinom()</code> und <code>rbinom()</code>.</p>
<p>Tabelle <a href="stat-stoch.html#tab:diskret">7.2</a> gibt einen Überblick über gängige Abkürzungen und die
Parameter der oben besprochenen diskreten Verteilungen.</p>
<table>
<caption><span id="tab:diskret">Table 7.2: </span> Überblick der besprochenen diskreten Verteilungen.</caption>
<thead>
<tr class="header">
<th>Verteilung</th>
<th>Abkürzung</th>
<th>Parameter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Binomialverteilung</td>
<td><code>binom</code></td>
<td><code>size</code>, <code>prob</code></td>
</tr>
<tr class="even">
<td>Poisson-Verteilung</td>
<td><code>pois</code></td>
<td><code>lambda</code></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="stetige-wahrscheinlichkeitsmodelle" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Stetige Wahrscheinlichkeitsmodelle</h2>
<div id="stetige-zv" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Stetige ZV</h3>
<p>In vorangegangen Abschnitt haben wir uns mit diskreten Wahrscheinlichkeitsmodellen
beschäftigt.
Die diesen Modellen zugrundeliegenden ZV hatten einen abzählbaren Wertebereich.
Häufig interessieren wir uns aber für ZV mit einem nicht abzählbaren
Wertebereich, z.B. <span class="math inline">\(\mathbb{R}\)</span> oder <span class="math inline">\([0,1]\)</span>.<a href="#fn68" class="footnote-ref" id="fnref68"><sup>68</sup></a></p>
<p>Bei stetigen Wahrscheinlichkeitsmodellen liegen zwischen zwei Punkten
unendlich viele Punkte.
Das hat bedeutende Implikationen für die Angabe von Wahrscheinlichkeiten.
Im Gegensatz zu diskreten Wahrscheinlichkeitsmodellen hat demnach jeder einzelne
Punkt im Wertebereich der ZV die Wahrscheinlichkeit 0:</p>
<p><span class="math display">\[\mathbb{P}(X=x_k)=0 \quad \forall x_k \in W_X\]</span>
wobei <span class="math inline">\(W_X\)</span> für den Wertebereich von ZV <span class="math inline">\(X\)</span> steht.</p>
<p>Als Lösung werden Wahrscheinlichkeiten bei stetigen ZV nicht als
Punktwahrscheinlichkeiten, sondern als Intervallwahrscheinlichkeiten angeben.
Aus <span class="math inline">\(\mathbb{P}(X=x)\)</span> im diskreten Fall wird im stetigen Fall also:</p>
<p><span class="math display">\[\mathbb{P}(a&lt;X\leq b) = \int_a^bf(x)dx, \quad a&lt;b\]</span></p>
<p>Entsprechend wird für stetige ZV eine etwas andere Notation als für diskrete
ZV verwendet, wobei das Prinzip gleich bleibt.
Zudem werden Sie merken, dass im stetigen Fall anstatt Summen immer Integrale
verwendet werden.
Informell kann man ja auch sagen, dass ein Integral nichts anderes ist als eine
Summe über stetige Werte.</p>
<p>Wo wir bei diskreten ZV eine Wahrscheinlichkeitsfunktion (PMF) verwendet haben
verwenden wir nun eine <strong>Wahrscheinlichkeitsdichte</strong>
(<em>probability density function</em>, PDF).
Aus der PMF
<span class="math inline">\(p_X(x)=\mathbb{P}(X=x_k)\)</span>
im diskreten Fall wird nun also die PDF
<span class="math inline">\(f_X(x)\)</span>, für die gilt, dass <span class="math inline">\(\mathbb{P}(a&lt;X\leq b) = \int_a^bf(x)dx\)</span>
für den stetigen Fall.</p>
<p>Für die PDF gilt äquivalent zum diskreten Fall, dass <span class="math inline">\(f_X(x)\geq0\)</span> (keine negativen
Werte) und <span class="math inline">\(\int_{-\infty}^{\infty}f(x)dx=1\)</span> (das Integral (die
‘stetige Summe’) über den ganzen Wertebereich ergibt 1).
Allerdings gibt es einen wichtigen Unterschied: im Gegensatz zur PMF <span class="math inline">\(p_X(x)\)</span>
gibt die PDF <span class="math inline">\(f_X(x)\)</span> <em>keine</em> Wahrscheinlichkeiten an - daher auch nur die
Restriktion <span class="math inline">\(f_X(x)\geq0\)</span> und <em>nicht</em> <span class="math inline">\(1\geq f_X(x)\geq0\)</span>.
Wenn wir von Wahrscheinlichkeiten reden wollen, müssen wir die PDF integrieren:</p>
<p><span class="math display">\[\mathbb{P}((a,b])=\int_a^bf(x)dx\]</span></p>
<p>da wir im stetigen Fall für einzelne Punkte keine von Null verschiedenen
Wahrscheinlichkeiten haben.</p>
<p>Wen übrigens die unterschiedlichen Bezeichnungen “probability <em>mass</em>” und
“probablity <em>density</em>” irritieren: tatsächlich ist die Verwendung dieser
Bezeichnungen ganz analog zu den physikalischen Pendents “Masse” und “Dichte”
in der Physik:
wenn wir die Masse für einzelne Teile einer Stange haben bekommen wir die
Gesamtmasse indem wir die Masse der Teile addieren - das ist genauso wie bei
der PMF: wir bekommen die Gesamtwahrscheinlichkeit indem wir die
Wahrscheinlichkeiten für einzelne Events addieren und die Einzelgewichte indem
wir uns die Masse der einzelnen Teile ansehen.
Wenn wir jetzt eine Stange haben, die unterschiedlich dicht ist, bekommen wir
die gesamte Masse indem wir über die Dichte integrieren.
Wir können die Stange auch in kleinere Teile schneiden und deren Masse addieren.
Wenn die Teile unendlich klein werden kommen wir zum gleichen Ergebnis wie bei
der Integration.</p>
<p>Die <strong>kumulative Verteilungsfunktion</strong> (CDF) ist im stetigen Fall genauso
definiert wie im diskreten Fall:
<span class="math inline">\(F_X(x)=\mathbb{P}(X\leq x)\)</span>, wobei immer gilt:</p>
<p><span class="math display">\[\mathbb{P}(a&lt;X\leq b) = F(b)-F(a)=\int_a^bf(x)dx\]</span></p>
<p>Man sieht hier, dass die Dichtefunktion (PDF) einer ZV die Ableitung ihrer
kumulative Verteilungsfunktion (CDF) ist:</p>
<p><span class="math display">\[F_X&#39;(x)=f_X(x)\]</span></p>
<p>Wie oben beschrieben können wir die Werte an einzelnen Punkten der PDF nicht als
<em>absolute</em> Wahrscheinlichkeiten interpretieren, da die Wahrscheinlichkeit
für einzelne Punkte immer gleich 0 ist und die PDF auch Werte größer 1
annehmen kann.
Wir können aber die Werte der PDF an zwei oder mehr Punkten vergleichen um die
<em>relative</em> Wahrscheinlichkeit der einzelnen Punkte zu bekommen.</p>
<p>Wie bei den diskreten ZV beschreiben wir eine ZV mit Hilfe von bestimmten
Kennzahlen, wie dem <strong>Erwartungswert</strong>, der <strong>Varianz</strong> und den <strong>Quantilen</strong>.
Diese sind quasi äquivalent zum diskreten Fall definiert, nur eben über
Integrale (wir vergleichen alle folgenden Definitionen mit ihrem diskreten
Pendant am Ende des Abschnitts).
Für den Erwartungswert der ZV <span class="math inline">\(X\)</span> gilt somit:</p>
<p><span class="math display">\[\mathbb{E}(X)=\int_{-\infty}^{\infty}xf(x)dx\]</span></p>
<p>Für die Varianz und die Standardabweichung entsprechend:</p>
<p><span class="math display">\[Var(X)= \mathbb{E}(X-\mathbb{E}\left(X)\right)^2=\int_{-\infty}^{\infty}(x-\mathbb{E}(X))^2f(x)dx\]</span></p>
<p><span class="math display">\[\sigma_X=\sqrt{Var(X)}\]</span></p>
<p>Und, schlussendlich, gilt für das <span class="math inline">\(\alpha\)</span>-Quantil <span class="math inline">\(q(\alpha)\)</span>:</p>
<p><span class="math display">\[\mathbb{P}(X\leq q(\alpha))=\alpha\]</span></p>
<p>In Abbildung <a href="stat-stoch.html#fig:quantile">7.10</a> werden das <span class="math inline">\(0.25\)</span> und <span class="math inline">\(0.5\)</span>-Quantil visuell dargestellt.</p>
<div class="figure" style="text-align: center"><span id="fig:quantile"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/quantile-1.png" alt="Vergleich des $0.25$- und $0.5$-Quantils" width="672" />
<p class="caption">
Figure 7.10: Vergleich des <span class="math inline">\(0.25\)</span>- und <span class="math inline">\(0.5\)</span>-Quantils
</p>
</div>
<p>Tabelle <a href="stat-stoch.html#tab:diskretstetig">7.3</a> vergleicht noch einmal die Definitionen der Kennzahlen und
charakteristischer Verteilungen für den stetigen und diskreten Fall.</p>
<table>
<caption><span id="tab:diskretstetig">Table 7.3: </span> Vergleich der Kennzahlen charakteristischer Verteilungen im stetigen und im diskreten Fall.</caption>
<colgroup>
<col width="18%" />
<col width="41%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th>Bezeichnung</th>
<th>Diskreter Fall</th>
<th>Stetiger Fall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Erwartungswert</td>
<td><span class="math inline">\(\mathbb{E}(x)=\sum_{x\in W_X}\mathbb{P}(X=x)x\)</span></td>
<td><span class="math inline">\(\mathbb{E}(X)=\int_{-\infty}^{\infty}xf(x)dx\)</span></td>
</tr>
<tr class="even">
<td>Varianz</td>
<td><span class="math inline">\(Var(X)=\sum_{x\in W_X}\left[x-\mathbb{E}(X)\right]^2 \mathbb{P}(X=x)x\)</span></td>
<td><span class="math inline">\(Var(X)= \mathbb{E}(X-\mathbb{E}\left(X)\right)^2\)</span></td>
</tr>
<tr class="odd">
<td>Standard-abweichung</td>
<td><span class="math inline">\(\sqrt{Var(X)}\)</span></td>
<td><span class="math inline">\(\sqrt{Var(X)}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha\)</span>-Quantil</td>
<td><span class="math inline">\(\mathbb{P}(X\leq q(\alpha))=\alpha\)</span></td>
<td><span class="math inline">\(\mathbb{P}(X\leq q(\alpha))=\alpha\)</span></td>
</tr>
<tr class="odd">
<td>Dichtefunktion (PDF)</td>
<td>NA</td>
<td><span class="math inline">\(f_X(x)=\mathbb{P}([a,b])=\int_a^bf(x)dx\)</span></td>
</tr>
<tr class="even">
<td>Wahrsch’s-funktion (PMF)</td>
<td><span class="math inline">\(p_X(x_k)=\mathbb{P}(X=x_k)\)</span></td>
<td>NA</td>
</tr>
<tr class="odd">
<td>Kumulierte Verteilungsfunktion (CDF)</td>
<td><span class="math inline">\(\mathbb{P}(X\leq x)\)</span></td>
<td><span class="math inline">\(F(x)=\mathbb{P}(X\leq x)\)</span></td>
</tr>
</tbody>
</table>
<p>Analog zum diskreten Fall wollen wir uns nun die am häufigsten vorkommenden
stetigen Verteilungen noch einmal genauer anschauen.
Vorher wollen wir jedoch die oben eingeführten Konzepte (statistische
Unabhängigkeit, bedingte Wahrscheinlichkeiten, etc.) noch für den stegigen
Fall formulieren - am Prinzip ändert sich hier nichts, nur an der Notation.</p>
</div>
<div id="beispiel-die-uniformverteilung" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Beispiel: die Uniformverteilung</h3>
<p>Die Uniformverteilung kann auch mit einem beliebigen Intervall <span class="math inline">\([a,b]\)</span> mit <span class="math inline">\(a&lt;b\)</span>
definiert werden und ist dadurch gekennzeichnet, dass die Dichte über <span class="math inline">\([a,b]\)</span>
vollkommen konstant ist.
Ihre einzigen Parameter sind die Grenzen des Intervalls, <span class="math inline">\(a\)</span> und <span class="math inline">\(b\)</span>.</p>
<p>Da bei stetigen Verteilungen die Dichte aller Werte außerhalb des
Wertebereichs per definitionem gleich Null ist, haben wir folgenden Ausdruck
für die Dichte der Uniformverteilung:</p>
<p><span class="math display">\[f(x)=
\begin{cases} 
      \frac{1}{b-a} &amp; a\leq x \leq b \\
      0 &amp; \text{sonst} \left(x\notin W_X\right)
   \end{cases}
   \]</span>
Auch der Erwartungswert ist dann intuitiv definiert, er liegt nämlich genau in
der Mitte des Intervalls <span class="math inline">\([a,b]\)</span> und ist definiert als <span class="math inline">\(\mathbb{E}(X)=\frac{a+b}{2}\)</span>.
Die Varianz ist mit <span class="math inline">\(Var(X)=\frac{(b-a)^2}{12}\)</span> gegeben.</p>
<p>Die Dichtefunktion der Uniformverteilung für <span class="math inline">\([a,b]=[2,4]\)</span> ist in Abbildung <a href="stat-stoch.html#fig:dichteuniform">7.11</a> dargestellt:</p>
<div class="figure" style="text-align: center"><span id="fig:dichteuniform"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/dichteuniform-1.png" alt="Dichte der Uniformverteilung mit $a = 2$ und $b = 4$" width="672" />
<p class="caption">
Figure 7.11: Dichte der Uniformverteilung mit <span class="math inline">\(a = 2\)</span> und <span class="math inline">\(b = 4\)</span>
</p>
</div>
<p>Die Abkürung in R für die Uniformverteilung ist <code>unif</code>. Endsprechend berechnen
wir Werte für die Dichte mit <code>dunif()</code>, welches lediglich die Argumente <code>a</code> und
<code>b</code> für die Grenzen des Intervalls benötigt:</p>
<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb695-1"><a href="stat-stoch.html#cb695-1" aria-hidden="true"></a><span class="kw">dunif</span>(<span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">0.1</span>), <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##  [1] 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25</code></pre>
<p>Wie wir sehen erhalten wir hier immer den gleichen Wert <span class="math inline">\(\frac{1}{b-a}\)</span>, was
die zentrale Eigenschaft der Uniformverteilung ist.
Hier wird auch deutlich, dass dieser Wert die <em>relative</em> Wahrscheinlichkeit
angibt, da die absolute Wahrscheinlichkeit für jeden einzelnen Wert wie oben
beschrieben bei stetigen ZV 0 ist.</p>
<p>Die CDF berechnen wir entsprechend mit <code>punif()</code>.
Wenn <span class="math inline">\(X\propto U(0,4)\)</span> erhalten wir <span class="math inline">\(\mathbb{P}(X\leq3)\)</span> entprechend mit:</p>
<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb697-1"><a href="stat-stoch.html#cb697-1" aria-hidden="true"></a><span class="kw">punif</span>(<span class="fl">0.8</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## [1] 0.2</code></pre>
<p>Abbildung <a href="stat-stoch.html#fig:CDFuniform">7.12</a> zeigt dies grafisch.</p>
<div class="figure" style="text-align: center"><span id="fig:CDFuniform"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/CDFuniform-1.png" alt="CDF der Uniformverteilung mit $a = 0$ und $b = 1$" width="672" />
<p class="caption">
Figure 7.12: CDF der Uniformverteilung mit <span class="math inline">\(a = 0\)</span> und <span class="math inline">\(b = 1\)</span>
</p>
</div>
<p>Auch ansonsten können wir die Syntax der diskreten Verteilungen mehr oder weniger
übernehmen: <code>qunif()</code> akzeptiert die gleichen Parameter wie <code>punif()</code> und
gibt uns Werte der inversen CDF.
<code>runif()</code> kann verwendet werden um Realisierungen einer uniform verteilten ZV
zu generieren:</p>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb699-1"><a href="stat-stoch.html#cb699-1" aria-hidden="true"></a>uniform_sample &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">5</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">4</span>)</span>
<span id="cb699-2"><a href="stat-stoch.html#cb699-2" aria-hidden="true"></a>uniform_sample</span></code></pre></div>
<pre><code>## [1] 3.5209862 1.4563675 1.1529571 0.6825809 0.6886870</code></pre>
</div>
<div id="beispiel-die-normalverteilung" class="section level3" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Beispiel: die Normalverteilung</h3>
<p>Die wahrscheinlich bekannteste stetige Verteilung ist die Normalverteilung.
Das liegt nicht nur daran, dass viele natürliche Phänomene als die
Realisierung einer normalverteilten ZV modelliert werden können, sondern auch
weil es sich mit der Normalverteilung in der Regel sehr einfach rechnen lässt.
Sie ist also häufig auch einfach eine bequeme Annahme.</p>
<p>Bei der Normalverteilung handelt es sich um eine <strong>zwei-parametrige</strong>
Verteilung über den Wertebereich <span class="math inline">\(W_X=\mathbb{R}\)</span>.
Die beiden Parameter sind <span class="math inline">\(\mu\)</span> und <span class="math inline">\(\sigma^2\)</span>, welche unmittelbar als
Erwartungswert (<span class="math inline">\(\mathbb{E}(X)=\mu\)</span>) und Varianz (<span class="math inline">\(Var(X)=\sigma^2\)</span>) gelten.
Wir schreiben <span class="math inline">\(X\propto \mathscr{N}(\mu, \sigma^2)\)</span> wenn für die PDF von <span class="math inline">\(X\)</span>
gilt:</p>
<p><span class="math display">\[f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span></p>
<p>Unter der <strong>Standard-Normalverteilung</strong> verstehen wir eine Normalverteilung mit
den Paramtern <span class="math inline">\(\mu=0\)</span> und <span class="math inline">\(\sigma=1\)</span>.<a href="#fn69" class="footnote-ref" id="fnref69"><sup>69</sup></a>
Sie verfügt über die deutlich vereinfachte PDF:</p>
<p><span class="math display">\[f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\]</span></p>
<p>Die CDF der Normalverteilung ist analytisch nicht einfach darzustellen, die
Werte können in R aber leicht über die Funktion <code>pnorm</code> (s.u.) abgerufen werden.</p>
<p>In Abbildung <a href="stat-stoch.html#fig:PDFCDFnormal">7.13</a> sind die PDF und CDF für exemplarische Parameterkombinationen dargestellt.</p>
<div class="figure" style="text-align: center"><span id="fig:PDFCDFnormal"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/PDFCDFnormal-1.png" alt="Beispielhafter Vergleich einer PDF und CDF bei Normalverteilung" width="672" />
<p class="caption">
Figure 7.13: Beispielhafter Vergleich einer PDF und CDF bei Normalverteilung
</p>
</div>
<p>Die Abkürzung in R ist <code>norm</code>. Alle Funktionen nehmen die Paramter <span class="math inline">\(\mu\)</span> und
<span class="math inline">\(\sigma\)</span> (nicht <span class="math inline">\(\sigma^2\)</span>) über <code>mean</code> und <code>sd</code> als notwendige Argumente.
Ansonsten ist die Verwendung äquivalent zu den vorherigen Beispielen:</p>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="stat-stoch.html#cb701-1" aria-hidden="true"></a><span class="kw">dnorm</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.75</span>), <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>) <span class="co"># relative Wahrscheinlichkeiten über PDF</span></span></code></pre></div>
<pre><code>## [1] 0.1933341 0.1979188</code></pre>
<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb703-1"><a href="stat-stoch.html#cb703-1" aria-hidden="true"></a><span class="kw">pnorm</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.75</span>), <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>) <span class="co"># Werte der CDF</span></span></code></pre></div>
<pre><code>## [1] 0.4012937 0.4502618</code></pre>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="stat-stoch.html#cb705-1" aria-hidden="true"></a><span class="kw">qnorm</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.75</span>), <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>) <span class="co"># Werte der I-CDF</span></span></code></pre></div>
<pre><code>## [1] 1.00000 2.34898</code></pre>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="stat-stoch.html#cb707-1" aria-hidden="true"></a>norm_sample &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">5</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>) <span class="co"># 5 Realisierungen der ZV</span></span>
<span id="cb707-2"><a href="stat-stoch.html#cb707-2" aria-hidden="true"></a>norm_sample</span></code></pre></div>
<pre><code>## [1]  0.9099446 -0.5698089 -2.3358839  0.2395470  2.8379932</code></pre>
<blockquote>
<p><strong>Beispiel zum Zusammenhang</strong> <code>dnorm()</code> und <code>qnorm()</code></p>
</blockquote>
</div>
<div id="beispiel-die-exponentialverteilung" class="section level3" number="7.4.4">
<h3><span class="header-section-number">7.4.4</span> Beispiel: die Exponentialverteilung</h3>
<p>Sehr häufig wird uns auch die Exponentialverteilung begegnen. Außerhalb der
Ökonomik wird sie v.a. zur Modellierung von Zerfallsprozessen oder Wartezeiten
verwendet, in der Ökonomik spielt sie in der Wachstumstheorie eine zentrale
Rolle.
Es handelt sich bei der Exponentialverteilung um eine <strong>ein-parametrige</strong>
Verteilung mit Parameter <span class="math inline">\(\lambda \in \mathbb{R}^+\)</span> und mit dem Wertebereich
<span class="math inline">\(W_X=[0, \infty ]\)</span>.</p>
<p>Die PDF der Exponentialverteilung ist:</p>
<p><span class="math display">\[f(x)=\begin{cases}
0 &amp; x &lt; 0\\
\lambda e^{-\lambda x} &amp; x \geq 0
\end{cases}\]</span></p>
<p>wobei <span class="math inline">\(e\)</span> die <a href="https://de.wikipedia.org/wiki/Eulersche_Zahl">Eulersche Zahl</a> ist.
Die CDF ist entsprechend:</p>
<p><span class="math display">\[F(x)=\begin{cases}
0 &amp; x &lt; 0\\
1-e^{-\lambda x} &amp; x \geq 0
\end{cases}\]</span></p>
<p>Beide Verteilungen sind in Abbildung <a href="stat-stoch.html#fig:PDFCDFexponential">7.14</a> dargestellt.</p>
<div class="figure" style="text-align: center"><span id="fig:PDFCDFexponential"></span>
<img src="Chap-Wahrscheinlichkeitstheorie_files/figure-html/PDFCDFexponential-1.png" alt="Beispielhafter Vergleich einer PDF und CDF bei Exponentialverteilung" width="672" />
<p class="caption">
Figure 7.14: Beispielhafter Vergleich einer PDF und CDF bei Exponentialverteilung
</p>
</div>
<p>Der Erwartungswert und die Varianz sind für die Exponentialverteilung
äquivalent und hängen ausschließlich von <span class="math inline">\(\lambda\)</span> ab:
<span class="math inline">\(\mathbb{E}(X)=\sigma_X=\frac{1}{\lambda}\)</span>.</p>
<p>Die Abkürzung in R ist <code>exp</code>. Alle Funktionen nehmen den Paramter <span class="math inline">\(\lambda\)</span> über
das Argument <code>rate</code> an:</p>
<div class="sourceCode" id="cb709"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb709-1"><a href="stat-stoch.html#cb709-1" aria-hidden="true"></a><span class="kw">dexp</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.75</span>), <span class="dt">rate =</span> <span class="dv">1</span>) <span class="co"># relative Wahrscheinlichkeiten über PDF</span></span></code></pre></div>
<pre><code>## [1] 0.6065307 0.4723666</code></pre>
<div class="sourceCode" id="cb711"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb711-1"><a href="stat-stoch.html#cb711-1" aria-hidden="true"></a><span class="kw">pexp</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.75</span>), <span class="dt">rate =</span> <span class="dv">1</span>) <span class="co"># Werte der CDF</span></span></code></pre></div>
<pre><code>## [1] 0.3934693 0.5276334</code></pre>
<div class="sourceCode" id="cb713"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb713-1"><a href="stat-stoch.html#cb713-1" aria-hidden="true"></a><span class="kw">qexp</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.75</span>), <span class="dt">rate =</span> <span class="dv">1</span>) <span class="co"># Werte der I-CDF</span></span></code></pre></div>
<pre><code>## [1] 0.6931472 1.3862944</code></pre>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="stat-stoch.html#cb715-1" aria-hidden="true"></a>exp_sample &lt;-<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">5</span>, <span class="dt">rate =</span> <span class="dv">1</span>) <span class="co"># 5 Realisierungen der ZV</span></span>
<span id="cb715-2"><a href="stat-stoch.html#cb715-2" aria-hidden="true"></a>exp_sample</span></code></pre></div>
<pre><code>## [1] 0.8232605 0.4757590 3.4635949 1.2740277 1.0814852</code></pre>
<p>Es gibt übrigens einen
<a href="https://www.exponentialverteilung.de/vers/beweise/uebergang_poissonverteilung.html">wichtigen Zusammenhang</a>
zwischen der stetigen Exponential- und der diskreten Poisson-Verteilung.</p>
</div>
</div>
<div id="zusammenfassung-wahrscheinlichkeitsmodelle-für-einzelne-zv" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Zusammenfassung Wahrscheinlichkeitsmodelle für einzelne ZV</h2>
<p>Tabelle <a href="stat-stoch.html#tab:verteilungen">7.4</a> fasst noch einmal alle Wahscheinlichkeitsmodelle zusammen,
die wir bislang betrachtet haben.</p>
<table>
<caption><span id="tab:verteilungen">Table 7.4: </span> Überblick der Verteilungen und ihrer Parameter.</caption>
<thead>
<tr class="header">
<th>Verteilung</th>
<th>Art</th>
<th>Abkürzung</th>
<th>Parameter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Binomialverteilung</td>
<td>Diskret</td>
<td><code>binom</code></td>
<td><code>size</code>, <code>prob</code></td>
</tr>
<tr class="even">
<td>Poisson-Verteilung</td>
<td>Diskret</td>
<td><code>pois</code></td>
<td><code>lambda</code></td>
</tr>
<tr class="odd">
<td>Uniform-Verteilung</td>
<td>Kontinuierlich</td>
<td><code>punif</code></td>
<td><code>min</code>, <code>max</code></td>
</tr>
<tr class="even">
<td>Normalverteilung</td>
<td>Kontinuierlich</td>
<td><code>norm</code></td>
<td><code>mean</code>, <code>sd</code></td>
</tr>
<tr class="odd">
<td>Exponential-Verteilung</td>
<td>Kontinuierlich</td>
<td><code>exp</code></td>
<td><code>rate</code></td>
</tr>
</tbody>
</table>
<p>In der statistischen Praxis sind das die Modelle, die wir verwenden, um die DGP
(<em>data generating processes</em>) zu beschreiben - also die Prozesse, welche die Daten,
die wir in unserer Forschung verwenden, generiert haben.</p>
<p>Deswegen sprechen Statistiker*innen auch häufig von <em>Populationsmodellen</em>.
Am besten stellt man es sich mit Hilfe der <code>r*()</code> Funktionen vor:
man nimmt an, dass es einen DGP gibt, und dass unsere Daten der Output der
<code>r*()</code>-Funktion zum Ziehen von Realisierungen sind.
Mit dem Begriff des Populationsmodells macht man dabei deutlich, dass unsere
Stichprobe nur eine Stichprobe darstellt - und nicht die gesamte Population aller
möglichen Realisierungen des DGP.</p>
<p>Nun wird auch deutlich, warum Kenntnisse in der Wahrscheinlichkeitsrechnung so
wichtig sind:
wenn wir statistisch mit Daten arbeiten, dann versuchen wir in der Regel über
die Daten Rückschlüsse auf den DGP zu schließen.
Dafür müssen wir zunächst einmal eine grobe Struktur für den DGP annehmen, und
dafür brauchen wir Kenntnisse in der Wahrscheinlichkeitsrechnung und für
den entsprechenden Anwendungsfall konkrete Vorannahmen.
Dann können wir, gegeben unsere Daten, unsere Beschreibung des DGP verfeinern.</p>
<p>Das bedeutet, dass wir für den DGP ein bestimmtes
Wahrscheinlichkeitsmodell annehmen und dann auf Basis unserer Daten die Parameter
für dieses Modell schätzen. Dieses Vorgehen nennen wir <em>parametrisch</em>, weil wir
hier vor allem Parameter schätzen wollen.<a href="#fn70" class="footnote-ref" id="fnref70"><sup>70</sup></a></p>
</div>
<div id="analyse-mehrerer-zufallsvariablen-gemeinsame-und-marginale-verteilungen" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> Analyse mehrerer Zufallsvariablen: gemeinsame und marginale Verteilungen</h2>
<p>Zum Abschluss dieses Kapitels wollen wir uns noch mit der Analyse von ZV
beschäftigen, die ihrerseits aus der Kombination anderer ZV entstehen.
Wir wissen ja bereits, dass die Summe oder das Produkt von ZV selbst wieder eine
ZV ergibt.<a href="#fn71" class="footnote-ref" id="fnref71"><sup>71</sup></a>
Und diese ‘neuen’ ZV sind genau das, was uns in diesem Abschnitt interessiert.
Diese ‘kombinierten’ ZV sind in der Praxis häufig besonders relevant.</p>
<p>Die Verteilungen, die wir bislang kennengelernt haben beschreiben alle die
Verteilung einer einzelnen ZV.
Anhand der Konzepte der bedingten Wahrscheinlichkeit und der statistischen
Unabhängigkeit konnten wir ja schon erahnen, dass es häufig von besonderem
Interesse ist, wie mehrere ZV miteinander interagieren.
Häufig sind wir daran interessiert, die Verteilung dieser neuen ZV zu
charakterisieren.
Wir nennen die Verteilung einer ZV, die sich aus mehreren ZV ergibt eine
<strong>gemeinsame Verteilung</strong>.
Eine gemeinsame Verteilung gibt uns Informationen über die Wahrscheinlichkeit
von Ereignissen, die von allen beteiligten ZV abhängen.</p>
<blockquote>
<p><strong>Beispiel:</strong>
Ein Beispiel für eine praktisch sehr relevante ‘kombinierte’ ZV wäre die
gemeinsame Verteilung von Luftverschmutzung und Atemwegserkrankungen.
Sowohl der Grad an Luftverschmutzung als auch das Auftreten einer
Atemwegserkrankung kann jeweils als isolierte ZV modelliert werden, aber von
besonderem Interesse ist natürlich deren gemeinsame Verteilung, bzw. die
bedingten Wahrscheinlichkeiten (also für einen bestimmten Grad an Luftverschmutung
eine Atemwegserkrankung zu bekommen).</p>
</blockquote>
<div id="gemeinsame-verteilungen-für-diskrete-zv" class="section level3" number="7.6.1">
<h3><span class="header-section-number">7.6.1</span> Gemeinsame Verteilungen für diskrete ZV</h3>
<p>Nehmen wir einmal an wir haben es mit zwei diskreten ZV Variablen, <span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span>,
zu tun. <span class="math inline">\(X\)</span> kann dabei die Werte <span class="math inline">\(\{x_1, x_2,...,x_n\}\)</span> und <span class="math inline">\(Y\)</span> die Werte
<span class="math inline">\(\{y_1, y_2,...,y_n\}\)</span> annehmen.
Die gemeinsame Verteilungsfunktion sollte nun Wahrscheinlichkeiten für alle
möglichen Kombinationen <span class="math inline">\(\{(x_1,y_1), (x_1,y_2),...,(x_n,y_m)\}\)</span> angeben.
Wir sprechen hier also von einer gemeinsamen PMF <span class="math inline">\(p_{XY}(x_i,y_j)\)</span> für die gilt:</p>
<p><span class="math display">\[p_{XY}(x_i,y_j)=\mathbb{P}(X=x_i, Y=y_i) \]</span></p>
<p>Eine solche gemeinsame PMF hat zwei Eigenschaften, ganz analog zur ‘normalen’
PMF:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(0\leq p_{XY}(x_i,y_i)\leq1\)</span>: die Wahrscheinlichkeiten für jede
Kombination müssen zwischen 0 und 1 liegen;</li>
<li><span class="math inline">\(\sum_{i=1}^n\sum_{j=1}^m p_{XY}(x_i,y_i)=1\)</span>: die Summe aller
Einzelwahrscheinlichkeiten ist 1.</li>
</ol>
<blockquote>
<p><strong>Beispiel: das Werfen zweier Würfel</strong> Da wir den Wurf eines einzelnen Würfels
als diskrete ZV repräsentieren können, können wir den Wurf zweier Würfel als
eine gemeinsame diskrete ZV repräsentieren. Grafisch können wir die
Wahrscheinlichkeiten recht anschaulich in einer Tabelle abbilden, wobei die
einzelnen Zellen jeweils die Werte der gemeinsamen PMF enthalten
(siehe Abbildung <a href="#tab:joint-dice"><strong>??</strong></a>).</p>
</blockquote>
<p><span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span> beschreiben dabei jeweils die ZV für den ersten und zweiten Würfel.</p>
</div>
<div id="gemeinsame-verteilungen-für-stetige-zv" class="section level3" number="7.6.2">
<h3><span class="header-section-number">7.6.2</span> Gemeinsame Verteilungen für stetige ZV</h3>
<p>Die Darstellung des stetigen Falles ist komplett äquivalent:
nehmen wir an, <span class="math inline">\(X\)</span> sei eine stetige ZV mit Wertebereich <span class="math inline">\([a,b]\)</span> und <span class="math inline">\(Y\)</span> eine
stetige ZV mit Wertebereich <span class="math inline">\([c,d]\)</span>, dann ist der Wertebereich der gemeinsamen
PDF <span class="math inline">\(f_{XY}(x,y)\)</span> gegeben durch <span class="math inline">\([a,b]\times [c,d]\)</span>.
Auch für <span class="math inline">\(f_{XY}(x,y)\)</span> gilt analog zum einfachen Fall:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(p_{XY}(x_i,y_i)\geq0\)</span>: die Wahrscheinlichkeitsdichte für jede
Kombination muss größer Null sein;</li>
<li><span class="math inline">\(\int_{c}^d\int_{a}^b f_{XY}(x,y)=1\)</span>: das Integral über den gesamten
Wertebereich ist 1.</li>
</ol>
<p>Grafisch könnten wir die gemeinsame PDF als Quadrat darstellen
(siehe Abbildung <a href="stat-stoch.html#fig:jointstetig">7.15</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:jointstetig"></span>
<img src="figures/continuous_joint_dist.pdf" alt="Gemeinsame Verteilung zweier stetiger ZV. Die Wahrscheinlichkeit eines Events korrespondiert zur Fläche." width="50%" />
<p class="caption">
Figure 7.15: Gemeinsame Verteilung zweier stetiger ZV. Die Wahrscheinlichkeit eines Events korrespondiert zur Fläche.
</p>
</div>
</div>
<div id="gemeinsame-kumulative-verteilungen" class="section level3" number="7.6.3">
<h3><span class="header-section-number">7.6.3</span> Gemeinsame kumulative Verteilungen</h3>
<p>Auch die kumulativen Verteilungen für mehrere ZV sind äquivalent zum einfachen Fall definiert.
In der allgemeinen Schreibweise schreiben wir:</p>
<p><span class="math display">\[F_{XY}(x,y)=\mathbb{P}(X\leq x, Y\leq y).\]</span></p>
<p>Für den diskreten Bereich übersetzt sich das konkret in:</p>
<p><span class="math display">\[F_{XY}(x,y)=\sum_{x_i\leq x} \sum_{y_j\leq y}p(x,y) \]</span>.</p>
<p>Im kontiniueirlichen Fall ist diese Funktion wieder für den Wertebereich <span class="math inline">\([a,b]\times [c,d]\)</span> definiert als:</p>
<p><span class="math display">\[F_{XY}(x,y==\int_{c}^d\int_a^b f(x,y)dxdy\]</span>.</p>
<p>Wenn wir aus der CDF die PDF herleiten wollen müssen wir die CDF nach beiden Variablen ableiten:</p>
<p><span class="math display">\[f(x,y)=\frac{\partial^2F}{\partial x \partial y}(x,y) \]</span>.</p>
<p>Ansonsten sind die Eigenschaften der gemeinsamen CDF wieder vergleichbar zu denen der einfachen CDF (wachsend und für positive/negative Extremwerte von x und y geht der Wert gegen 0/1).</p>
</div>
<div id="marginale-verteilungen" class="section level3" number="7.6.4">
<h3><span class="header-section-number">7.6.4</span> Marginale Verteilungen</h3>
<p>Häufig kennen wir die gemeinsame Verteilung von zwei oder mehr ZV und wollen aus
dieser gemeinsamen Verteilung die Verteilungen der einzelnen ZV ableiten.
Haben wir es z.B. mit einer gemeinsamen Verteilung <span class="math inline">\(p_{XY}(x,y)\)</span> zu tun wollen
wir häufig die separaten Verteilungen <span class="math inline">\(p_X(x)\)</span> und <span class="math inline">\(p_Y(y)\)</span> ableiten.
Wir sprechen in diesem Fall von der Herleitung einer <em>marginalen</em> Verteilung von
<span class="math inline">\(X\)</span> bzw. <span class="math inline">\(Y\)</span>.
Im Ergebnis ist eine marginale Verteilung eine ‘ganz normale’ Verteilung, so
wie wir sie vor diesem Abschnitt besprochen haben - der Zusatz ‘marginal’
ergibt sich nur daraus, dass sie aus einer gemeinsamen Verteilung abgeleitet
wurde.</p>
<p>Im diskreten Fall erhalten wir die marginalen Verteilungen durch das Aufsummieren
bei Konstanthaltung der anderen Variablen.
Im Falle der gemeinsamen Verteilung <span class="math inline">\(p_{XY}(x,y)\)</span> gilt dabei also:</p>
<p><span class="math display">\[p_X(x_i) = \sum_jp(x_i,y_j)\]</span>
und</p>
<p><span class="math display">\[p_Y(y_i) = \sum_ip(x_i,y_j)\]</span>
<strong>Beispiel:</strong> Für das oben beschriebene Beispiel des Werfens zweier Würfel
können wir die marginale Verteilung des ersten Würfelwurfes (also von <span class="math inline">\(X\)</span>)
auf die in Abbildung <a href="#tab:joint-dice-marg"><strong>??</strong></a> dargestellte Art und Weise
erhalten.
Hier wird auch deutlich, wo der Name ‘marginal’ herkommt: wir betrachten die
aufsummierten Wahrscheinlichkeiten ‘am Rand’.
Die marginale Verteilung im stetigen Fall hat genau die gleiche Bedeutung wie
im diskreten Fall.</p>
</div>
<div id="bedingte-verteilungen-und-bedinge-momente" class="section level3" number="7.6.5">
<h3><span class="header-section-number">7.6.5</span> Bedingte Verteilungen und bedinge Momente</h3>
<p>Die bedingte Verteilung einer ZV beschreibt die Verteilung einer ZV für den
Fall dass eine andere ZV auf einen bestimmten Realisationswert bedingt ist.
Die Definition ist analog zur allgemeinen bedingten Wahrscheinlichkeit, die wir
schon weiter oben eingeführt haben.
Daher betrachten wir hier nur ein Beispiel und zwar den folgenden Zusammenhang
zwischen <span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span>, wobei gilt, dass <span class="math inline">\(X:\)</span> “Es schneit” und <span class="math inline">\(Y:\)</span> “Es ist kalt”.
Dann wäre eine solche gemeinsame Verteilung plausibel:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Kalt (<span class="math inline">\(X=1\)</span>)</th>
<th>Warm (<span class="math inline">\(X=0\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Schnee (<span class="math inline">\(Y=1\)</span>)</td>
<td><span class="math inline">\(0.15\)</span></td>
<td><span class="math inline">\(0.07\)</span></td>
</tr>
<tr class="even">
<td>Kein Schnee (<span class="math inline">\(Y=0\)</span>)</td>
<td><span class="math inline">\(0.15\)</span></td>
<td><span class="math inline">\(0.63\)</span></td>
</tr>
</tbody>
</table>
<p>Um aus dieser gemeinsamen Verteilung die bedingte Verteilung von <span class="math inline">\(Y\)</span>
abzuleiten verwenden wir die bereits oben eingeführte Formel:</p>
<p><span class="math display">\[\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\]</span></p>
<p>und passen sie für unseren Verteilungsfall an:</p>
<p><span class="math display">\[\mathbb{P}(X=x | Y=y)=\frac{\mathbb{P}(X=x, Y=y)}{\mathbb{P}(Y=y)}\]</span></p>
<p>Sind wir z.B. an der Wahrscheinlichkeit für Schnee interessiert, gegeben dass
das Wetter kalt ist, dann ergibt sich:</p>
<p><span class="math display">\[\mathbb{P}(Y=1 | X=1)=\frac{\mathbb{P}(X=1, Y=1)}{\mathbb{P}(X=1)}=\frac{0.15}{0.3}=0.5\]</span></p>
<p>Wir können nun die bereits oben beschriebene Eigenschaft der <em>Unabhängigkeit</em> auch
noch einmal im Kontext von gemeinsamen Verteilungen formulieren:
zwei ZV <span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span> gelten als unabhängig, wenn die bedingte Verteilung von
<span class="math inline">\(X\)</span> gegeben <span class="math inline">\(Y\)</span> nicht von <span class="math inline">\(Y\)</span> abhängt, also gilt, dass
<span class="math inline">\(\mathbb{P}(X=x | Y=y)=\mathbb{P}(X=x)\)</span>.</p>
<p>Ganz analog zur Verteilung können wir bedingte Momente
(wie den Erwartungswert oder die Varianz) formulieren.
Besonders häufig verwendet wird dabei der <em>bedingte Erwartungswert</em>.
Ein in diesem Kontext häufig gebrauchtes Konzept ist das
<em>Gesetz der wiederholten Erwartungen</em> (<strong>law of iterated expectations</strong>), das
einen Zusammenhang zwischen dem Erwartungswert und dem bedingten
Erwartungswert herstellt:</p>
<p><span class="math display">\[\mathbb{E}(X)=\mathbb{E}\left[\mathbb{E}\left(X | Y\right)\right]\]</span></p>
<p>Im diskreten Fall können wir das Konzept noch leichter verdeutlichen.
Hier gilt:</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(X)=\mathbb{E}\left[\mathbb{E}\left(X | Y\right)\right] = \sum_i \mathbb{E}\left(X | Y=y_i\right) \cdot \mathbb{P}\left(Y=y\right)
\end{align}\]</span></p>
<p>Für den Fall zweier Würfe ist das nichts anderes aus das Summieren der
Zeile, wobei jedes Event mit der Eintrittswahrscheinlichkeit gewichtet wird.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathbb{E}(X) ={} &amp; \mathbb{E}\left[\mathbb{E}\left(X | Y\right)\right] = \mathbb{E}\left(X | Y=1\right) \cdot \frac{1}{6} + \mathbb{E}\left(X | Y=2\right) \cdot \frac{1}{6} + \mathbb{E}\left(X | Y=3\right) \cdot \frac{1}{6} + \mathbb{E}\left(X | Y=4\right) \cdot \frac{1}{6} + \\ &amp; \mathbb{E}\left(X | Y=5\right) \cdot \frac{1}{6} + \mathbb{E}\left(X | Y=6\right) \cdot \frac{1}{6}
\end{aligned}
\end{equation}\]</span></p>
<p>Für den Fall, dass wir am Erwartungswert für eine 6 beim ersten Würfel
interessiert sind wäre das also:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathbb{E}(X=6) ={} &amp;\mathbb{E}\left[\mathbb{E}\left(X=6 | Y\right)\right] = \mathbb{E}\left(X=6 | Y=1\right) \cdot \frac{1}{6} + \mathbb{E}\left(X=6 | Y=2\right) \cdot \frac{1}{6} + \mathbb{E}\left(X=6 | Y=3\right) \cdot \frac{1}{6}\\
&amp; + \mathbb{E}\left(X=6 | Y=4\right) \cdot \frac{1}{6} + \mathbb{E}\left(X=6 | Y=5\right) \cdot \frac{1}{6} + \mathbb{E}\left(X=6 | Y=6\right) \cdot \frac{1}{6}\\
{}={} &amp; \mathbb{E}\left[\mathbb{E}\left(X=6 | Y\right)\right] = \frac{1}{6} \cdot \frac{1}{6} + \frac{1}{6} \cdot \frac{1}{6} + \frac{1}{6}\cdot \frac{1}{6} + \frac{1}{6} \cdot \frac{1}{6} + \frac{1}{6} \cdot \frac{1}{6} + \frac{1}{6} \cdot \frac{1}{6}= \frac{1}{6}
\end{aligned}
\end{equation}\]</span></p>
<p>Im Falle der Würfel liegt übrigens ein Beispiel von <em>mean independence</em> vor,
denn in diesem Fall gilt:</p>
<p><span class="math display">\[\mathbb{E}(X|Y) = \mathbb{E}(X)\]</span></p>
<p>Im Falle des doppelten Wüfelwurfes gilt nämlich, dass
<span class="math inline">\(\mathbb{E}(X=6|Y) = \mathbb{E}(X=6)=\frac{1}{6}\)</span>.</p>
<p>Das Gesetz der wiederholten Erwartungen funktioniert auch bei abhängigen ZV.
Schauen wir noch einmal auf das Beispiel mit der Kälte und dem Schnee und
berechnen wir die Erwartung, dass es schneit:</p>
<p><span class="math display">\[\mathbb{E}(Y=1)=\mathbb{E}\left[\mathbb{E}\left(Y=1 | X\right)\right]=0.15\cdot0.3 + 0.07\cdot0.7=0.094\]</span></p>
<p>Dabei liegt hier <em>keine mean independence</em> vor, denn:
<span class="math inline">\(\mathbb{E}(Y=1)=\mathbb{E}(X=1, Y=1) + \mathbb{E}(X=0, Y=1)=0.22\)</span>,
aber <span class="math inline">\(\mathbb{E}(Y=1|X=0)=0.07\)</span> und <span class="math inline">\(\mathbb{E}(Y=1|X=1)=0.15\)</span>.
Sie können sich leicht merken, dass für abhängige ZV nie, und für unabhängige ZV
immer <em>mean independence</em> bevorliegt (der umgekehrte Fall gilt jedoch nicht!).</p>
<p>Wir werden später im Kontext der Regressionsanalyse noch sehr häufig auf dieses
Gesetz und die Konzepte der marginalen und bedingten Verteilungen zurückkommen.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="63">
<li id="fn63"><p>Wir nennen eine Menge
abzählbar wenn sie mit Hilfe der ganzen Zahlen <span class="math inline">\(\mathbb{N}\)</span> indiziert werden
kann. Das bedeutet, dass auch unendlich große Mengen als abzählbar gelten können.<a href="stat-stoch.html#fnref63" class="footnote-back">↩︎</a></p></li>
<li id="fn64"><p>An der Formel wird noch einmal deutlich, dass wenn <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span>
stochastisch unabhängig sind wir nichts von <span class="math inline">\(B\)</span> über <span class="math inline">\(A\)</span> und umgekehrt lernen
können, also gilt: <span class="math inline">\(\mathbb{P}(A|B)=\mathbb{P}(A)\)</span> und
<span class="math inline">\(\mathbb{P}(B|A)=\mathbb{P}(B)\)</span>.<a href="stat-stoch.html#fnref64" class="footnote-back">↩︎</a></p></li>
<li id="fn65"><p>Aus den
<em>Kolmogorow Axiomen</em> oben ergibt sich, dass die Summe all dieser
Wahrscheinlichkeiten 1 ergeben muss: <span class="math inline">\(\sum_{k\geq 1}\mathbb{P}(X=x_k)=1\)</span>.<a href="stat-stoch.html#fnref65" class="footnote-back">↩︎</a></p></li>
<li id="fn66"><p>Zu
jeder Wahrscheinlichkeitsverteilung gibt es eine
eindeutige Wahrscheinlichkeitsfunktion und jede Wahrscheinlichkeitsfunktion
definiert umgekehrt eine eindeutig bestimmte diskrete
Wahrscheinlichkeitsverteilung.<a href="stat-stoch.html#fnref66" class="footnote-back">↩︎</a></p></li>
<li id="fn67"><p>Die Herleitung finden Sie
im Statistikbuch Ihres Vertrauens oder auf
<a href="https://de.wikipedia.org/wiki/Binomialverteilung#Erwartungswert">Wikipedia</a>.<a href="stat-stoch.html#fnref67" class="footnote-back">↩︎</a></p></li>
<li id="fn68"><p>Die Intervallschreibweise <span class="math inline">\([0,1]\)</span>
ist potenziell verwirrend. Es gilt:
<span class="math inline">\([a,b]=\{x\in\mathbb{R} | a\leq x \leq b\}\)</span> (geschlossenes Intervall),
<span class="math inline">\((a,b)=\{x\in\mathbb{R} | a &lt; x &lt; b\}\)</span> (offenes Intervall),
<span class="math inline">\((a,b)=\{x\in\mathbb{R} | a &lt; x \leq b\}\)</span>(linksoffenes Intervall) und
<span class="math inline">\((a,b)=\{x\in\mathbb{R} | a \leq x &lt; b\}\)</span>(rechtsoffenes Intervall).<a href="stat-stoch.html#fnref68" class="footnote-back">↩︎</a></p></li>
<li id="fn69"><p>Viele Tabellen mit bestimmten Kennzahlen
der Normalverteilung beziehen sich auf die Standard-Normalverteilung. Wenn man
diese Werte verwenden will, muss man die tatsächlich verwendete Stichprobe ggf.
erst <a href="https://de.wikipedia.org/wiki/Standardisierung_(Statistik)">z-transformieren</a>.
Unter Letzterem versteht man die <em>Normalisierung</em> einer ZV sodass sie den
Erwartungswert 0 und die Varianz 1 besitzt. Dies geht i.d.R. für jede ZV <span class="math inline">\(X\)</span> recht
einfach über die Formel <span class="math inline">\(Z=\frac{X-\mu}{\sigma}\)</span>, wobei <span class="math inline">\(Z\)</span> die standardisierte ZV,
<span class="math inline">\(\mu\)</span> den Erwartungswert und <span class="math inline">\(\sigma\)</span> die Standardabweichung von <span class="math inline">\(X\)</span> bezeichnet<a href="stat-stoch.html#fnref69" class="footnote-back">↩︎</a></p></li>
<li id="fn70"><p>Die Alternative, <em>nicht-parametrische</em>
Verfahren, nehmen kein konkretes Wahrscheinlichkeitsmodell an, sondern wählen das
Modell auch auf Basis der Daten.<a href="stat-stoch.html#fnref70" class="footnote-back">↩︎</a></p></li>
<li id="fn71"><p>Trivialerweise ist die Summe einer ZV und einer normalen Zahl
ebenfalls eine ZV. Diese ZV folgt aber einfach der Verteilung der alten ZV, weil
ihr anderer ‘Baustein’ rein deterministisch ist. Dieser Fall ist daher weniger
interessant (oder problematisch, je nach Perspektive).<a href="stat-stoch.html#fnref71" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="formalia.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="desk-stat.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["R-SocioEcon-dt.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
