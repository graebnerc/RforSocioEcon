# Lineare statistische Modelle in R {#linmodel}

```{r include=FALSE}
knitr::opts_chunk$set(comment = "#>")
knitr::opts_chunk$set(out.height = '75%', out.width = '75%', fig.align = 'center') 
```

## Einleitung und Überblick 

### Einführung in die lineare Regression

Zentrales Lernziel dieses Kapitels ist der Umgang mit einfachen linearen Regressionsmodellen in R.
Dabei werden die Inhalte der Kapitel zu Wahrscheinlichkeitstheorie sowie deskriptiver und schließender Statistik als bekannt vorausgesetzt (Kapitel \@ref(stat-stoch), \@ref(desk-stat) und \@ref(stat-rep)). Schauen Sie als erstes in diesen Kapiteln nach wenn Sie ein hier verwendetes Konzept nicht verstehen und konsultieren Sie ansonsten ein Statistiklehrbuch
(und freundliche Kommiliton\*innen) Ihrer Wahl.

In diesem Kapitel werden die folgenden R Pakete verwendet:

```{r, message=FALSE}
library(here)
library(tidyverse)
library(data.table)
library(latex2exp)
library(icaeDesign)
library(ggpubr)
```

Ziel solcher Modelle ist es, ausgehend von einem Datensatz ein lineares Modell zu schätzen. 
Ein solches lineares Modell hat in der Regel die Form

$$Y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon_i$$
und soll uns helfen den linearen Zusammenhang zwischen den Variablen in $x_i$ und $Y_i$ zu verstehen. 
Dazu müssen wir die Parameter $\beta_i$ *schätzen*, denn $\beta_i$ gibt uns Informationen über den Zusammenhang zwischen $x_i$ und $Y_i$.

Sobald wir konkrete Werte für $\beta_i$ geschätzt haben, können wir im Optimalfall von unseren Daten auf eine größere Population schließen und Vorhersagen für zukünftiges Verhalten des untersuchten Systems treffen. Damit das funktioniert, müssen jedoch einige Annahmen erfüllt sein, und in diesem Kapitel geht es nicht nur darum, die geschätzten Werte $\hat{\beta}_i$ zu 
identifizieren, sondern auch die der Regression zugrundeliegenden Annahmen zu überprüfen.

Bevor wir uns Schritt für Schritt mit der Regression auseinandersetzen, wollen wir uns noch ein konkretes Beispiel anschauen. 

```{r, echo=FALSE}
daten <- data.frame(
  Konsum = c(3081.5, 3240.6, 3407.6, 3566.5, 3708.7, 3822.3, 3972.7, 4064.6, 
             4132.2, 4105.8, 4219.8, 4343.6, 4486, 4595.3, 4714.1),
  BIP = c(4620.3, 4803.7, 5140.1, 5323.5, 5487.7, 5649.5, 5865.2, 6062, 6136.3, 
          6079.4, 6244.4, 6389.6, 6610.7, 6742.1, 6928.4)
) 
```

### Einführungsbeispiel

> **Beispiel: Konsum und Nationaleinkommen** Wir sind daran interessiert wie zusätzliches Einkommen auf die Konsumausgaben in einer Volkswirtschaft auswirken. Daher stellen wir folgendes Modell auf:

$$C_i = \beta_0 + \beta_1 Y_i + \epsilon_i$$

> wobei $C_i$ für die Konsumausgaben und $Y_i$ für das BIP steht. 
Diese Gleichung stellt unser statistisches Modell dar. Es hat
zwei Parameter, $\beta_0$ und $\beta_1$, die wir mit Hilfe unserer Daten schätzen möchten. Wir laden uns also Daten zum Haushaltseinkommen und zum BIP aus dem Internet herunter und inspizieren die Daten zunächst visuell:

<!-- Discuss: Mmmh ich frag mich ob das nicht etwas zirkulär wäre hierfür ein statistisches Modell zu machen & Parameter zu schätzen, weil C ja in der Verwendungsrechnung ein Bestandteil vom BIP ist. Also wir schätzen C mit Hilfe des BIPs obwohl wir C brauchen um das BIP zu ermitteln, nicht? -->

```{r, echo=FALSE}
ggplot2::ggplot(daten, aes(y=Konsum, x=BIP)) + geom_point() + 
  ggplot2::ggtitle("Konsumausgaben und BIP") + theme_icae() 
```

> Der Zusammenhang scheint gut zu unserem linearen Modell oben zu passen, sodass wir das Modell mit Hilfe der Daten schätzen um konkrete Werte für $\beta_0$ und
$\beta_1$ zu identifizieren:

```{r}
schaetzung_bip <- lm(Konsum~BIP, data = daten)
schaetzung_bip
```
<!-- Ich glaub hier wäre es gar nicht schlecht kurz die Funktion lm (linear model denk ich mal?) einzuführen und zu erklären was das Tilde Symbol macht, also einfach zu erklären wie die Funktion hier aufgebaut ist. Oder soll das erst später kommen weil es jetzt erst um das Beispiel geht, nicht um die technicalities? Dann vllt ECHO=false bei dem R chunk und hier erstmal nur die Ergebnisse anzeigen? -->

```{r, echo=FALSE, warning=FALSE}
ggplot2::ggplot(daten, aes(y=Konsum, x=BIP)) + 
  ggplot2::geom_abline(intercept = schaetzung_bip[["coefficients"]][1],
              slope = schaetzung_bip[["coefficients"]][2], alpha=.5) +
  ggplot2::geom_point() + 
  ggplot2::annotate(geom = "text", hjust=0,
           label = TeX("Steigung der Geraden: $\\hat{\\beta}_1=0.71$"), x=5600, y=3750) +
  ggplot2::ggtitle("Konsumausgaben und BIP") + 
  ggplot2::theme_bw() 
```

> In dieser Abbildung korrespondiert $\beta_0$ zum Achensabschnitt und $\beta_1$ zur Steigung der Konsumgerade. Wir können $\beta_0$ als die Konsumausgaben interpretieren, wenn das BIP Null betragen würde, und $\beta_1$ als die marginale Konsumquote, also den Betrag, um den die Konsumausgaben steigen, wenn das BIP um einen Euro steigt.
Die geschätzten Werte für $\beta_0$ und $\beta_1$ sind hier $-184$ und $0.7$. Auf dieser Basis können wir auch ausrechnen, wie hoch die Konsumausgaben in einer Volkswirtschaft mit einem BIP von 8000 wäre, indem wir uns einfach an der geschätzten Gerade bis zu diesem Betrag fortbewegen. 

```{r}
beta_0 <- schaetzung_bip[["coefficients"]][1]
beta_1 <- schaetzung_bip[["coefficients"]][2]
unname(beta_0 + beta_1*8000)
```

> Im aktuellen Beispiel wären das also ```r round(unname(beta_0 + beta_1*8000), 2)``` Euro.

### Überblick über die Inhalte des Kapitels

Im Folgenden werden wir uns zunächst mit den 
[formalen Grundlagen](#lin-grundlagen) der linearen
Einfachregression, also der Regression mit einer $x$-Variable, und ihrer Implementierung in R beschäftigen.
Insbesondere wird die Methode der kleinsten Quadrate (OLS) und die dafür notwendigen Annahmen eingeführt.

Danach werden wir typische [Kennzahlen einer Regression](#lin-kennzahlen) diskutieren und lernen, wie wir die Güte einer Regression beurteilen können.
Dieser Abschnitt enthält Auführungen zum $R^2$, Standardfehlern von Schätzern, Konfidenzintervallen und Residuenanalysen. 
Vieles ist eine Anwendung der in Kapitel \@ref(stat-rep)
beschriebenen Konzepte zu schließender Statistik.

Nachdem wir den [Ablauf einer Regressionsanalyse](#stat-ablauf) kurz zusammengefasst haben, generalisieren wir das Gelernte noch für den [multiplen Fall](#lin-multi), also den Fall wenn wir mehr als eine $x$-Variable in unserem Modell verwenden.

Am Schluss finden Sie ein konkretes [Anwendungsbeispiel](#lin-beispiel), bei dem 
wir eine lineare Regression von Anfang an implementieren. 
(*Hinweis: dieser Abschnitt wird später ergänzt*)

<!-- Discuss: Ist das noch aktuell? was fehlt hier noch? -->


## Grundlagen der einfachen linearen Regression {#lin-grundlagen}

### Grundlegende Begriffe

Wir betrachten zunächst den Fall der einfachen linearen Regression, das heißt wir untersuchen den Zusammenhang zwischen zwei Variablen, sodass unser theoretisches Modell folgendermaßen aussieht:

$$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

Alles was auf der linken Seite vom `=` steht bezeichnen wir als die LHS 
(engl. *left hand side*), alles auf der rechten Seite als RHS 
(engl. *right hand side*).

Wir bezeichnen $Y_i$ als die **abhängige Variable** 
(auch: *Zielvariable* oder *erklärte Variable*). 
Das ist die Variable, die wir typischerweise erklären wollen.
Im Eingangbeispiel waren das die Konsumausgaben.

Wir bezeichnen $x_i$ als die **unabhängige Variable** (auch: *erklärende Variable*).
Das ist die Variable, mit der wir die abhängige Variable erklären wollen.
Im Eingangsbeispiel war das das BIP, denn wir wollten über das BIP erklären wie viel Geld in einem Land für Konsum ausgegeben wird.

Jetzt ist es natürlich so, dass wir die erklärenden Variablen nie ganz genau beobachten können. Beim BIP sind z.B. Messfehler unvermeidlich, und auch ansonsten wird es sicher einige Unvollkommenheiten im Zusammenhang zwischen
der erkärenden Variable und der abhängigen Variable geben.
Um der Tatsäche Rechnung zu tragen, dass der Zusammenhang zwischen $x_i$ und $Y_i$ nicht exakt ist, führen wir auf der rechten Seite der Gleichung noch die **Fehlerterme** $\epsilon_i$ ein.

<!-- Geht es hier nicht auch darum, dass die Realität auch stochastisch ist? Wenn es nur Messfehler gäbe, dann hieße das ja dass, wenn wir genau messen könnten, wir genau vorhersagen könnten, aber in einem nicht-deterministischen Universum ist das ja nicht der Fall. -->

Wir müssen für unser Modell annehmen, dass die Fehlerterme nur einen nicht-systematischen Effekt auf $Y_i$ haben, ansonsten müssten wir sie explizit in unser Modell als erklärende Variable aufnehmen (dazu später mehr).
Sie absorbieren quasi alle Einflüsse auf $Y_i$, die nicht über $x_i$ wirken. Damit wir die Funktion richtig schätzen können nehmen wir für die Fehler ein bestimmtes Wahrscheinlichkeitsmodell an. In der Regel nimmt man an, die Fehler
seien i.i.d. (identically and independently distributed)^[d.h. die Fehler sind unabhängig voneinander und folgen alle der gleichen Verteilung.] normalverteilt mit Erwartungswert 0: 
$\epsilon_i \ i.i.d. \propto \mathcal{N}(0, \sigma^2)$.

Nun ergibt auch die Groß- und Kleinschreibung in der Gleichung mehr Sinn: die $x_i$ nehmen wir als beobachtete Größen hin und behandeln sie nicht als Zufallsvariablen (ZV).^[Wenn Sie Schwierigkeiten mit dem Konzept einer ZV haben, schauen Sie doch noch einmal in Kapitel \ref(stat-stoch) nach.]
Die $\epsilon_i$ sind als ZV definiert und da wir $Y_i$ als eine Funktion von $x_i$ und $\epsilon_i$ interpretieren sind die $Y_i$ auch ZV - und dementsprechend groß geschrieben. Die Fehlerterme werden per Konvention nie groß geschrieben - 
wahrscheinlich weil sich das für Fehler nicht gehört. Wer es ganz genau nehmen würde, müsste sie aber auch groß schreiben, denn sie sind als ZV definiert und diese werden eigentlich groß geschrieben.

Die Annahme von $\mathbb{E}(\epsilon_i)=0$, also die Annahme, dass der Erwartungswert für jeden Fehler gleich Null ist, ist neben der Annahme, dass wir einen linearen Zusammenhang modellieren zentral: wir gehen davon aus, dass unser Modell im Mittel stimmt.
Unter dieser Annahme gibt es keine *systematischen* Abweichungen der $Y_i$ von
der über $\beta_0$ und $\beta_1$ definierten Regressionsgeraden.
Das ist allerding nur der Fall, wenn bestimmte Annahmen erfüllt sind (dazu später mehr).

### Schätzung mit der Kleinste-Quadrate-Methode

Nachdem wir unser Modell aufgestellt haben, möchten wir nun die Parameter $\beta_0$
und $\beta_1$ *schätzen*. 
Wir schätzen diese Werte, denn sie sind für uns nicht unmittelbar beobachtbar: Wir brauchen also einen *Schätzer*.
Ein Schätzer ist eine Funktion, die uns für die Daten, die
wir haben, den optimalen Wert für den gesuchten Parameter gibt.^[Wenn Ihnen das 
Konzept eines Schätzers sehr fremd ist, schauen Sie noch mal in das Kapitel @ref(stat-rep) zu schließender Statistik.]
Wir suchen also nach den Werten für $\beta_0$ und $\beta_1$ sodass die resultierende Gerade möglichst nahe an allen $Y_i$ Werten ist, wie in Abbildung \@ref(fig:OLSGerade) aufgezeigt.

<!-- Discuss: soll ich hier die icae_colors rausnehmen? Wird ja eigl nicht gezeigt weil echo=False deshalb einfach drinlassen? -->

```{r OLSGerade, echo=FALSE, warning=FALSE, fig.cap="OLS Gerade als jene Gerade, welche den Abstand zu den quadrierten Residuen minimiert."}
set.seed(123)
wahres_b0 <- 3
wahres_b1 <- 1.4
cols_used <- get_icae_colors(2, palette_used = "mixed")

stichproben_n <- 10
x <- 1:stichproben_n * 0.1
fehler <- rnorm(stichproben_n, mean = 0, sd = 1)
y <- rep(NA, stichproben_n)

for (i in 1:stichproben_n){
  y[i] <- wahres_b0 + wahres_b1*x[i] + fehler[i]
}
datensatz <- data.frame(
  x = x,
  y = y
)
ggplot2::ggplot(datensatz, aes(x=x, y=y)) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(aes(color="Geschätzte Werte", 
                  intercept = wahres_b0, 
                  slope = wahres_b1), 
              alpha=0.75, show.legend = TRUE) +
  scale_color_manual(values = c("Geschätzte Werte"=cols_used[1], 
                                "Wahre Werte"=cols_used[2])
                     ) + 
  ggplot2::geom_segment(aes(x = x[6], y = y[6],
                   xend = x[6], yend=y[6]-fehler[6]),
               arrow = arrow(length = unit(0.03, "npc"), ends = "both"),
               colour = "#800080") +
  ggplot2::annotate(geom = "text", 
           label=TeX("$r_6=Y_6 - \\beta_0 - \\beta_1 x_6$"), 
           x = x[6]-0.09, 
           y = y[6]-0.75, 
           colour = "#800080") +
  icaeDesign::theme_icae() + 
  ggplot2::theme(legend.position = "none")
```

Wenn wir das händisch machen würden, könnten wir versuchen die Abstände zwischen den einzelnen $Y_i$ und der Regressionsgerade zu messen und letztere so lange herumschieben, bis die Summe der Abstände möglichst klein ist. In gewisser Weise ist das genau das, was wir in der Praxis auch machen. Nur arbeiten wir nicht mit den Abständen als solchen, denn dann würden sich positive und negative Abstände ja ausgleichen. 
Daher quadrieren wir die Abstände, bevor wir sie summieren.
Daher ist die gängigste Methode, Werte für $\beta_0$ und $\beta_1$ zu finden auch als **Kleinste-Quadrate Methode** (engl. *ordinary least squares* - OLS) bekannt.^[Warum summiert man nicht die Absolutwerte der Abweichungen, sondern ihre quadrierten Werte? Das hat technische Gründe: mit quadrierten Werten lässt
sich einfach leichter rechnen als mit Absolutwerten.]
Die dadurch definierten Schätzer $\hat{\beta}_0$ und $\hat{\beta}_1$ sind entsprechend als *OLS-Schätzer* bekannt.

Wir bezeichnen die Abweichung von $Y_i$ zu Regressionsgeraden als *Residuum* $e_i$. Wie in der Abbildung zu sehen ist, gilt für die Abweichung von der Regressionsgeraden für die einzelnen $Y_i$: $e_i=(Y_i-\hat{\beta}_0-\hat{\beta}_1x_i)$. Wir suchen also nach den Werten für $\hat{\beta}_0$ und $\hat{\beta}_1$, für die die Summe aller Residuen minimal ist:

$$\hat{\beta}_0, \hat{\beta}_1 =\text{argmin}_{\beta_0, \beta_1} \sum_{i=1}^n(Y_i-\beta_0-\beta_1x_i)^2$$

Dabei bedeutet $\text{argmin}_{\beta_0, \beta_1}$: wähle die Werte für $\beta_0$ und $\beta_1$, welche den nachfolgenden Ausdruck minimieren.

Diesen Ausdruck kann man analytisch so lange umformen bis gilt:^[Jede\*r Interessierte
findet die genaue Herleitung im Kapitel zu [linearen Algebra](#ols-deriv).]

$$\hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}$$
und

$$\hat{\beta_0}=\bar{y}-\hat{\beta}_1\bar{x}$$

Zum Glück gibt es in R die Funktion `lm()`, welche diese Berechnungen für uns übernimmt. Wir wollen dennoch anhand eines Minimalbeispiels die Werte selber schätzen, um unser Ergebnis dann später mit dem Ergebnis von `lm()` zu vergleichen.

<!-- Ahja, hier wird die Funktion eingeführt. Dann würde ich bei meinem Kommentar oben zu lm dafür plädieren, den chunk einfach als echo=FALSE zu setzen. -->

Dazu betrachten wir folgenden (artifiziellen) Datensatz:

```{r, echo=FALSE}
set.seed(123)
wahres_b0 <- 3
wahres_b1 <- 1.4

stichproben_n <- 5
x <- 1:stichproben_n * 0.1
fehler <- rnorm(stichproben_n, mean = 0, sd = 1)
y <- rep(NA, stichproben_n)

for (i in 1:stichproben_n){
  y[i] <- wahres_b0 + wahres_b1*x[i] + fehler[i]
}
datensatz <- data.frame(
  x = x,
  y = round(y, 2)
)
```
```{r}
datensatz
```

Zuerst berechnen wir $\hat{\beta}_1$:

$$\hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}$$

Dazu brauchen wir zunächst $\bar{x}$, das ist in diesem Fall ```r mean(datensatz$x)```,
und $\bar{y}$, in unserem Fall ```r mean(datensatz$y)```.
Dann können wir bereits rechnen:

$$\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=(0.1-0.3)(2.58-3.614)+(0.2-0.3)(3.05-3.614)\\+(0.3-0.3)(4.98-3.614)+(0.4-0.3)(3.63-3.614)+(0.5-0.3)(3.83-3.614)=0.308$$
und 

$$\sum_{i=1}^n(x_i-\bar{x})^2=(0.1-0.3)^2+(0.2-0.3)^2+(0.3-0.3)^2+(0.4-0.3)^2+(0.5-0.3)^2=0.1$$
Daher:

$$\hat{\beta_1}=\frac{0.308}{0.1}=3.08$$

Entsprechend ergibt sich für $\hat{\beta}_0$:

$$\hat{\beta_0}=\bar{y}-\hat{\beta}_1\bar{x}=3.614-3.08\cdot 0.3=2.69$$

In R können wir für diese Rechnung wie gesagt die Funktion `lm()` verwenden. In der Praxis sind für uns vor allem die folgenden zwei Argumente von `lm()` relevant: `formula` und `data`.

Über `data` informieren wir `lm` über den Datensatz, der für die Schätzung verwendet werden soll. Dieser Datensatz muss als `data.frame` oder vergleichbares Objekt vorliegen.

Über `formula` teilen wir `lm` dann die zu schätzende Formel mit.
Die LHS und RHS werden dabei mit dem Symbol `~` abgegrenzt.
Wir können die Formel entweder direkt als `y~x` an `lm()` übergeben, oder wir speichern sie vorher als `character` und verwenden die Funktion `as.formula()`.
Entsprechend sind die folgenden beiden Befehle äquivalent:

```{r, eval=FALSE}
lm(y~x, data = datensatz)
reg_formel <- as.formula("y~x")
lm(reg_formel, data = datensatz)
```

Der Output von `lm()` ist eine Liste mit mehreren interessanten Informationen:

```{r}
schaetzung <- lm(y~x, data = datensatz)
typeof(schaetzung)
schaetzung
```

Die von `lm()` produzierte Liste enthält also die basalsten Informationen über unsere Schätzung. Wir sehen unmittelbar, dass wir vorher richtig gerechnet haben, da wir die gleichen Werte herausbekommen haben.

Wenn wir noch genauer wissen wollen, wie die Ergebnisliste aufgebaut ist, können wir die Funktion `str()` verwenden:

```{r, eval=FALSE}
str(schaetzung)
```

Da die Liste aber tatsächlich sehr lang ist, wird dieser Code hier nicht ausgeführt. Es sei aber darauf hingewiesen, dass wir die geschätzen Werte auf folgende Art und Weise direkt ausgeben lassen können:

```{r}
schaetzung[["coefficients"]]
```

Dies ist in der Praxis häufig nützlich, z.B. wenn wir wie in der Einleitung Werte mit Hilfe unseres Modell vorhersagen wollen.
Zum Abschluss sehen wir in Abbildung \@ref(fig:BerechnungOLS) die Daten mit der von uns gerade berechneten Regressionsgeraden.

```{r BerechnungOLS, echo=FALSE, warning=FALSE, fig.cap="Regressionsgerade mit den berechneten Parameterwerten."}
ggplot2::ggplot(datensatz, aes(x=x, y=y)) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(aes(color="Geschätzte Werte", 
                  intercept = schaetzung[["coefficients"]][1], 
                  slope = schaetzung[["coefficients"]][2]), 
              alpha=0.75, show.legend = TRUE, color=get_icae_colors("dark blue")) +
  ggplot2::scale_x_continuous(limits = c(0, 0.55), expand = c(0, 0)) +
  ggplot2::geom_segment(aes(x = 0.1, y = 3.5,
                   xend = 0, yend=schaetzung[["coefficients"]][1]),
               arrow = arrow(length = unit(0.03, "npc")),
               colour =  get_icae_colors("purple")) +
  ggplot2::annotate(geom = "text", 
           label=TeX("$\\beta_0 = 2.69$"), 
           x = 0.1, y = 3.6,
           colour = get_icae_colors("purple")) +
    ggplot2::annotate(geom = "text", 
           label=TeX("Steigung: $\\beta_1 = 3.08$"), 
           x = 0.28, y = 3.8,
           colour = get_icae_colors("purple")) +
  icaeDesign::theme_icae() + 
  ggplot2::theme(legend.position = "none")
```


Zwar wissen wir jetzt, wie wir eine einfache lineare Regression schätzen, allerdings hört die Arbeit hier nicht auf! 
Unsere bisherige Tätigkeiten korrespondieren zu der in Kapitel @ref(stat-rep) beschriebenen *Parameterschätzung*.
Wir wollen aber auch noch die anderen beiden Verfahren, Hypothesentests und Konfidenzintervalle, 
abdecken und lernen, wie wir die Güte unserer Schätzung besser einschätzen können.

Zuvor wollen wir aber noch einmal genauer überprüfen, welche Annahmen genau erfüllt sein müssen, damit die OLS-Prozedur auch funktioniert.

### Annahmen für den OLS Schätzer {#ols-ass}

<!--Ich bin versucht diesen Teil rauszunehmen oder nur sehr kurz zu halten, und vor allem auf das nächste Kapitel zu verweisen. Oder aber ein informelle Beschreibung der Annahmen so wie sie dort eingeführt werden um die Studis nicht zu verwirren. -->
<!-- Antwort Birte: Okay, ich achte im nächsten Kapitel darauf, was sich hier doppelt :) Denke aber auch, dass ggf die Gauss-Markov Sachen hier rausgenommen werden könnten wenn wir weiter oben einen Hinweis setzen, dass die genauen Annahmen die erfüllt werden müssen im folgenden Kapitel behandelt werden. -->

Das lineare Regressionsmodell wird sehr häufig in der sozioökonomischen Forschung verwendet. 
Wie jedes statistisches Modell basiert es jedoch auf bestimmten Annahmen, aus denen sich der sinnvolle Anwendungsbereich des Modells ergibt. Wann immer wir die lineare Regression verwenden sollten wir daher kritisch prüfen ob die entsprechenden Annahmen für den Anwendungsfall plausibel sind. Daher wollen wir uns im Folgenden etwas genauer mit diesen Annahmen vertraut machen. 

Zwar schwankt die genaue Anzahl der Annahmen je nach Formulierung, in der Essenz handelt es sich aber um Folgende:

1. **A1: Erwartungswert der Fehler gleich Null**: $\mathbb{E}(\epsilon=0)$ 
Diese Annahme setzt voraus, dass $\epsilon$ keine Struktur hat und im Mittel gleich Null ist. Das ist plausibel, denn würden wir Informationen über eine Struktur in $\epsilon$ haben, bedeutet das, dass wir eine weitere erklärende Variable in das Modell aufnehmen könnten, welche diese Struktur explizit macht, oder die lineare Strukur des Modells ändern könnten. 
Die Annahme impliziert auch, dass der Zusammenhang zwischen der erklärten und erklärenden Variablen auch tatsächlich linear ist. Grob ausgedrückt: wir nehmen hier an, dass wir unser Modell clever spezifiziert haben.^[Es ist wichtig, dass wir hier eine Annahme über eine unbeobachtbare Größe der Population treffen, nicht
über die Residuen $e_i$ unserer Regression. Die Residuen $e_i$ können wir beobachten, die echten Fehler $\epsilon_i$ nicht.]

<!-- Ich würde evtl diesen Unterschied zw Residuen und Fehlern aus der Fußnote raus und in den Text reinnehmen, da der ja shcon recht wichtig ist und zumindest mir aber lange nicht offensichtlich klar war- hab vor 2 Wochen noch gegoogelt was das nochmal genau war haha -->

2. **A2: Unabhängigkeit der Fehler mit den erklärenden Variablen**: Das bedeutet, dass es keinen systematischen Zusammenhang zwischen den Fehlern und den erklärenden Variablen gibt. Die Annahme wäre verletzt, wenn für größere Werte von $x$ die Messgenauigkeit drastisch in eine Richtung hin abnehmen würde. 

3. **A3: Konstante Varianz der Fehlerterme** Diese Annahme wird auch **Homoskedastizität** genannt und sagt einfach: $Var(\epsilon_i)=\sigma^2\forall i$

<!-- Würde hier vielleicht nochmal erklären weshalb das wichtig ist, gerade weil ich das ganze Thema Heteroskedastizität irgendwie kompliziert finde obwohl ich es schon 1000mal gelernt habe. -->

4. **A4: Keine Autokorrelation der Fehlerterme** Die Annahme verlangt, dass die Fehler nicht untereinander korreliert sind: 
$Cov(\epsilon_i, \epsilon_j)=0 \forall i,j$. Das kann vor allem ein
Problem sein, wenn die gleichen erklärenden Variablen zu unterschiedlichen Zeitpunkten gemessen werden.

5. **A5: Keine perfekte Multikollinearität** Diese Annahme verbietet den Fall, dass eine der erklärenden Variablen eine lineare Transformation einer anderen erklärenden Variable ist, also $\nexists a,b:   x_i= q+b\cdot x_j \forall i,j$.
<!-- (Wäre das hier nicht der Fall bei BIP und bei C im Eingangsbeispiel oben?) -->

Praktisch tritt dieser Fall nur selten auf. In diesem Fall ist $\hat{\beta}$ schlicht nicht definiert. Problematisch wird es aber schon wenn eine erklärende Variable *fast* eine lineare Transformation einer anderen ist. Als generellen *take-away* können wir mitnehmen, dass wir in den erklärenden Variablen möglichst
wenig Redundanz haben sollten.

6. **A6: Normalverteilung der Fehlerterme:** $\epsilon\propto\mathcal{N}(0,\sigma^2)$ 
Diese Annahme ist notwendig für die Hypothesentests und Berechnung von Konfidenzintervallen für die Schätzer. 

Diese Annahmen bilden die Grundlage für das so genannte **Gauss-Markov-Theorem**, gemäß dem der OLS-Schätzer für lineare Modelle der beste konsistente Schätzer ist, den wir finden können. In anderen Worten: OLS ist der BLUE - 
*Best Linear Unbiased Estimator*. 
Mit konsistent ist gemeint, dass die Schätzer im Mittel den wahren Wert $\beta_i$ treffen, also der Erwartungswert jedes Schätzers $\hat{\beta}_i$ der wahre Werte $\beta$ ist. Mit "der beste" meinen wir "den effizientesten" im Sinne einer minimalen Varianz. Was mit der Varianz eines Schätzers gemeint wird, wird weiter unten erklärt.

Dabei gilt, dass der OLS-Schätzer bereits unter den Annahmen **A1** und **A2** erwartungstreu ist. Die weiteren Annahmen sind notwendig um die Effizienz sicherzustellen, und die Standardfehler für die Schätzer berechnen zu können. 
Es gibt Varianten von OLS in der man die Abhängigkeit von diesen Annahmen reduzieren kann. Wir lernen solcherlei Methoden später in der Veranstaltung kennen.

Das bedeutet aber auch, dass wann immer eine oder mehrere Annahmen verletzt ist, wir unseren Ergebnissen nur bedingt vertrauen können und einige Ergebnisse und Kennzahlen unserer Regression möglicherweise irreführend sind. Daher sollte zu 
jeder Regressionsanalyse die Überprüfung der Annahmen dazugehören. 
Die notwendigen Methoden dazu lernen wir weiter unten kennen.

<!-- Aber A6 zum Beispiel kann man doch nicht prüfen oder? -->

Es ist wichtig zu beachten, dass wir eine Regression mit OLS schätzen können und keine Fehlermeldungen bekommen, obwohl die Annahmen für OLS nicht erfüllt sind.^[
Die einzige Ausnahme ist A5, denn in diesem Fall ist der Schätzer gar nicht definiert.]
In diesem Fall sind die Ergebnisse jedoch möglicherweise irreführend. Daher ist es immer wichtig, die Korrektheit der Annahmen zu überprüfen und weitere Kennzahlen der Regression zu betrachten Methoden dafür lernen wir nun genauer kennen.

## Kennzahlen in der linearen Regression {#lin-kennzahlen}

### Erklärte Varianz und das $R^2$

Als erstes wollen wir fragen, 'wie gut' unser geschätztes Modell unsere Daten erklären kann. 
In der ökonometrischen Praxis können wir dazu fragen,
wie viel 'Variation' der abhängigen Variable $Y_i$ durch die Regression erklärt wird. 
Als Maß für die Variation wird dabei die Summe der quadrierten Abweichungen von $Y_i$ von seinem Mittelwert verwendet, auch $TSS$ (für engl. *Total Sum of Squares* - 'Summe der Quadrate der Totalen Abweichungen') genannt:

$$TSS=\sum_{i=1}^n(Y_i-\bar{Y})^2$$
In R:

```{r}
tss <- sum((datensatz$y - mean(datensatz$y))**2)
tss
```

Diese Werte sind in Abbildung \@ref(fig:TSS) für unseren Beispieldatensatz von 
oben grafisch dargestellt:

```{r TSS, echo=FALSE, warning=FALSE, fig.cap="Werte für die Summe der totalen Abweichungen (TSS)."}
ggplot2::ggplot(datensatz, aes(x=x, y=y)) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(aes(color="Geschätzte Werte", 
                  intercept = mean(datensatz$y), 
                  slope = 0), 
              alpha=0.75, show.legend = TRUE, color=get_icae_colors("dark blue")) +
  ggplot2::scale_x_continuous(limits = c(0, 0.55), expand = c(0, 0)) +
  ggplot2::geom_segment(aes(x = 0.1, y = 3.9,
                   xend = 0, yend=mean(datensatz$y)),
               arrow = arrow(length = unit(0.03, "npc")),
               colour =  get_icae_colors("dark blue")) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[1], y = datensatz$y[1],
      xend = datensatz$x[1], yend=mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark red")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[2], y = datensatz$y[2],
      xend = datensatz$x[2], yend=mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark red")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[3], y = datensatz$y[3],
      xend = datensatz$x[3], yend=mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark red")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[4], y = datensatz$y[4],
      xend = datensatz$x[4], yend=mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark red")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[5], y = datensatz$y[5],
      xend = datensatz$x[5], yend=mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark red")
      ) +
  ggplot2::annotate(geom = "text", 
           label=TeX("$\\bar{Y} = 3.614$"), 
           x = datensatz$x[1], y = 4,
           colour = get_icae_colors("dark blue")) +
    ggplot2::annotate(geom = "text", 
           label=TeX("$Y_1-\\bar{Y} = -1$"), 
           x = datensatz$x[1]-0.05, y = 3,
           colour = get_icae_colors("dark red")) +
      ggplot2::annotate(geom = "text", 
           label=TeX("$Y_2-\\bar{Y} = -0.56$"), 
           x = datensatz$x[2]-0.038, y = 3.25,
           colour = get_icae_colors("dark red")) +
      ggplot2::annotate(geom = "text", 
           label=TeX("$Y_3-\\bar{Y} = 1.4$"), 
           x = datensatz$x[3]-0.038, y = 4.45,
           colour = get_icae_colors("dark red")) +
      ggplot2::annotate(geom = "text", 
           label=TeX("$Y_4-\\bar{Y} = 0.02$"), 
           x = datensatz$x[4]-0.038, y = 3.7,
           colour = get_icae_colors("dark red")) +
      ggplot2::annotate(geom = "text", 
           label=TeX("$Y_5-\\bar{Y} = 0.22$"), 
           x = datensatz$x[5]-0.038, y = 3.75,
           colour = get_icae_colors("dark red")) + 
      ggplot2::annotate(geom = "text", 
           label=TeX("$TSS = 3.3$"), 
           x = 0.45, y = 3.0,
           colour = get_icae_colors("dark red")) +   
  icaeDesign::theme_icae() + 
  ggplot2::theme(legend.position = "none")
```

Die TSS wollen wir nun aufteilen in eine Komponente, die in unserer Regression erklärt wird, und eine Komponente, die nicht erklärt werden kann. Bei letzterer handelt es sich um die Abweichungen der geschätzten Werte $\hat{Y_i}$ und den tatsächlichen Werten $Y_i$, den oben definierten Residuen $e_i$.
Entsprechend definieren wir die *Residual Sum of Squares (RSS)* 
(dt.: *Residuenquadratsumme*) als:

$$RSS=\sum_i^ne_i^2$$
In R:

```{r}
rss <- sum(schaetzung[["residuals"]]**2)
rss
```


Diese sehen wir in Abbildung \@ref(fig:RSS).

```{r RSS, echo=FALSE, warning=FALSE, fig.cap="Abweichungen der geschätzen Werten und den tatsächlichen Werten, i.e. den Residuen (RSS)."}
ggplot2::ggplot(datensatz, aes(x=x, y=y)) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(aes(color="Geschätzte Werte", 
                  intercept = schaetzung[["coefficients"]][1], 
                  slope = schaetzung[["coefficients"]][2]), 
              alpha=0.75, show.legend = TRUE, color=get_icae_colors("dark blue")) +
  ggplot2::scale_x_continuous(limits = c(0, 0.55), expand = c(0, 0)) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[1], y = datensatz$y[1],
      xend = datensatz$x[1], yend=schaetzung$fitted.values[1]
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[2], y = datensatz$y[2],
      xend = datensatz$x[2], yend=schaetzung$fitted.values[2]
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[3], y = datensatz$y[3],
      xend = datensatz$x[3], yend=schaetzung$fitted.values[3]
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[4], y = datensatz$y[4],
      xend = datensatz$x[4], yend=schaetzung$fitted.values[4]
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[5], y = datensatz$y[5],
      xend = datensatz$x[5], yend=schaetzung$fitted.values[5]
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::annotate(geom = "text", 
           label=TeX("$\\epsilon_1$"), 
           x = datensatz$x[1]-0.007, y = 2.75,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text", 
           label=TeX("$\\epsilon_2$"), 
           x = datensatz$x[2]-0.008, y = 3.15,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text", 
           label=TeX("$\\epsilon_3$"), 
           x = datensatz$x[3]-0.008, y = 4.45,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text", 
           label=TeX("$\\epsilon_4$"), 
           x = datensatz$x[4]-0.008, y = 3.8,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text", 
           label=TeX("$\\epsilon_5$"), 
           x = datensatz$x[5]-0.008, y = 3.95,
           colour = get_icae_colors("dark blue")) + 
      ggplot2::annotate(geom = "text", 
           label=TeX("$RSS = 2.35148$"), 
           x = 0.45, y = 3.0,
           colour = get_icae_colors("dark blue")) +   
  icaeDesign::theme_icae() + 
  ggplot2::theme(legend.position = "none")
```

Was noch fehlt sind die *Explained Sum of Squares (ESS)* 
(dt. *Summe der Quadrate der Erklärten Abweichungen*), also die Variation in der abhängigen Variable, die durch die Regression erklärt wird. Dabei handelt es sich um die quadrierte Differenz zwischen $\bar{Y}$ und den geschätzten Werten $\hat{Y}$:

$$ESS=\sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2$$
Diese ergibt sich in R als:

```{r}
ess <- sum((schaetzung[["fitted.values"]] - mean(datensatz$y))**2)
ess
```

Und grafisch wie in Abbildung \@ref(fig:ESS) beschrieben.

```{r ESS, warning=FALSE, echo=FALSE, fig.cap="Variation in der abhängigen Variable, die durch die Regression erklärt wird (ESS)."}
ggplot2::ggplot(datensatz, aes(x=x, y=y)) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(aes(color="Geschätzte Werte", 
                  intercept = schaetzung[["coefficients"]][1], 
                  slope = schaetzung[["coefficients"]][2]), 
              alpha=0.75, show.legend = TRUE, color=get_icae_colors("dark blue")) +
    ggplot2::geom_abline(aes(color="Geschätzte Werte", 
                  intercept = mean(datensatz$y), 
                  slope = 0), 
              alpha=0.75, show.legend = TRUE, color=get_icae_colors("dark red")) +
    ggplot2::geom_segment(aes(x = 0.1, y = 3.9,
                   xend = 0, yend=mean(datensatz$y)),
               arrow = arrow(length = unit(0.03, "npc")),
               colour =  get_icae_colors("dark red")) +
    ggplot2::annotate(geom = "text", 
           label=TeX("$\\bar{Y} = 3.614$"), 
           x = datensatz$x[1], y = 4,
           colour = get_icae_colors("dark red")) +
  ggplot2::scale_x_continuous(limits = c(0, 0.55), expand = c(0, 0)) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[1], y = schaetzung$fitted.values[1],
      xend = datensatz$x[1], yend = mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[2], y = schaetzung$fitted.values[2],
      xend = datensatz$x[2], yend = mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[3], y = schaetzung$fitted.values[3],
      xend = datensatz$x[3], yend = mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[4], y = schaetzung$fitted.values[4],
      xend = datensatz$x[4], yend = mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::geom_segment(aes(
      x = datensatz$x[5], y = schaetzung$fitted.values[5],
      xend = datensatz$x[5], yend = mean(datensatz$y)
      ),
      arrow = arrow(length = unit(0.01, "npc"), ends = "both"),
      colour =  get_icae_colors("dark blue")
      ) +
    ggplot2::annotate(geom = "text",
           label=TeX("$\\hat{Y}_1-\\bar{Y}$"),
           x = datensatz$x[1]-0.017, y = 3.25,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text",
           label=TeX("$\\hat{Y}_2-\\bar{Y}$"),
           x = datensatz$x[2]-0.018, y = 3.4,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text",
           label=TeX("$\\hat{Y}_3-\\bar{Y}$"),
           x = datensatz$x[3]-0.018, y = 3.7,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text",
           label=TeX("$\\hat{Y}_4-\\bar{Y}$"),
           x = datensatz$x[4]-0.018, y = 3.75,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text",
           label=TeX("$\\hat{Y}_5-\\bar{Y}$"),
           x = datensatz$x[5]-0.018, y = 3.95,
           colour = get_icae_colors("dark blue")) +
      ggplot2::annotate(geom = "text",
           label=TeX("$ESS = 0.94864$"),
           x = 0.45, y = 3.0,
           colour = get_icae_colors("dark blue")) +
  icaeDesign::theme_icae() + 
  ggplot2::theme(legend.position = "none")
```

Für die drei gerade eingeführten Teile der Gesamtvarianz gilt im Übrigen:

$$TSS=ESS+RSS$$

Aus diesen Werten können wir nun das **Bestimmtheitsmaß** $R^2$ berechnen, welches Informationen darüber gibt, welchen Anteil der Variation in $Y_i$ durch unser Modell erklärt wird:

$$R^2=\frac{ESS}{TSS}=1-\frac{RSS}{TSS}$$

Wir können das für unseren Anwendungsfall natürlich händisch berechnen:

```{r}
r_sq_manual <- ess / tss
r_sq_manual
```

Leider wird diese Größe im Output von `lm()` direkt nicht ausgegeben.
Wir können aber einen ausführlicheren Output unserer Regression mit der Funktion
`summary()` erstellen, dort ist das $R^2$ dann auch enthalten:

```{r}
info_schaetzung <- summary(schaetzung)
info_schaetzung[["r.squared"]]
```

In unserem Fall erklärt unser Modell also ca. 28% der Gesamtvarianz der erklärten Variable.

<!-- Wie kommst du auf 28%? Bei mir steht als Wert 0.035, also 3,5%, oder? -->


In einer sozialwissenschaftlichen Anwendung wäre das nicht so wenig, denn aufgrund der vielen Faktoren, die hier eine Rolle spielen, darf man keine zu hohen Werte für $R^2$ erwarten.
Vielmehr legen sehr hohe Werte eine gewisse Skepsis nahe, ob nicht eher ein tautologischer Zusammenhang geschätzt wurde.

<!-- In meinem Ökonometriekurs wurde jetzt nochmal betont, dass man R2 nur verwenden soll als Wert wenn man 2 Modelle vergleicht, welche den selben Sachverhalt erklären wollen. Dann kann man das Modell verwenden, welches ein höheres R2 hat. Das kam mir sehr plausibel vor, hier vllt sowas ähnliches auch nochmal schreiben? -->

Ein großer Nachteil vom $R^2$ ist, dass es größer wird sobald wir einfach mehr erklärende Variablen in unsere Regression aufnehmen. 
Warum? Eine neue Variable kann unmöglich $TSS$ verändern (denn die 
erklärenden Variablen kommen in der Formel für TSS nicht vor), aber erhöht immer zumindest ein bisschen die ESS.
Wenn unser alleiniges Ziel also die Maximierung von $R^2$ wäre, dann müssten wir einfach ganz viele erklärenden Variablen in unser Modell aufnehmen. Das kann ja nicht Sinn sozioökonomischer Forschung sein!

Zur Lösung dieses Problems wurde das adjustierte $R^2$ entwickelt, was bei Regressionen auch standardmäßig angegeben wird.
Hier korrigieren wir das $R^2$ mit Hilfe der **Freiheitsgrade** (engl. *degrees of freedom*). Die Freiheitsgerade sind die Differenz zwischen Beobachtungen und Anzahl der zu
schätzenden Parameter und werden in der Regel mit $df$ bezeichnet.
In unserem Fall hier ist $N=5$ und $K=3$, da mit $\beta_0$ und $\beta_1$ zwei Parameter geschätzt werden.

<!-- Was ist K hier? -->

Das adjustierte $R^2$, häufig als $\bar{R}^2$ bezeichnet, ist definiert als:

$$\bar{R}^2=1-\frac{\sum_{i=1}^n\epsilon^2/(N-K-1)}{\sum_{i=1}^n(Y_i-\bar{Y})^2/(N-1)}$$
Um dieses Maß aus unserem Ergebnisobjekts auszugeben schreiben wir:

```{r}
info_schaetzung[["adj.r.squared"]]
```

Leider hat es keine so eindeutige Interpretation wie das $R^2$, aber es sollte immer gemeinsam mit letzterem beachtet werden.
Häufig vergleicht man das $\bar{R}^2$ vor und nach der Inklusion einer weiteren erklärenden Variable. Wenn $\bar{R}^2$ steigt geht man häufig davon aus, dass sich die Inklusion auszahlt, allerdings sind das 'nur' Konventionen. Man sollte nie eine Variabel ohne gute theoretische Begründung aufnehmen!

### Hypothesentests und statistische Signifikanz

Wie sicher können wir uns mit den geschätzten Parametern für $\beta_0$ und $\beta_1$ sein? 
Wenn z.B. $\hat{\beta}_1>0$, bedeutet das wirklich, dass wir einen positiven Effekt gefunden haben? 
Immerhin sind ja unsere Fehler ZV und vielleicht haben wir einfach zufällig eine Stichprobe erhoben, wo der Effekt von $x_1$ positiv erscheint, tatsächlich aber kein Effekt existiert?
Um die Unsicherheit, die mit der Parameterschätzung einhergeht, zu quantifizieren können wir uns die Annahme, dass unsere Fehler normalverteilt sind, zu Nutze machen und testen wie plausibel die tatsächliche Existenz eines Effekts ist.

Wir verlassen nun also das Gebiet der reinen Parameterschätzung und beschäftigen uns mit Hypothesentests und Konfidenzintervallen für unsere Schätzer $\hat{\beta}_0$ und $\hat{\beta}_1$.
Das ist analog zu den in Kapitel @ref(stat-rep) zur schließenden Statistik besprochenen Herangehensweisen.

Wir wissen bereits, dass es sich bei unseren Schätzern $\hat{\beta}_0$ und $\hat{\beta}_1$ um ZV handelt. 
Aber welcher Verteilung folgen sie? Tatsächlich können wir das aus unseren oben getroffenen Annahmen direkt ableiten:

$$\hat{\beta}_0 \propto \mathcal{N}\left(\beta_0, \sigma^2\left( \frac{1}{n} +
\frac{\bar{x}^2}{SS_X}\right) \right), \quad SS_X=\sum_{i=1}^n(x_i-\bar{x})^2\\
\hat{\beta}_1 = \mathcal{N}\left(\beta_1, \frac{\sigma^2}{SS_X}\right)$$


An der Tatsache, dass $\mathbb{E}(\hat{\beta_i})=\beta_i$ sehen wir, dass die Schätzer *erwartungstreu* sind. Es ist dann plausibel die *Genauigkeit* oder *Effizienz* eines Schätzers durch
seine Varianz zu messen: wenn ein Schätzer eine große Varianz hat bedeutet das, dass wir bei dem einzelnen Schätzwert eine große Unsicherheit haben, ob der Schätzer tatsächlich nahe an
seinem Erwartungswert liegt. Am besten kann man das an einem simulierten Beispiel illustrieren.

> **Beispiel: Die Varianz von $\hat{\beta}_1$**: Im Folgenden kreieren wir einen künstlichen Datensatz, bei dem wir den wahren DGP kennen. Dieser wahre DGP wird beschrieben durch:

<!-- was heißt DGP nochmal? -->

$$Y_i=\beta_0+\beta_1 x_i + \epsilon_i, \quad \epsilon_i\propto\mathcal{N}(0,5)$$

> Wenn wir nun mit diesem DGP mehrere Datensätze kreieren, sieht natürlich jeder Datensatz anders aus. Schließlich sind die $\epsilon_i$ zufällig. Dennoch wissen wir, dass, da unsere Schätzer erwartungstreu sind, sie im Mittel die wahren Werte von $\beta_0$ und $\beta_1$ treffen sollten. Aber wie sehr streuen die geschätzten Werte um diesen wahren Wert? Zunächst erstellen wir den künstlichen Datensatz. Dazu spezifizieren wir zunächst
die Grundstruktur des DGT:

<!-- DGT? DGP? -->

```{r}
set.seed(123)
true_DGP <- function(x, b0, b1){
  y <- b0 + b1*x + rnorm(length(x), 0, 5)
  return(y)
}
beta_0_wahr <- 3
beta_1_wahr <- 2
sample_size <- 100
x <- runif(sample_size, 0, 10)
```

> Nun erstellen wir mit Hilfe einer Schleife 1000 Realisierungen der Daten. Wir können uns das wie 1000 Erhebungen vorstellen. Für jede dieser Realisierungen schätzen wir dann die lineare Regressionsgleichung von oben:

```{r}
set.seed(123)
n_datensaetze <- 1000
beta_0_estimates <- rep(NA, n_datensaetze)
beta_1_estimates <- rep(NA, n_datensaetze)

for (i in 1:n_datensaetze){
  daten_satz <- data.frame(
    x = x,
    y = true_DGP(x, beta_0_wahr, beta_1_wahr)
  )
  schaetzung_2 <- lm(y~x, data = daten_satz)
  beta_0_estimates[i] <- schaetzung_2[["coefficients"]][1]
  beta_1_estimates[i] <- schaetzung_2[["coefficients"]][2]
}
```

> Nun können wir die Streuung der Schätzer in Abbildung \@ref(fig:Schaetzervarianz) ablesen.

```{r Schaetzervarianz, echo=FALSE, warning=FALSE, fig.cap="Vergleich der Effizienz zweier Schätzer über ihre jeweilige Streuung."}
beta_0_plot <- ggplot2::ggplot(
  data.frame(x=beta_0_estimates), aes(x=x)
  ) +
  ggplot2::geom_histogram(binwidth = 0.25, aes(y=..density..)) +
  ggplot2::geom_vline(xintercept = beta_0_wahr) +
  ggplot2::scale_y_continuous(expand = expand_scale(c(0, 0), c(0, 0.1))) +
  ggplot2::ylab(TeX("Relative Häufigkeit von $\\hat{\\beta_0}}$")) +
  ggplot2::xlab(TeX("$\\hat{\\beta_0}}$")) +
  ggplot2::ggtitle(TeX("Streuung von $\\hat{\\beta_0}}$")) + 
  ggplot2::annotate(geom = "text", label=TeX("$\\beta_0$"), x = beta_0_wahr-0.25, y=0.45) + 
  icaeDesign::theme_icae()

beta_1_plot <- ggplot2::ggplot(
  data.frame(x=beta_1_estimates), aes(x=x)
  ) +
  ggplot2::geom_histogram(binwidth = 0.05, aes(y=..density..)) +
  ggplot2::geom_vline(xintercept = beta_1_wahr) +
  ggplot2::scale_y_continuous(expand = expand_scale(c(0, 0), c(0, 0.1))) +
  ggplot2::ylab(TeX("Relative Häufigkeit von $\\hat{\\beta_1}}$")) +
  ggplot2::xlab(TeX("$\\hat{\\beta_1}}$")) +
  ggplot2::ggtitle(TeX("Streuung von $\\hat{\\beta_1}}$")) + 
  ggplot2::annotate(geom = "text", label=TeX("$\\beta_1$"), 
           x = beta_1_wahr-0.05, y=2.8) + 
  icaeDesign::theme_icae()

ggpubr::ggarrange(beta_0_plot, beta_1_plot, ncol = 2)
```

> Wie wir sehen, treffen die Schätzer im Mittel den richtigen Wert, streuen aber auch. Die Varianz gibt dabei die Breite des jeweiligen Histograms an und je stärker die relativen Häufigkeiten des geschätzten Wertes um den wahren Wert konzentriert sind, also desto geringer die Varianz, desto genauer und somit effizienter ist der Schätzer.

Ein Maß für die Genauigkeit eines Schätzers ist sein **Standardfehler**. Für $\hat{\beta}_1$ ist dieser wie oben beschrieben definiert als $\frac{\sigma}{\sqrt{SS_X}}$.
Da $\sigma$ (die Varianz der Fehler) nicht bekannt ist, müssen wir sie aus den Daten schätzen. Das geht mit $\frac{1}{n-2}\sum_{i=1}^ne_i^2$, wobei die
detaillierte Herleitung hier nicht diskutiert wird. Grundsätzlich handelt es sich hier um die empirische Varianz. Das $n-2$ kommt von den um zwei reduzierten Freiheitsgraden dieser Schätzung.

Dieser Standardfehler ist ein erstes Maß für die Genauigkeit des Schätzers. Er wird aufgrund seiner Wichtigkeit auch in der Summary jeder Schätzung angegeben. 
Hier betrachten wir die Schätzung aus dem einführenden Beispiel:

```{r}
summary(schaetzung_bip)
```

Sie sind hier unter `Std. Error` zu finden. Wir können diese Information jedoch noch weiter verwenden und Hypothesen im 
Zusammenhang mit den Schätzern testen. Eine besonders relevante Frage ist immer ob ein bestimmter *Schätzer* signifikant
von 0 verschieden ist. Dazu können wir fragen: "Wie wahrscheinlich ist es, gegeben der Daten, dass $\beta_i$ gleich Null ist?".

Das ist die klassische Frage für einen Hypothesentest^[Lesen Sie noch einmal im Kapitel @ref(#stat-rep) zur schließenden Statistik nach, wenn Sie nicht mehr wissen was ein Hypothesentest ist.] mit $H_0: \beta_0=0$ und $H_1: \beta_0 \neq 0$.

Für einen Hypothesentest brauchen wir zunächst eine Teststatistik, also die Verteilung für den Schätzer wenn $H_0$ wahr wäre.
Da wir annehmen, dass die Fehlerterme in unserem Fall normalverteilt sind, ist das in unserem Falle eine $t$-Verteilung mit $n-2$ Freiheitsgraden.^[Warum jetzt
genau eine $t$-Verteilung und keine Normalverteilung? Das liegt daran, dass wir die Varianz unserer Fehler $\sigma$ nicht beobachten können und durch $\hat{\sigma}$ geschätzt haben. Das führt dazu, dass die resultierende Teststatistik nicht mehr 
normalverteilt ist. Mit zunehmendem Stichprobenumfang wird die Abweichung immer irrelevanter, jedoch ist die t-Verteilung so einfach zu handhaben, dass man sie eigentlich immer benutzen kann.]
Damit können wir überprüfen wie wahrscheinlich unser Schätzwert unter der $H_0$ wäre. Wenn er sehr unwahrscheinlich wäre, würden wir $H_0$ verwerfen.

Die Wahrscheinlichkeit, dass wir unseren Schätzer gefunden
hätten, wenn $H_0$ wahr wäre wird durch den $p$-Wert des Schätzers angegeben. Dieser findet sich in der Spalte `Pr(>|t|)`.
In unserem Fall mit $\hat{\beta}_1$ ist dieser Wert mit $2\cdot 10^{-16}$ extrem klein. Das bedeutet, wenn $H_0: \beta_1=0$ wahr wäre, würden wir unseren Wert für $\hat{\beta}_1$ mit einer Wahrscheinlichkeit nahe Null beobachten. 
Es erscheint daher sehr unplausibel, dass $\beta_1=0$. 
Tatsächlich würden wir diese Hypothese auf quasi jedem beliebigen
Signifikanzniveau verwerfen. Daher ist der Schätzer in der Zusammenfassung mit drei Sternen gekennzeichnet:

```{r}
summary(schaetzung_bip)
```

Grundsätzlich gilt, dass wir $H_0: \beta_i = 0$ auf dem $\alpha$-Signifikanzniveau verwerfen können wenn $p<1-\alpha$.
Wenn wir $H_0: \beta_i$ auf dem Signifikanzniveau von mindestens $\alpha=0.05$ verwerfen können, sprechen wir von einem signfikanten Ergebnis. In unserem Beispiel der Konsumfunktion sind also sowohl die Schätzer $\beta_0$ und $\beta_1$ hochsignifikant und wir können, under den oben getroffenen Annahmen, mit großer Sicherheit davon ausgehen, dass beide von Null verschieden sind.

Dabei ist jedoch wichtig darauf hinzuweisen, dass *statistische Signifikanz* nicht mit *sozioökonomischer Relevanz* zu tun hat: 
ein Effekt kann hochsignifikant, aber extrem klein sein.
Dennoch ist die Signifikanz eine wichtige und häufig verwendete Kennzahl für jede lineare Regression. Gleichzeitig ist die wissenschaftliche Praxis, nur Studien mit signifikanten
Ergebnissen ernst zu nehmen, sehr problematisch, Stichwort 
[p-Hacking](https://de.wikipedia.org/wiki/P-Hacking).

### Konfidenzintervalle für die Schätzer

Ausgehend von den Überlegungen zur Signifikanz können wir nun **Konfidenzintervalle** für unsere Schätzer konstruieren.
Wie im Kapitel @ref(#stat-rep) zur schließenden Statistik genauer erläutert besteht ein ein Konfidenzintervall $I_{\alpha}$ aus 
allen geschätzten Parameterwerten, für die wir bei einem zweiseitigen Hypothesentest zum Signifikanzniveau $\alpha$ die Nullhypothese $\beta_i=0$ nicht verwerfen können.

Um diese Intervalle für eine Schätzung in R zu konstruieren verwenden wir die Funktion `confint`, die als erstes Argument das geschätzte Modell und als Argument `level` das Signifikanzniveau $1-\alpha$ akzeptiert:

```{r}
confint(schaetzung_bip, level=0.95)
```

Für $\hat{\beta}_1$ ist das 95%-Konfidenzintervall also $[0.69, 0.72]$. 
Das bedeutet, wenn der zugrundeliegende Datengenerierungsprozess sehr häufig wiederholt werden würde, dann würden 95% der so für $\hat{\beta}_1$ berechneten 95%-Konfidenzintervalle $\beta_1$ enthalten.


### Zur Rolle der Stichprobengröße

Um die Rolle der Stichprobengröße besser beurteilen zu können, verwenden wir hier einen künstlich hergestellten Datensatz für den wir die 'wahren' Werte $\beta_0$ und $\beta_1$ kennen:^[Die Befehle sollten Ihnen weitgehen bekannt sein. Die Funktion `set.seed()` verwenden wir um den 
[Zufallszahlengenerator von R](https://de.wikipedia.org/wiki/Mersenne-Twister) so
zu kalibrieren, dass bei jedem Durchlaufen des Skripts die gleichen Realisierungen der ZV gezogen werden und die Ergebnisse somit reproduzierbar sind.]

```{r}
set.seed(123)
wahres_b0 <- 3
wahres_b1 <- 1.4

stichproben_n <- 50
x <- 1:stichproben_n * 0.1
fehler <- rnorm(stichproben_n, mean = 0, sd = 3)
y <- rep(NA, stichproben_n)

for (i in 1:stichproben_n){
  y[i] <- wahres_b0 + wahres_b1*x[i] + fehler[i]
}
datensatz <- data.frame(
  x = x,
  y = y
)
```


Wie wir in Abbildung \@ref(fig:stichproben) sehen ist die geschätzte Gerade nicht exakt deckungsgleich zur 'wahren' Gerade, aber doch durchaus nahe dran.

```{r stichproben, echo=FALSE, fig.cap="Vergleich der geschätzten und wahren Gerade unserer Stichprobe."}
reg_obj <- lm(y~x, data = datensatz)
gesch_b0 <- reg_obj[["coefficients"]][1]
gesch_b1 <- reg_obj[["coefficients"]][2]

cols_used <- get_icae_colors(2, palette_used = "mixed")

ggplot2::ggplot(datensatz, aes(x=x, y=y)) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(aes(color="Geschätzte Werte", 
                  intercept = gesch_b0, 
                  slope = gesch_b1), 
              alpha=0.75, show.legend = TRUE) +
  ggplot2::geom_abline(aes(color="Wahre Werte", 
                  intercept = wahres_b0, 
                  slope = wahres_b1), 
              alpha=0.75, show.legend = TRUE) +
  ggplot2::scale_color_manual(values = c("Geschätzte Werte"=cols_used[1], 
                                "Wahre Werte"=cols_used[2])
                     ) + 
  icaeDesign::theme_icae() + 
  ggplot2::theme(legend.position = "bottom")
```

Grundsätzlich gilt, dass die erwartete Deckung der beiden dann höher ist wenn (1) die Annahmen für die einfache lineare Regression erfüllt sind und (2) die Stichprobe groß ist.
Im Moment sind wir in einer Luxussituation, da wir die 'wahre' Gerade kennen: wir haben ja den Datensatz, für den wir die Gerade schätzen, selbst erstellt.
In der Praxis bleibt uns nichts anderes üblich als (1) so gut es geht zu überprüfen und die restliche Unsicherheit so gut es geht zu quantifizieren. Im Folgenden wollen wir uns genauer anschauen welche Methoden uns dafür zur Verfügung stehen. 
Vorher wollen wir uns aber noch in Abbildung \@ref(fig:stichprobengroesse) ansehen, wie eine größere Stichprobe die Schätzgenauigkeit beeinflusst, Die formale Begründung warum das so ist folgt am Ende des Kapitels.

<!-- Hab jetzt das Kapitel zuende gelesen und bin mir nicht sicher welches von den Sachen die noch kamen jetzt genau die formale Begründung hierfür war? -->


```{r stichprobengroesse, echo=FALSE, fig.cap="Vergleich der geschätzten Parameterwerte mit den wahren Werten unter verschiedenen Stichprobengrößen."}
plot_data <- function(n_val, wahres_b0, wahres_b1){
  for (n in n_val){
  x <- runif(n_val, min = 0, max = 10)
  fehler <- rnorm(n_val, mean = 0, sd = 3)
  y <- rep(NA, n_val)
  for (i in 1:n_val){
    y[i] <- wahres_b0 + wahres_b1*x[i] + fehler[i]
    }
  data <- data.frame(
    x = x,
    y = y
  )
  }
  
  cols_used <- get_icae_colors(2, palette_used = "mixed")
  reg_obj <- lm(y~x, data = data)
  gesch_b0 <- reg_obj[["coefficients"]][1]
  gesch_b1 <- reg_obj[["coefficients"]][2]
  
  ggplot2::ggplot(data, aes(x=x, y=y)) +
    ggplot2::geom_point() +
    ggplot2::geom_abline(aes(color="Geschätzte Werte", 
                  intercept = gesch_b0, 
                  slope = gesch_b1), 
              alpha=0.75, show.legend = TRUE) +
    ggplot2::geom_abline(aes(color="Wahre Werte", 
                  intercept = wahres_b0, 
                  slope = wahres_b1), 
              alpha=0.75, show.legend = TRUE) +
    ggplot2::annotate(geom="text",
             label=TeX(paste0("$\\hat{\\beta_0} =", round(gesch_b0, 2), "$,  $",  
                       "\ \\hat{\\beta_1} =", round(gesch_b1, 2), " $")), 
             x = 7.5, y = 0) +
    ggplot2::scale_color_manual(values = c("Geschätzte Werte"=cols_used[1], 
                                "Wahre Werte"=cols_used[2])
                     ) + 
    ggplot2::scale_x_continuous(limits = c(0, 10), expand = c(0,0)) +
    ggplot2::scale_y_continuous(limits = c(-5, 25), expand = c(0,0)) +
    ggplot2::ggtitle(paste0("Stichprobengröße: ", n_val)) +
    icaeDesign::theme_icae() + 
    ggplot2::theme(legend.position = "bottom")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(234)
wahres_b0 <- 3
wahres_b1 <- 1.4
  
stichproben_n <- c(5, 10, 25, 100)

plot_liste <- lapply(stichproben_n, plot_data, 
                     wahres_b0, wahres_b1)

full_plot <- ggpubr::ggarrange(plotlist = plot_liste, ncol = 2, nrow = 2, 
          common.legend = T, legend = "bottom")

ggpubr::annotate_figure(full_plot, 
                text_grob(TeX(paste0("Regressionen mit $\\beta_0 = ",
                                     wahres_b0, "$ und $\\beta_1 = ", 
                                     wahres_b1, "$")), 
                          face = "bold", size = 14))
```


### Residuenanalyse {#linmod-residuals}

Eine sehr hilfreiche Art, die Modellannahmen von oben zu überprüfen ist die Analyse der Residuen. 

<!-- was heißt "von oben"? -->

Diese sind im Ergebnisobjekt der Regression gespeichert:

```{r, eval=FALSE}
schaetzung_bip[["residuals"]]
```

Wir wollen nun die Residuen verwenden um die folgenden Annahmen unseres Regressionsmodells zu überprüfen:

1. **A3: ** $Var(\epsilon_i)=\sigma^2\forall i$

2. **A4: ** $Cov(\epsilon_i, \epsilon_j)=0 \forall i,j$

3. **A6:** $\epsilon \propto \mathcal{N}(0, \sigma^2)$

<!-- Ich dachte bei 3/A6 ginge es um die Fehler, nicht um die Residuen. I'm confused :(  -->

Um die ersten beiden Annahmen zu überpüfen bilden wir die $e_i$ gegen $\hat{Y}$ ab und erhalten so den so genannten **Tukey-Anscombe-Plot** (siehe Abbildung \@ref(fig:TAPlot)).

```{r TAPlot, echo=FALSE, fig.cap="Tukey-Anscombe-Plot."}
ggplot2::ggplot(
  data.frame(
    GefitteteWerte=schaetzung_bip[["fitted.values"]],
    Residuen=schaetzung_bip[["residuals"]]),
  aes(x=GefitteteWerte, y=Residuen)
  ) +
  ggplot2::ggtitle("Tukey-Anscombe-Plot") +
  ggplot2::geom_hline(yintercept = 0) + 
  ggplot2::geom_point() +
  icaeDesign::theme_icae()
```


Hier geht es nun darum eine Struktur zu erkennen.
Wenn alle Annahmen korrekt sind, sehen wir nur eine unstrukturierte Punktewolke. In dem vorliegenden Fall können wir aufgrund der wenigen Datenpunkte den Plot aber nur mit großer Schwerierigkeit interpretieren - auch deswegen sind große Stichproben immer besser.
Es scheint aber so zu sein, dass die Varianz der Fehler mit $x_i$ steigt, also A3 möglicherweise verletzt ist - zum Glück kann man den OLS-Schätzer leicht modifizieren um damit umzugehen.
<!-- Varianz der Residuen oder? -->

Ansonsten ist keine Struktur zumindest unmittelbar ersichtlich.

Als nächstes wollen die Annahme normalverteilter Residuen überprüfen. Das geht mit dem so genannten **Q-Q-Plot**, siehe dazu Abbildung \@ref(fig:QQPlot).

```{r QQPlot,echo=FALSE, fig.cap="Q-Q-Plot zur Überprüfung der Residuen."}
ggplot2::ggplot(data.frame(Residuen=schaetzung_bip[["residuals"]]),
       aes(sample=Residuen)) + 
  ggplot2::stat_qq() + ggplot2::stat_qq_line() +
  ggplot2::ggtitle("Q-Q-Plot für die Residuen") +
  icaeDesign::theme_icae()
```

Bei normalverteilten Residuen würden die Punkte möglichst exakt auf der Linie liegen. Das ist hier nur bedingt der Fall, deswegen sollten wir skeptisch bezüglich aller Ergebnisse sein, die auf der Normalverteilungsannahme aufbauen, also auf den Standardfehlern, $p$-Werten und den Konfidenzintervallen.

Natürlich gibt es auch noch weitere Probleme, die bei einer linearen Regression auftreten können. So ist es immer ein Problem, wenn wir eine wichtige erklärende Variable in 
unserem Modell vergessen haben (**omitted variable bias**), da deren Effekt dann durch die Fehlerterme abgefangen wird und zu einer Verletzung von **A1** und **A2** führt.

Ein großes Problem stellt auch die so genannte **Simultanität** dar: diese tritt auf, wenn zwischen erklärter und erklärender Variable ein wechelseitiges kausales Verhältnis besteht. Wir sprechen dann auch von einem **Endogenitätsproblem**, welches leider sehr häufig auftritt und letztendlich vor allem theoretisch identifiziert werden muss.

Ausführlichere Tests und Möglichkeiten mit verletzten Annahmen umzugehen werden im folgenden Kapitel @ref(#advlin) genauer diskutiert.


## Zum Ablauf einer Regression {#stat-ablauf}

Insgesamt ergibt sich aus den eben beschriebenen Schritten also folgendes Vorgehen bei einer Regression:

1. Aufstellen des statistischen Modells

2. Erheben und Aufbereitung der Daten

3. Schätzen des Modells

4. Überprüfung der Modellannahmen durch die Residuenanalyse

5. Inspektion der relevanten Kennzahlen wie $R^2$ und der statistischen Signifikanz der geschätzten Werte; falls relevant: Angabe von Konfidenzintervallen

## Multiple lineare Regression {#lin-multi}

Zum Abschluss wollen wir noch das bislang besprochene für den Fall von mehreren erklärenden Variablen generalisieren.
In der Praxis werden Sie nämlich so gut wie immer mehr als eine erklärende Variable verwenden. Zwar sind die resultierenden Plots häufig nicht so einfach zu interpretieren wie im Fall der einfachen Regression, das Prinzip ist jedoch quasi das gleiche.
Zudem ist die Implementierung in R nicht wirklich schwieriger.

Im Folgenden wollen wir den uns bereits aus früheren Kapiteln bekannten Beispieldatensatz verwenden, in dem Informationen über die Preise von ökonomischen Journalen gesammelt sind:

<!-- Das ist doch derselbe oder? (hab den Text geändert zu "uns bereits bekannten". ansonsten wenn nicht der selbe wieder zurückändern zu einfach nur "wollen einen Datensatz verwenden") -->

```{r}
journal_data <- fread(here("data/tidy/journaldaten.csv")) %>%
  select(Titel, Preis, Seitenanzahl, Zitationen)
head(journal_data)
```

In einer einfachen linearen Regression könnten wir z.B. folgendes Modell schätzen:

$$PREIS_i = \beta_0 + \beta_1 SEITEN + \epsilon$$

Das würden wir mit folgendem Befehl in R implementieren:

```{r}
reg <- lm(Preis~Seitenanzahl, data=journal_data)
summary(reg)
```

Allerdings ergibt es auch Sinn anzunehmen, dass beliebte Journale teurer sind. Daher würden wir gerne die Anzahl der Zitationen in das obige Modell als zweite erklärende Variable aufnehmen. In diesem Fall würden wir mit einem *multiplen* linearen Modell arbeiten:

$$PREIS_i = \beta_0 + \beta_1 SEITEN + \beta_2 ZITATE + \epsilon$$


Tatsächlich ist die einzige Änderungen, die wir auf der technischen Seite machen müssen, die Inklusion der neuen erklärenden Variable in die Schätzgleichung:

```{r}
reg <- lm(Preis~Seitenanzahl + Seitenanzahl + Zitationen, data=journal_data)
```

<!-- warum ist denn jetzt Seitenanzahl hier 2mal drin? Sollte das nicht nur einmal drin sein? -->

Hierbei ist zu beachten, dass das `+` nicht im additiven Sinne gemeint ist, sondern in der Logik einer Regressionsgleichung. 

Wenn wir uns die Zusammenfassung dieses Objekts anschauen, sehen wir einen sehr ähnlichen Output wie für den einfachen linearen Fall, nur dass wir eine weitere Zeile für die neue erklärende Variable haben:

```{r}
summary(reg)
```

Zwei Punkte sind bei der multiplen Regression zu beachten:
Erstens sind die geschätzten Effekte als **isolierte Effekte** zu interpretieren, also in einer Situation in der alle anderen erklärenden Variablen fix gehalten werden. Das ist die berühmte *ceteris paribus* Formel.

Der geschätzte Wert für `Seitenanzahl` sagt uns dementsprechend: "*Ceteris paribus*, also alle anderen Einflussfaktoren fix gehalten, geht ein um eine Seite dickeres Journal mit einem um $0.6$ Dollar höherem Abo-Preis einher."
Beachten Sie den relevanten Unterschied zur einfachen Regression, die sehr wahrscheinlich unter dem oben angesprochenen *omitted variable bias* gelitten hat.

Der zweite zu beachtende Aspekt bezieht sich auf die Korrelation der verschiedenen erklärenden Variablen. Die Annahmen für OLS schließen an sich nur so genannte *perfekte Kollinearität* (A5) aus, das heißt die Situation in der eine erklärende 
Variable eine perfekte lineare Transformation einer anderen erklärenden Variable ist. Problematisch sind aber auch schon geringere, aber immer noch hohe Korrelationen: denn je stärker die erklärenden Variablen untereinander korrelieren, desto größer werden die Standardfehler unserer Schätzer. Mit diesem Problem werden wir uns im folgenden Kapitel noch genauer 
auseinandersetzen.


<!-- Würde hier ein Umstrukturierung vorschlagen und das Unterkapitel " Zum Ablauf einer Regression" hier ans Ende dieses Kapitels setzen - der Ablauf ändert sich ja an sich nicht großartig bei einer einfachen vs multiplen Regression, und glaube das wäre ein netterer Kapitelabschluss als der, der jetzt hier steht. -->
